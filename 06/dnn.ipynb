{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978c2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sining Sun, Zhanheng Yang, Binbin Zhang\n",
    "\n",
    "import numpy as np\n",
    "import kaldi_io\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f59a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_list = ['Z', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "targets_mapping = {}\n",
    "for i, x in enumerate(targets_list):\n",
    "    targets_mapping[x] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6bf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, input):\n",
    "        ''' Forward function by input\n",
    "        Args:\n",
    "            input: input, B * N matrix, B for batch size\n",
    "        Returns:\n",
    "            output when applied this layer\n",
    "        '''\n",
    "        raise 'Not implement error'\n",
    "\n",
    "    def backward(self, input, output, d_output):\n",
    "        ''' Compute gradient of this layer's input by (input, output, d_output)\n",
    "            as well as compute the gradient of the parameter of this layer\n",
    "        Args:\n",
    "            input: input of this layer\n",
    "            output: output of this layer\n",
    "            d_output: accumulated gradient from final output to this\n",
    "                      layer's output\n",
    "        Returns:\n",
    "            accumulated gradient from final output to this layer's input\n",
    "        '''\n",
    "        raise 'Not implement error'\n",
    "\n",
    "    def set_learning_rate(self, lr):\n",
    "        ''' Set learning rate of this layer'''\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def update(self):\n",
    "        ''' Update this layers parameter if it has or do nothing\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c71effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, input):\n",
    "        # BEGIN_LAB\n",
    "        return (abs(input) + input) / 2\n",
    "        # END_LAB\n",
    "\n",
    "    def backward(self, input, output, d_output):\n",
    "        # BEGIN_LAB\n",
    "        d_output[input<=0]=0\n",
    "        return d_output\n",
    "        # END_LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c260d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnect(Layer):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.w = np.random.randn(out_dim, in_dim) * np.sqrt(2.0 / in_dim)\n",
    "        self.b = np.zeros(out_dim)\n",
    "        self.dw = np.zeros((out_dim, in_dim))\n",
    "        self.db = np.zeros(out_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # BEGIN_LAB\n",
    "        return np.dot(input,self.w.T)+self.b\n",
    "        # END_LAB\n",
    "\n",
    "    def backward(self, input, output, d_output):          #d_out是误差\n",
    "        #print(input.shape,output.shape,d_output.shape,self.w.shape)\n",
    "        batch_size = input.shape[0]\n",
    "        in_diff = None\n",
    "        # BEGIN_LAB, compute in_diff/dw/db here\n",
    "        \n",
    "        self.dw = np.dot(d_output.T,input)\n",
    "        \n",
    "        self.db = np.sum(d_output, axis=0)\n",
    "            \n",
    "        in_diff = np.dot(d_output,self.w)\n",
    "        \n",
    "        # END_LAB\n",
    "        # Normalize dw/db by batch size\n",
    "        self.dw = self.dw / batch_size\n",
    "        self.db = self.db / batch_size\n",
    "        return in_diff\n",
    "\n",
    "    def update(self):\n",
    "        self.w = self.w - self.learning_rate * self.dw\n",
    "        self.b = self.b - self.learning_rate * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae881af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        row_max = input.max(axis=1).reshape(input.shape[0], 1)\n",
    "        x = input - row_max\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(x.shape[0], 1)\n",
    "\n",
    "    def backward(self, input, output, d_output):\n",
    "        ''' Directly return the d_output as we show below, the grad is to\n",
    "            the activation(input) of softmax\n",
    "        '''\n",
    "        return d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affd7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim, num_hidden):\n",
    "        self.layers = []\n",
    "        self.layers.append(FullyConnect(in_dim, hidden_dim))\n",
    "        self.layers.append(ReLU())\n",
    "        for i in range(num_hidden):\n",
    "            self.layers.append(FullyConnect(hidden_dim, hidden_dim))\n",
    "            self.layers.append(ReLU())\n",
    "        self.layers.append(FullyConnect(hidden_dim, out_dim))\n",
    "        self.layers.append(Softmax())\n",
    "\n",
    "    def set_learning_rate(self, lr):\n",
    "        for layer in self.layers:\n",
    "            layer.set_learning_rate(lr)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.forward_buf = []\n",
    "        out = input\n",
    "        self.forward_buf.append(out)\n",
    "        for i in range(len(self.layers)):\n",
    "            out = self.layers[i].forward(out)\n",
    "            self.forward_buf.append(out)\n",
    "        assert (len(self.forward_buf) == len(self.layers) + 1)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad):\n",
    "        '''\n",
    "        Args:\n",
    "            grad: the grad is to the activation before softmax\n",
    "        '''\n",
    "        self.backward_buf = [None] * len(self.layers)\n",
    "        self.backward_buf[len(self.layers) - 1] = grad\n",
    "        for i in range(len(self.layers) - 2, -1, -1):\n",
    "            grad = self.layers[i].backward(self.forward_buf[i],\n",
    "                                           self.forward_buf[i + 1],\n",
    "                                           self.backward_buf[i + 1])\n",
    "            self.backward_buf[i] = grad\n",
    "\n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c5c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, total_label):\n",
    "    output = np.zeros((labels.shape[0], total_label))\n",
    "    for i in range(labels.shape[0]):\n",
    "        output[i][labels[i]] = 1.0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69084c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dnn):\n",
    "    utt2feat, utt2target = read_feats_and_targets('train/feats.scp',\n",
    "                                                  'train/text')                #特征字典，标签字典\n",
    "    print(len(utt2feat),len(utt2target))                              #长度330\n",
    "    inputs, labels = build_input(targets_mapping, utt2feat, utt2target)\n",
    "    print(inputs.shape,labels.shape)\n",
    "    num_samples = inputs.shape[0]                                             #总长度\n",
    "    # Shuffle data\n",
    "    permute = np.random.permutation(num_samples)                       #乱序数组\n",
    "    inputs = inputs[permute]                                              #1个input是429行，2维\n",
    "    labels = labels[permute]                                             #1维，状态\n",
    "    num_epochs = 20\n",
    "    batch_size = 100\n",
    "    for i in range(num_epochs):\n",
    "        cur = 0\n",
    "        while cur < num_samples:\n",
    "            end = min(cur + batch_size, num_samples)         #这一次的epoch的最后是end，开始是cur\n",
    "            input = inputs[cur:end]\n",
    "            label = labels[cur:end]\n",
    "                                                              # Step1: forward\n",
    "            out = dnn.forward(input)                          #正向传播\n",
    "            #print(out.shape,label.shape)                                 #最后一层是每一帧对于11个目标状态的概率值\n",
    "            one_hot_label = one_hot(label, out.shape[1]) \n",
    "                                                                # Step2: Compute cross entropy loss and backward\n",
    "            loss = -np.sum(np.log(out + 1e-20) * one_hot_label) / out.shape[0]\n",
    "                                                              # The grad is to activation before softmax\n",
    "            grad = out - one_hot_label\n",
    "            dnn.backward(grad)\n",
    "                                                                # Step3: update parameters\n",
    "            dnn.update()\n",
    "            print('Epoch {} num_samples {} loss {}'.format(i, cur, loss))\n",
    "            cur += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2b336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dnn):\n",
    "    utt2feat, utt2target = read_feats_and_targets('test/feats.scp',\n",
    "                                                  'test/text')                    \n",
    "    total = len(utt2feat)\n",
    "    correct = 0\n",
    "    for utt in utt2feat:\n",
    "        t = utt2target[utt]\n",
    "        ark = utt2feat[utt]\n",
    "        mat = kaldi_io.read_mat(ark)\n",
    "        mat = splice(mat, 5, 5)\n",
    "        posterior = dnn.forward(mat)\n",
    "        posterior = np.sum(posterior, axis=0) / float(mat.shape[0])\n",
    "        predict = targets_list[np.argmax(posterior)]\n",
    "        if t == predict: correct += 1\n",
    "        print('label: {} predict: {}'.format(t, predict))\n",
    "    print('Acc: {}'.format(float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d228396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 330\n",
      "(18593, 429) (18593,)\n",
      "Epoch 0 num_samples 0 loss 15.628633396495088\n",
      "Epoch 0 num_samples 100 loss 15.596826659003375\n",
      "Epoch 0 num_samples 200 loss 12.858356923044662\n",
      "Epoch 0 num_samples 300 loss 10.824284632153876\n",
      "Epoch 0 num_samples 400 loss 9.603603500457153\n",
      "Epoch 0 num_samples 500 loss 8.570295367936513\n",
      "Epoch 0 num_samples 600 loss 10.832593148085698\n",
      "Epoch 0 num_samples 700 loss 8.338319712491744\n",
      "Epoch 0 num_samples 800 loss 8.974606807639109\n",
      "Epoch 0 num_samples 900 loss 7.947300554497961\n",
      "Epoch 0 num_samples 1000 loss 7.354009499320913\n",
      "Epoch 0 num_samples 1100 loss 7.8172664478058325\n",
      "Epoch 0 num_samples 1200 loss 7.496290822826072\n",
      "Epoch 0 num_samples 1300 loss 5.335715814252815\n",
      "Epoch 0 num_samples 1400 loss 6.226340094973284\n",
      "Epoch 0 num_samples 1500 loss 6.29869771450125\n",
      "Epoch 0 num_samples 1600 loss 6.763292102542775\n",
      "Epoch 0 num_samples 1700 loss 6.473956151828652\n",
      "Epoch 0 num_samples 1800 loss 6.129419316061203\n",
      "Epoch 0 num_samples 1900 loss 6.333406284110201\n",
      "Epoch 0 num_samples 2000 loss 5.85459317436561\n",
      "Epoch 0 num_samples 2100 loss 5.638160197578525\n",
      "Epoch 0 num_samples 2200 loss 5.837136459892963\n",
      "Epoch 0 num_samples 2300 loss 5.322828210804719\n",
      "Epoch 0 num_samples 2400 loss 4.808674566010265\n",
      "Epoch 0 num_samples 2500 loss 4.860289657503931\n",
      "Epoch 0 num_samples 2600 loss 4.231108564988315\n",
      "Epoch 0 num_samples 2700 loss 5.106304773844417\n",
      "Epoch 0 num_samples 2800 loss 4.593041496638187\n",
      "Epoch 0 num_samples 2900 loss 4.291163171910182\n",
      "Epoch 0 num_samples 3000 loss 4.7709985563994\n",
      "Epoch 0 num_samples 3100 loss 4.285365809846432\n",
      "Epoch 0 num_samples 3200 loss 4.636179516868347\n",
      "Epoch 0 num_samples 3300 loss 4.33608642058452\n",
      "Epoch 0 num_samples 3400 loss 3.8892782141925513\n",
      "Epoch 0 num_samples 3500 loss 4.273771322554524\n",
      "Epoch 0 num_samples 3600 loss 3.6233240915125737\n",
      "Epoch 0 num_samples 3700 loss 3.5314741736445763\n",
      "Epoch 0 num_samples 3800 loss 3.8771611962227155\n",
      "Epoch 0 num_samples 3900 loss 3.763601275378045\n",
      "Epoch 0 num_samples 4000 loss 3.5419264590889292\n",
      "Epoch 0 num_samples 4100 loss 3.1275148584575687\n",
      "Epoch 0 num_samples 4200 loss 3.45482160867253\n",
      "Epoch 0 num_samples 4300 loss 3.7182572330550334\n",
      "Epoch 0 num_samples 4400 loss 3.8905100707267457\n",
      "Epoch 0 num_samples 4500 loss 4.057013456572184\n",
      "Epoch 0 num_samples 4600 loss 3.6101643546264155\n",
      "Epoch 0 num_samples 4700 loss 4.065614961314199\n",
      "Epoch 0 num_samples 4800 loss 3.372287552059955\n",
      "Epoch 0 num_samples 4900 loss 3.099508310398482\n",
      "Epoch 0 num_samples 5000 loss 3.9122203383652585\n",
      "Epoch 0 num_samples 5100 loss 3.558066731402554\n",
      "Epoch 0 num_samples 5200 loss 3.491987062192595\n",
      "Epoch 0 num_samples 5300 loss 3.425261901802953\n",
      "Epoch 0 num_samples 5400 loss 3.7081860780099705\n",
      "Epoch 0 num_samples 5500 loss 3.7555820812722653\n",
      "Epoch 0 num_samples 5600 loss 3.5118457247539165\n",
      "Epoch 0 num_samples 5700 loss 3.6882505174117735\n",
      "Epoch 0 num_samples 5800 loss 3.5009525606529515\n",
      "Epoch 0 num_samples 5900 loss 3.0703715365706183\n",
      "Epoch 0 num_samples 6000 loss 2.915686920148983\n",
      "Epoch 0 num_samples 6100 loss 2.703145377819193\n",
      "Epoch 0 num_samples 6200 loss 3.357601180056041\n",
      "Epoch 0 num_samples 6300 loss 3.2094837459681558\n",
      "Epoch 0 num_samples 6400 loss 3.087984224815896\n",
      "Epoch 0 num_samples 6500 loss 3.4136717532710623\n",
      "Epoch 0 num_samples 6600 loss 2.7603709206292155\n",
      "Epoch 0 num_samples 6700 loss 2.984956299699259\n",
      "Epoch 0 num_samples 6800 loss 3.418374347538406\n",
      "Epoch 0 num_samples 6900 loss 2.510350462035528\n",
      "Epoch 0 num_samples 7000 loss 3.1690881930323074\n",
      "Epoch 0 num_samples 7100 loss 3.1925724709107755\n",
      "Epoch 0 num_samples 7200 loss 2.810263854601756\n",
      "Epoch 0 num_samples 7300 loss 2.8756042168688647\n",
      "Epoch 0 num_samples 7400 loss 3.1327138014043885\n",
      "Epoch 0 num_samples 7500 loss 2.846406235734796\n",
      "Epoch 0 num_samples 7600 loss 2.6988836335408304\n",
      "Epoch 0 num_samples 7700 loss 2.8332059235631197\n",
      "Epoch 0 num_samples 7800 loss 2.6744340237793525\n",
      "Epoch 0 num_samples 7900 loss 2.08401522645797\n",
      "Epoch 0 num_samples 8000 loss 2.887952874299755\n",
      "Epoch 0 num_samples 8100 loss 2.769988889182091\n",
      "Epoch 0 num_samples 8200 loss 2.4302584711478157\n",
      "Epoch 0 num_samples 8300 loss 2.69582515168284\n",
      "Epoch 0 num_samples 8400 loss 2.7048061170528466\n",
      "Epoch 0 num_samples 8500 loss 2.221369602534076\n",
      "Epoch 0 num_samples 8600 loss 2.9895092723254133\n",
      "Epoch 0 num_samples 8700 loss 2.60861940961619\n",
      "Epoch 0 num_samples 8800 loss 1.846035075309282\n",
      "Epoch 0 num_samples 8900 loss 2.645366336456673\n",
      "Epoch 0 num_samples 9000 loss 3.0312820594153385\n",
      "Epoch 0 num_samples 9100 loss 2.3430958708917524\n",
      "Epoch 0 num_samples 9200 loss 2.8004964264439707\n",
      "Epoch 0 num_samples 9300 loss 2.0489673922222664\n",
      "Epoch 0 num_samples 9400 loss 2.7686941317399887\n",
      "Epoch 0 num_samples 9500 loss 2.425210181382309\n",
      "Epoch 0 num_samples 9600 loss 2.059262583423842\n",
      "Epoch 0 num_samples 9700 loss 2.505713089843164\n",
      "Epoch 0 num_samples 9800 loss 2.289715886278218\n",
      "Epoch 0 num_samples 9900 loss 2.7563562038513805\n",
      "Epoch 0 num_samples 10000 loss 2.2727121220362636\n",
      "Epoch 0 num_samples 10100 loss 2.604543878939163\n",
      "Epoch 0 num_samples 10200 loss 2.304321661603945\n",
      "Epoch 0 num_samples 10300 loss 2.1797656327522326\n",
      "Epoch 0 num_samples 10400 loss 2.281643690360866\n",
      "Epoch 0 num_samples 10500 loss 2.2230813302796246\n",
      "Epoch 0 num_samples 10600 loss 2.5578937631379204\n",
      "Epoch 0 num_samples 10700 loss 2.3739648256730272\n",
      "Epoch 0 num_samples 10800 loss 2.031020321832008\n",
      "Epoch 0 num_samples 10900 loss 2.5010475546753557\n",
      "Epoch 0 num_samples 11000 loss 2.3810904112824494\n",
      "Epoch 0 num_samples 11100 loss 1.700484177226258\n",
      "Epoch 0 num_samples 11200 loss 2.855746239827764\n",
      "Epoch 0 num_samples 11300 loss 2.59416696654989\n",
      "Epoch 0 num_samples 11400 loss 2.2145441751090433\n",
      "Epoch 0 num_samples 11500 loss 2.2563421851919356\n",
      "Epoch 0 num_samples 11600 loss 1.8676678341704207\n",
      "Epoch 0 num_samples 11700 loss 2.3947981186518277\n",
      "Epoch 0 num_samples 11800 loss 2.0125459291658996\n",
      "Epoch 0 num_samples 11900 loss 1.8656741686784815\n",
      "Epoch 0 num_samples 12000 loss 2.6370979028834505\n",
      "Epoch 0 num_samples 12100 loss 2.225632368861176\n",
      "Epoch 0 num_samples 12200 loss 1.8511735234559916\n",
      "Epoch 0 num_samples 12300 loss 2.0815191396282215\n",
      "Epoch 0 num_samples 12400 loss 1.9566829827790766\n",
      "Epoch 0 num_samples 12500 loss 2.249292027821392\n",
      "Epoch 0 num_samples 12600 loss 1.9263757587333101\n",
      "Epoch 0 num_samples 12700 loss 1.875792646182871\n",
      "Epoch 0 num_samples 12800 loss 1.8219158365676684\n",
      "Epoch 0 num_samples 12900 loss 1.8724997279896285\n",
      "Epoch 0 num_samples 13000 loss 1.7350618590944507\n",
      "Epoch 0 num_samples 13100 loss 1.7288313311914802\n",
      "Epoch 0 num_samples 13200 loss 1.991502593728898\n",
      "Epoch 0 num_samples 13300 loss 2.0493710615920584\n",
      "Epoch 0 num_samples 13400 loss 1.9013855474350538\n",
      "Epoch 0 num_samples 13500 loss 2.0142144301508065\n",
      "Epoch 0 num_samples 13600 loss 1.638750047611805\n",
      "Epoch 0 num_samples 13700 loss 2.1796900479089074\n",
      "Epoch 0 num_samples 13800 loss 1.9150502283913\n",
      "Epoch 0 num_samples 13900 loss 2.3982354110875024\n",
      "Epoch 0 num_samples 14000 loss 1.9573776564603205\n",
      "Epoch 0 num_samples 14100 loss 1.8059908019022168\n",
      "Epoch 0 num_samples 14200 loss 2.1107814596573\n",
      "Epoch 0 num_samples 14300 loss 2.030673124761488\n",
      "Epoch 0 num_samples 14400 loss 2.1776289331491245\n",
      "Epoch 0 num_samples 14500 loss 1.6433578041104542\n",
      "Epoch 0 num_samples 14600 loss 2.0931118876160038\n",
      "Epoch 0 num_samples 14700 loss 1.772423597908871\n",
      "Epoch 0 num_samples 14800 loss 2.0581077165837645\n",
      "Epoch 0 num_samples 14900 loss 1.8594316841880636\n",
      "Epoch 0 num_samples 15000 loss 1.664026177906173\n",
      "Epoch 0 num_samples 15100 loss 1.6055971646165192\n",
      "Epoch 0 num_samples 15200 loss 1.7640142854843242\n",
      "Epoch 0 num_samples 15300 loss 1.9909444385474564\n",
      "Epoch 0 num_samples 15400 loss 1.7408457285799266\n",
      "Epoch 0 num_samples 15500 loss 1.7598599537505526\n",
      "Epoch 0 num_samples 15600 loss 2.015532501115553\n",
      "Epoch 0 num_samples 15700 loss 1.9997825169670025\n",
      "Epoch 0 num_samples 15800 loss 2.027316376659384\n",
      "Epoch 0 num_samples 15900 loss 1.8805493375276003\n",
      "Epoch 0 num_samples 16000 loss 1.6949306803600397\n",
      "Epoch 0 num_samples 16100 loss 1.7919067210981319\n",
      "Epoch 0 num_samples 16200 loss 1.7316489444891658\n",
      "Epoch 0 num_samples 16300 loss 2.027449267912042\n",
      "Epoch 0 num_samples 16400 loss 1.9888737445543132\n",
      "Epoch 0 num_samples 16500 loss 2.3196342639017713\n",
      "Epoch 0 num_samples 16600 loss 1.8150018011803024\n",
      "Epoch 0 num_samples 16700 loss 1.3518815703469986\n",
      "Epoch 0 num_samples 16800 loss 1.6791185243861646\n",
      "Epoch 0 num_samples 16900 loss 1.7094065126198057\n",
      "Epoch 0 num_samples 17000 loss 2.0084936621387373\n",
      "Epoch 0 num_samples 17100 loss 1.5960674999910578\n",
      "Epoch 0 num_samples 17200 loss 1.512390843676742\n",
      "Epoch 0 num_samples 17300 loss 1.4996802811889227\n",
      "Epoch 0 num_samples 17400 loss 1.6931152672986902\n",
      "Epoch 0 num_samples 17500 loss 1.5312064017136742\n",
      "Epoch 0 num_samples 17600 loss 1.6124701634019352\n",
      "Epoch 0 num_samples 17700 loss 1.9607578201106763\n",
      "Epoch 0 num_samples 17800 loss 1.593157334393578\n",
      "Epoch 0 num_samples 17900 loss 1.7477234330965388\n",
      "Epoch 0 num_samples 18000 loss 1.2489976801971516\n",
      "Epoch 0 num_samples 18100 loss 1.4920574565220661\n",
      "Epoch 0 num_samples 18200 loss 1.3086059929218834\n",
      "Epoch 0 num_samples 18300 loss 1.4414364729678761\n",
      "Epoch 0 num_samples 18400 loss 1.4464931015510978\n",
      "Epoch 0 num_samples 18500 loss 1.917462293737957\n",
      "Epoch 1 num_samples 0 loss 1.3250120965854466\n",
      "Epoch 1 num_samples 100 loss 1.746600674784065\n",
      "Epoch 1 num_samples 200 loss 1.0851016131047506\n",
      "Epoch 1 num_samples 300 loss 1.4212605833793968\n",
      "Epoch 1 num_samples 400 loss 1.0975893008537077\n",
      "Epoch 1 num_samples 500 loss 1.3515418127138343\n",
      "Epoch 1 num_samples 600 loss 1.9417850985424838\n",
      "Epoch 1 num_samples 700 loss 1.1218345650031305\n",
      "Epoch 1 num_samples 800 loss 1.4911845137304778\n",
      "Epoch 1 num_samples 900 loss 1.4129444361666128\n",
      "Epoch 1 num_samples 1000 loss 1.2426698879584699\n",
      "Epoch 1 num_samples 1100 loss 1.3828910614620977\n",
      "Epoch 1 num_samples 1200 loss 1.4640321330478525\n",
      "Epoch 1 num_samples 1300 loss 1.1904020697534832\n",
      "Epoch 1 num_samples 1400 loss 1.2395633284775935\n",
      "Epoch 1 num_samples 1500 loss 1.6811331446119309\n",
      "Epoch 1 num_samples 1600 loss 1.3259952818949654\n",
      "Epoch 1 num_samples 1700 loss 1.711513896466942\n",
      "Epoch 1 num_samples 1800 loss 1.4078761786607352\n",
      "Epoch 1 num_samples 1900 loss 1.6326550241017121\n",
      "Epoch 1 num_samples 2000 loss 1.1344721353885603\n",
      "Epoch 1 num_samples 2100 loss 1.3535208117501083\n",
      "Epoch 1 num_samples 2200 loss 1.521683707186928\n",
      "Epoch 1 num_samples 2300 loss 1.2812015851908853\n",
      "Epoch 1 num_samples 2400 loss 1.2708148594722561\n",
      "Epoch 1 num_samples 2500 loss 1.273826014898459\n",
      "Epoch 1 num_samples 2600 loss 1.1886075474763624\n",
      "Epoch 1 num_samples 2700 loss 1.4894151947758036\n",
      "Epoch 1 num_samples 2800 loss 1.2602035680141355\n",
      "Epoch 1 num_samples 2900 loss 1.2449831953152883\n",
      "Epoch 1 num_samples 3000 loss 1.6374686320551288\n",
      "Epoch 1 num_samples 3100 loss 1.2330158536925024\n",
      "Epoch 1 num_samples 3200 loss 1.2061632672043914\n",
      "Epoch 1 num_samples 3300 loss 1.3746181132884974\n",
      "Epoch 1 num_samples 3400 loss 1.1720502584823391\n",
      "Epoch 1 num_samples 3500 loss 1.0715011901579823\n",
      "Epoch 1 num_samples 3600 loss 1.259203496499969\n",
      "Epoch 1 num_samples 3700 loss 1.0257188850036527\n",
      "Epoch 1 num_samples 3800 loss 1.2688567458464595\n",
      "Epoch 1 num_samples 3900 loss 1.131193202962166\n",
      "Epoch 1 num_samples 4000 loss 1.4166731201199132\n",
      "Epoch 1 num_samples 4100 loss 0.9319870274127333\n",
      "Epoch 1 num_samples 4200 loss 1.1593899627306956\n",
      "Epoch 1 num_samples 4300 loss 1.2483309176257418\n",
      "Epoch 1 num_samples 4400 loss 1.2400878656944738\n",
      "Epoch 1 num_samples 4500 loss 1.5244659481178005\n",
      "Epoch 1 num_samples 4600 loss 1.407156450752766\n",
      "Epoch 1 num_samples 4700 loss 1.3784371833228584\n",
      "Epoch 1 num_samples 4800 loss 1.0646420288575456\n",
      "Epoch 1 num_samples 4900 loss 1.1513117845239182\n",
      "Epoch 1 num_samples 5000 loss 1.4188482397506343\n",
      "Epoch 1 num_samples 5100 loss 1.428863092774839\n",
      "Epoch 1 num_samples 5200 loss 1.2372820031857088\n",
      "Epoch 1 num_samples 5300 loss 1.3869865661464678\n",
      "Epoch 1 num_samples 5400 loss 1.5475585121228443\n",
      "Epoch 1 num_samples 5500 loss 1.3460262527870277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 num_samples 5600 loss 1.6676728845549156\n",
      "Epoch 1 num_samples 5700 loss 1.4839630858861073\n",
      "Epoch 1 num_samples 5800 loss 1.3207216568758295\n",
      "Epoch 1 num_samples 5900 loss 1.1588621165373225\n",
      "Epoch 1 num_samples 6000 loss 1.0243989809623566\n",
      "Epoch 1 num_samples 6100 loss 1.1810389035826099\n",
      "Epoch 1 num_samples 6200 loss 1.3274508799786944\n",
      "Epoch 1 num_samples 6300 loss 1.4384953650542838\n",
      "Epoch 1 num_samples 6400 loss 1.3313889840565363\n",
      "Epoch 1 num_samples 6500 loss 1.4728314343410032\n",
      "Epoch 1 num_samples 6600 loss 0.9463642249480503\n",
      "Epoch 1 num_samples 6700 loss 1.2757745959393514\n",
      "Epoch 1 num_samples 6800 loss 1.4057678242684375\n",
      "Epoch 1 num_samples 6900 loss 1.0567166659183584\n",
      "Epoch 1 num_samples 7000 loss 1.265610905486918\n",
      "Epoch 1 num_samples 7100 loss 1.416689417501393\n",
      "Epoch 1 num_samples 7200 loss 0.9498825387817841\n",
      "Epoch 1 num_samples 7300 loss 1.3333214745220416\n",
      "Epoch 1 num_samples 7400 loss 1.4465545948235203\n",
      "Epoch 1 num_samples 7500 loss 1.2549480206709058\n",
      "Epoch 1 num_samples 7600 loss 1.2407931973442394\n",
      "Epoch 1 num_samples 7700 loss 1.2928107530874389\n",
      "Epoch 1 num_samples 7800 loss 1.1348733686885497\n",
      "Epoch 1 num_samples 7900 loss 0.8071268973438432\n",
      "Epoch 1 num_samples 8000 loss 1.5250561710026331\n",
      "Epoch 1 num_samples 8100 loss 1.3189218776006215\n",
      "Epoch 1 num_samples 8200 loss 1.096463869139427\n",
      "Epoch 1 num_samples 8300 loss 1.1452986227760558\n",
      "Epoch 1 num_samples 8400 loss 1.2238635672311613\n",
      "Epoch 1 num_samples 8500 loss 1.1444792195987425\n",
      "Epoch 1 num_samples 8600 loss 1.2457798355400325\n",
      "Epoch 1 num_samples 8700 loss 1.306517383774173\n",
      "Epoch 1 num_samples 8800 loss 0.7434278393256838\n",
      "Epoch 1 num_samples 8900 loss 1.372297376442556\n",
      "Epoch 1 num_samples 9000 loss 1.1930156638166887\n",
      "Epoch 1 num_samples 9100 loss 1.3156637670073599\n",
      "Epoch 1 num_samples 9200 loss 1.3110398861779018\n",
      "Epoch 1 num_samples 9300 loss 0.9402649637087575\n",
      "Epoch 1 num_samples 9400 loss 1.2437436034439877\n",
      "Epoch 1 num_samples 9500 loss 1.2988206120936638\n",
      "Epoch 1 num_samples 9600 loss 0.9741221799448357\n",
      "Epoch 1 num_samples 9700 loss 1.2056134937523664\n",
      "Epoch 1 num_samples 9800 loss 1.0400293217798358\n",
      "Epoch 1 num_samples 9900 loss 1.3932226240706398\n",
      "Epoch 1 num_samples 10000 loss 1.0557755622417093\n",
      "Epoch 1 num_samples 10100 loss 1.4196726078915256\n",
      "Epoch 1 num_samples 10200 loss 1.1990154719062556\n",
      "Epoch 1 num_samples 10300 loss 0.9907564608143185\n",
      "Epoch 1 num_samples 10400 loss 1.1714893456364652\n",
      "Epoch 1 num_samples 10500 loss 1.0462908263023871\n",
      "Epoch 1 num_samples 10600 loss 1.3269159771619972\n",
      "Epoch 1 num_samples 10700 loss 1.2420481902834928\n",
      "Epoch 1 num_samples 10800 loss 1.1164384754657615\n",
      "Epoch 1 num_samples 10900 loss 1.284315212564343\n",
      "Epoch 1 num_samples 11000 loss 1.137153538587586\n",
      "Epoch 1 num_samples 11100 loss 0.9242047428058079\n",
      "Epoch 1 num_samples 11200 loss 1.5161453210963516\n",
      "Epoch 1 num_samples 11300 loss 1.334324514277956\n",
      "Epoch 1 num_samples 11400 loss 1.221484447841452\n",
      "Epoch 1 num_samples 11500 loss 1.2228516928316742\n",
      "Epoch 1 num_samples 11600 loss 1.106559592731957\n",
      "Epoch 1 num_samples 11700 loss 1.0724353103638347\n",
      "Epoch 1 num_samples 11800 loss 1.043045355660632\n",
      "Epoch 1 num_samples 11900 loss 0.8722313248193616\n",
      "Epoch 1 num_samples 12000 loss 1.4636133244533798\n",
      "Epoch 1 num_samples 12100 loss 1.0050802271665262\n",
      "Epoch 1 num_samples 12200 loss 1.0438197898125912\n",
      "Epoch 1 num_samples 12300 loss 1.1401673638847476\n",
      "Epoch 1 num_samples 12400 loss 1.0844465138368629\n",
      "Epoch 1 num_samples 12500 loss 1.1712790637015154\n",
      "Epoch 1 num_samples 12600 loss 1.030112737284744\n",
      "Epoch 1 num_samples 12700 loss 1.0316318445131276\n",
      "Epoch 1 num_samples 12800 loss 0.9510778814362506\n",
      "Epoch 1 num_samples 12900 loss 1.099516201006285\n",
      "Epoch 1 num_samples 13000 loss 0.9194046455657815\n",
      "Epoch 1 num_samples 13100 loss 0.8116156156339902\n",
      "Epoch 1 num_samples 13200 loss 1.157700986274537\n",
      "Epoch 1 num_samples 13300 loss 1.0552861467398968\n",
      "Epoch 1 num_samples 13400 loss 0.9011349261674758\n",
      "Epoch 1 num_samples 13500 loss 1.058028755970347\n",
      "Epoch 1 num_samples 13600 loss 0.8982120639991393\n",
      "Epoch 1 num_samples 13700 loss 1.2222559620218485\n",
      "Epoch 1 num_samples 13800 loss 1.1501349568807668\n",
      "Epoch 1 num_samples 13900 loss 1.2800255928118707\n",
      "Epoch 1 num_samples 14000 loss 1.0916515298959297\n",
      "Epoch 1 num_samples 14100 loss 1.0003760355746245\n",
      "Epoch 1 num_samples 14200 loss 1.2345061530104458\n",
      "Epoch 1 num_samples 14300 loss 1.2151591724156234\n",
      "Epoch 1 num_samples 14400 loss 1.076032368168054\n",
      "Epoch 1 num_samples 14500 loss 1.0612777880534991\n",
      "Epoch 1 num_samples 14600 loss 1.1929045024460896\n",
      "Epoch 1 num_samples 14700 loss 1.0227282844267647\n",
      "Epoch 1 num_samples 14800 loss 1.2540924360500028\n",
      "Epoch 1 num_samples 14900 loss 1.0755769445797498\n",
      "Epoch 1 num_samples 15000 loss 1.0328297850366388\n",
      "Epoch 1 num_samples 15100 loss 0.7974177530558311\n",
      "Epoch 1 num_samples 15200 loss 1.011607522408668\n",
      "Epoch 1 num_samples 15300 loss 1.0597138949681395\n",
      "Epoch 1 num_samples 15400 loss 0.9313622591492472\n",
      "Epoch 1 num_samples 15500 loss 0.9096414235789223\n",
      "Epoch 1 num_samples 15600 loss 1.1065303858732105\n",
      "Epoch 1 num_samples 15700 loss 1.106503021319532\n",
      "Epoch 1 num_samples 15800 loss 1.151454118981067\n",
      "Epoch 1 num_samples 15900 loss 1.2768664599711428\n",
      "Epoch 1 num_samples 16000 loss 1.0602219397925967\n",
      "Epoch 1 num_samples 16100 loss 1.056347279805855\n",
      "Epoch 1 num_samples 16200 loss 1.1337378839385692\n",
      "Epoch 1 num_samples 16300 loss 1.2852405640695173\n",
      "Epoch 1 num_samples 16400 loss 1.213644014990966\n",
      "Epoch 1 num_samples 16500 loss 1.279933763696448\n",
      "Epoch 1 num_samples 16600 loss 1.1153599940996808\n",
      "Epoch 1 num_samples 16700 loss 0.75602026127236\n",
      "Epoch 1 num_samples 16800 loss 1.0560026891951007\n",
      "Epoch 1 num_samples 16900 loss 1.0456685504011904\n",
      "Epoch 1 num_samples 17000 loss 1.2070508548812109\n",
      "Epoch 1 num_samples 17100 loss 0.9876784628850388\n",
      "Epoch 1 num_samples 17200 loss 0.994359840845528\n",
      "Epoch 1 num_samples 17300 loss 0.9309285769439335\n",
      "Epoch 1 num_samples 17400 loss 1.025556492953216\n",
      "Epoch 1 num_samples 17500 loss 0.8981920806093068\n",
      "Epoch 1 num_samples 17600 loss 0.9186446022712986\n",
      "Epoch 1 num_samples 17700 loss 1.2098391695062092\n",
      "Epoch 1 num_samples 17800 loss 0.9396986998145904\n",
      "Epoch 1 num_samples 17900 loss 0.9918561248108441\n",
      "Epoch 1 num_samples 18000 loss 0.8167909415087169\n",
      "Epoch 1 num_samples 18100 loss 0.9114203858299001\n",
      "Epoch 1 num_samples 18200 loss 0.7560812262153195\n",
      "Epoch 1 num_samples 18300 loss 0.9585510932051684\n",
      "Epoch 1 num_samples 18400 loss 1.0107035836082607\n",
      "Epoch 1 num_samples 18500 loss 1.1502349764975486\n",
      "Epoch 2 num_samples 0 loss 0.8672517250732846\n",
      "Epoch 2 num_samples 100 loss 1.0763682478260077\n",
      "Epoch 2 num_samples 200 loss 0.7543571691651357\n",
      "Epoch 2 num_samples 300 loss 0.8824365085117089\n",
      "Epoch 2 num_samples 400 loss 0.6827715666618989\n",
      "Epoch 2 num_samples 500 loss 0.808764451464175\n",
      "Epoch 2 num_samples 600 loss 1.3234661506889704\n",
      "Epoch 2 num_samples 700 loss 0.7226371390657624\n",
      "Epoch 2 num_samples 800 loss 0.9923329568607939\n",
      "Epoch 2 num_samples 900 loss 0.9297711012710141\n",
      "Epoch 2 num_samples 1000 loss 0.8100116326001517\n",
      "Epoch 2 num_samples 1100 loss 0.8265534640781311\n",
      "Epoch 2 num_samples 1200 loss 0.9730750259731872\n",
      "Epoch 2 num_samples 1300 loss 0.8217422269788835\n",
      "Epoch 2 num_samples 1400 loss 0.8237889829147546\n",
      "Epoch 2 num_samples 1500 loss 1.0757992585646103\n",
      "Epoch 2 num_samples 1600 loss 0.8651410633402054\n",
      "Epoch 2 num_samples 1700 loss 1.2379484188041368\n",
      "Epoch 2 num_samples 1800 loss 0.9787165049315537\n",
      "Epoch 2 num_samples 1900 loss 1.0844452443628214\n",
      "Epoch 2 num_samples 2000 loss 0.7032972547922048\n",
      "Epoch 2 num_samples 2100 loss 0.8470123823624367\n",
      "Epoch 2 num_samples 2200 loss 1.0065244876690413\n",
      "Epoch 2 num_samples 2300 loss 0.8429950795301763\n",
      "Epoch 2 num_samples 2400 loss 0.8289470758925863\n",
      "Epoch 2 num_samples 2500 loss 0.7818991941326925\n",
      "Epoch 2 num_samples 2600 loss 0.7846265797738928\n",
      "Epoch 2 num_samples 2700 loss 1.0433187286784378\n",
      "Epoch 2 num_samples 2800 loss 0.8107546988089296\n",
      "Epoch 2 num_samples 2900 loss 0.7663247322434589\n",
      "Epoch 2 num_samples 3000 loss 1.1318782031953802\n",
      "Epoch 2 num_samples 3100 loss 0.8673197774624744\n",
      "Epoch 2 num_samples 3200 loss 0.7754806507167635\n",
      "Epoch 2 num_samples 3300 loss 0.8949042361227407\n",
      "Epoch 2 num_samples 3400 loss 0.7680612441361777\n",
      "Epoch 2 num_samples 3500 loss 0.6685898744979465\n",
      "Epoch 2 num_samples 3600 loss 0.8652388629733042\n",
      "Epoch 2 num_samples 3700 loss 0.6841665963903324\n",
      "Epoch 2 num_samples 3800 loss 0.8263004162901574\n",
      "Epoch 2 num_samples 3900 loss 0.7095581389164372\n",
      "Epoch 2 num_samples 4000 loss 0.9160765731629258\n",
      "Epoch 2 num_samples 4100 loss 0.5975655760044192\n",
      "Epoch 2 num_samples 4200 loss 0.7527098799748322\n",
      "Epoch 2 num_samples 4300 loss 0.8380006859163296\n",
      "Epoch 2 num_samples 4400 loss 0.8001198805684737\n",
      "Epoch 2 num_samples 4500 loss 1.096902977094008\n",
      "Epoch 2 num_samples 4600 loss 0.8907097578308912\n",
      "Epoch 2 num_samples 4700 loss 0.8555488375601612\n",
      "Epoch 2 num_samples 4800 loss 0.7521245194365713\n",
      "Epoch 2 num_samples 4900 loss 0.8961194538755407\n",
      "Epoch 2 num_samples 5000 loss 1.0254043307183758\n",
      "Epoch 2 num_samples 5100 loss 1.0352779946099324\n",
      "Epoch 2 num_samples 5200 loss 0.7779623216909854\n",
      "Epoch 2 num_samples 5300 loss 0.9561449768469693\n",
      "Epoch 2 num_samples 5400 loss 1.053468537073057\n",
      "Epoch 2 num_samples 5500 loss 0.8447523118990026\n",
      "Epoch 2 num_samples 5600 loss 1.147997175584345\n",
      "Epoch 2 num_samples 5700 loss 1.0021519104986332\n",
      "Epoch 2 num_samples 5800 loss 0.8812433225173979\n",
      "Epoch 2 num_samples 5900 loss 0.8054021959791612\n",
      "Epoch 2 num_samples 6000 loss 0.6616304713751976\n",
      "Epoch 2 num_samples 6100 loss 0.8521064375648131\n",
      "Epoch 2 num_samples 6200 loss 0.9471394734689743\n",
      "Epoch 2 num_samples 6300 loss 1.0097513803218816\n",
      "Epoch 2 num_samples 6400 loss 0.8724699432119373\n",
      "Epoch 2 num_samples 6500 loss 0.9841987621734741\n",
      "Epoch 2 num_samples 6600 loss 0.6248791962995193\n",
      "Epoch 2 num_samples 6700 loss 0.8995812085269174\n",
      "Epoch 2 num_samples 6800 loss 0.9291338756726697\n",
      "Epoch 2 num_samples 6900 loss 0.7266930825623877\n",
      "Epoch 2 num_samples 7000 loss 0.8794972889564465\n",
      "Epoch 2 num_samples 7100 loss 0.9864706780590626\n",
      "Epoch 2 num_samples 7200 loss 0.6055561581642857\n",
      "Epoch 2 num_samples 7300 loss 0.9494887015719652\n",
      "Epoch 2 num_samples 7400 loss 0.9503518123968567\n",
      "Epoch 2 num_samples 7500 loss 0.8418389076653543\n",
      "Epoch 2 num_samples 7600 loss 0.7925891276457883\n",
      "Epoch 2 num_samples 7700 loss 0.9154934940458989\n",
      "Epoch 2 num_samples 7800 loss 0.7688550722137671\n",
      "Epoch 2 num_samples 7900 loss 0.5328951378123973\n",
      "Epoch 2 num_samples 8000 loss 1.1334455677456237\n",
      "Epoch 2 num_samples 8100 loss 0.91575979974555\n",
      "Epoch 2 num_samples 8200 loss 0.7984069609050352\n",
      "Epoch 2 num_samples 8300 loss 0.7888400667433038\n",
      "Epoch 2 num_samples 8400 loss 0.882027842029875\n",
      "Epoch 2 num_samples 8500 loss 0.8401635584223268\n",
      "Epoch 2 num_samples 8600 loss 0.8475602771623063\n",
      "Epoch 2 num_samples 8700 loss 0.9712066062257259\n",
      "Epoch 2 num_samples 8800 loss 0.5197986269160105\n",
      "Epoch 2 num_samples 8900 loss 0.940770878661559\n",
      "Epoch 2 num_samples 9000 loss 0.7535401695344529\n",
      "Epoch 2 num_samples 9100 loss 0.9295858124093211\n",
      "Epoch 2 num_samples 9200 loss 0.9550624020925744\n",
      "Epoch 2 num_samples 9300 loss 0.7085621687573415\n",
      "Epoch 2 num_samples 9400 loss 0.8236304667462733\n",
      "Epoch 2 num_samples 9500 loss 0.9529752132073338\n",
      "Epoch 2 num_samples 9600 loss 0.6997481578445803\n",
      "Epoch 2 num_samples 9700 loss 0.9101517179765825\n",
      "Epoch 2 num_samples 9800 loss 0.6610372308205312\n",
      "Epoch 2 num_samples 9900 loss 0.9699754223380088\n",
      "Epoch 2 num_samples 10000 loss 0.7032762228432665\n",
      "Epoch 2 num_samples 10100 loss 1.0161261680273366\n",
      "Epoch 2 num_samples 10200 loss 0.8412540528146383\n",
      "Epoch 2 num_samples 10300 loss 0.683367597021265\n",
      "Epoch 2 num_samples 10400 loss 0.8574062200140343\n",
      "Epoch 2 num_samples 10500 loss 0.7792726243423672\n",
      "Epoch 2 num_samples 10600 loss 0.9409401493038465\n",
      "Epoch 2 num_samples 10700 loss 0.823751735923521\n",
      "Epoch 2 num_samples 10800 loss 0.7919879230616735\n",
      "Epoch 2 num_samples 10900 loss 0.9759061675077955\n",
      "Epoch 2 num_samples 11000 loss 0.8523022631337571\n",
      "Epoch 2 num_samples 11100 loss 0.6720592467462896\n",
      "Epoch 2 num_samples 11200 loss 1.1392697761921926\n",
      "Epoch 2 num_samples 11300 loss 0.968067620777214\n",
      "Epoch 2 num_samples 11400 loss 0.919951435181912\n",
      "Epoch 2 num_samples 11500 loss 0.9543485731255592\n",
      "Epoch 2 num_samples 11600 loss 0.8657329796978019\n",
      "Epoch 2 num_samples 11700 loss 0.6666596674803758\n",
      "Epoch 2 num_samples 11800 loss 0.7801617598824581\n",
      "Epoch 2 num_samples 11900 loss 0.6113522862996715\n",
      "Epoch 2 num_samples 12000 loss 1.0454373386766271\n",
      "Epoch 2 num_samples 12100 loss 0.6753165483504152\n",
      "Epoch 2 num_samples 12200 loss 0.7826113729245293\n",
      "Epoch 2 num_samples 12300 loss 0.8247198989012774\n",
      "Epoch 2 num_samples 12400 loss 0.8170755716147501\n",
      "Epoch 2 num_samples 12500 loss 0.7831450768792849\n",
      "Epoch 2 num_samples 12600 loss 0.7253606156398839\n",
      "Epoch 2 num_samples 12700 loss 0.7619427661546636\n",
      "Epoch 2 num_samples 12800 loss 0.6954262001004503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 num_samples 12900 loss 0.817296963062241\n",
      "Epoch 2 num_samples 13000 loss 0.6498124489251984\n",
      "Epoch 2 num_samples 13100 loss 0.5695833699292735\n",
      "Epoch 2 num_samples 13200 loss 0.9146561458429707\n",
      "Epoch 2 num_samples 13300 loss 0.7238350129366089\n",
      "Epoch 2 num_samples 13400 loss 0.6189412334996253\n",
      "Epoch 2 num_samples 13500 loss 0.7563048086384001\n",
      "Epoch 2 num_samples 13600 loss 0.6648727846883569\n",
      "Epoch 2 num_samples 13700 loss 0.8726822869358131\n",
      "Epoch 2 num_samples 13800 loss 0.9219394214736301\n",
      "Epoch 2 num_samples 13900 loss 0.9054895110881557\n",
      "Epoch 2 num_samples 14000 loss 0.794376841256998\n",
      "Epoch 2 num_samples 14100 loss 0.7262202579321889\n",
      "Epoch 2 num_samples 14200 loss 0.9417526833994718\n",
      "Epoch 2 num_samples 14300 loss 0.9575829327391646\n",
      "Epoch 2 num_samples 14400 loss 0.7434520602167276\n",
      "Epoch 2 num_samples 14500 loss 0.8139019682524213\n",
      "Epoch 2 num_samples 14600 loss 0.8593817073842467\n",
      "Epoch 2 num_samples 14700 loss 0.7484529639500218\n",
      "Epoch 2 num_samples 14800 loss 0.930560521835829\n",
      "Epoch 2 num_samples 14900 loss 0.788917862264562\n",
      "Epoch 2 num_samples 15000 loss 0.7836884504501184\n",
      "Epoch 2 num_samples 15100 loss 0.5312886728218609\n",
      "Epoch 2 num_samples 15200 loss 0.7821523520769733\n",
      "Epoch 2 num_samples 15300 loss 0.7746878295533954\n",
      "Epoch 2 num_samples 15400 loss 0.6677613290272052\n",
      "Epoch 2 num_samples 15500 loss 0.6577888618829784\n",
      "Epoch 2 num_samples 15600 loss 0.7940411520879808\n",
      "Epoch 2 num_samples 15700 loss 0.8147017973143165\n",
      "Epoch 2 num_samples 15800 loss 0.8333939826211227\n",
      "Epoch 2 num_samples 15900 loss 1.003141866028655\n",
      "Epoch 2 num_samples 16000 loss 0.8509536897413826\n",
      "Epoch 2 num_samples 16100 loss 0.8016473900152583\n",
      "Epoch 2 num_samples 16200 loss 0.8621215530680066\n",
      "Epoch 2 num_samples 16300 loss 0.9702058918435936\n",
      "Epoch 2 num_samples 16400 loss 0.9336626734273353\n",
      "Epoch 2 num_samples 16500 loss 0.9436728443818743\n",
      "Epoch 2 num_samples 16600 loss 0.8648380439614795\n",
      "Epoch 2 num_samples 16700 loss 0.5605405612199807\n",
      "Epoch 2 num_samples 16800 loss 0.8333180678887914\n",
      "Epoch 2 num_samples 16900 loss 0.8270898626938117\n",
      "Epoch 2 num_samples 17000 loss 0.9112775364001792\n",
      "Epoch 2 num_samples 17100 loss 0.7385482672724248\n",
      "Epoch 2 num_samples 17200 loss 0.7584121564064135\n",
      "Epoch 2 num_samples 17300 loss 0.7027464365661763\n",
      "Epoch 2 num_samples 17400 loss 0.7942404607073152\n",
      "Epoch 2 num_samples 17500 loss 0.6649802438312378\n",
      "Epoch 2 num_samples 17600 loss 0.7003827445459193\n",
      "Epoch 2 num_samples 17700 loss 0.854292814873982\n",
      "Epoch 2 num_samples 17800 loss 0.7052285672028984\n",
      "Epoch 2 num_samples 17900 loss 0.6970851171735646\n",
      "Epoch 2 num_samples 18000 loss 0.605188972002658\n",
      "Epoch 2 num_samples 18100 loss 0.7090754380141766\n",
      "Epoch 2 num_samples 18200 loss 0.5523534449217281\n",
      "Epoch 2 num_samples 18300 loss 0.7741065839398792\n",
      "Epoch 2 num_samples 18400 loss 0.8267257249072191\n",
      "Epoch 2 num_samples 18500 loss 0.8492735266607976\n",
      "Epoch 3 num_samples 0 loss 0.6782388909316573\n",
      "Epoch 3 num_samples 100 loss 0.7767846669420635\n",
      "Epoch 3 num_samples 200 loss 0.5960185102676206\n",
      "Epoch 3 num_samples 300 loss 0.6565321966888316\n",
      "Epoch 3 num_samples 400 loss 0.49433593123416714\n",
      "Epoch 3 num_samples 500 loss 0.5702233129590173\n",
      "Epoch 3 num_samples 600 loss 0.9794843693156488\n",
      "Epoch 3 num_samples 700 loss 0.5275717146453804\n",
      "Epoch 3 num_samples 800 loss 0.762946675557946\n",
      "Epoch 3 num_samples 900 loss 0.6972493457151618\n",
      "Epoch 3 num_samples 1000 loss 0.6042683624232429\n",
      "Epoch 3 num_samples 1100 loss 0.631713518220338\n",
      "Epoch 3 num_samples 1200 loss 0.7172485457920686\n",
      "Epoch 3 num_samples 1300 loss 0.646172847292139\n",
      "Epoch 3 num_samples 1400 loss 0.6287102066346053\n",
      "Epoch 3 num_samples 1500 loss 0.8356589506690727\n",
      "Epoch 3 num_samples 1600 loss 0.6674983481578852\n",
      "Epoch 3 num_samples 1700 loss 0.9899892061958182\n",
      "Epoch 3 num_samples 1800 loss 0.8058670078764851\n",
      "Epoch 3 num_samples 1900 loss 0.8495716000327279\n",
      "Epoch 3 num_samples 2000 loss 0.5612276644135078\n",
      "Epoch 3 num_samples 2100 loss 0.6547870214354933\n",
      "Epoch 3 num_samples 2200 loss 0.7652581155514738\n",
      "Epoch 3 num_samples 2300 loss 0.6175015628785053\n",
      "Epoch 3 num_samples 2400 loss 0.6207419558798358\n",
      "Epoch 3 num_samples 2500 loss 0.5887251541233403\n",
      "Epoch 3 num_samples 2600 loss 0.6252226086100902\n",
      "Epoch 3 num_samples 2700 loss 0.8320023724067204\n",
      "Epoch 3 num_samples 2800 loss 0.6023944097731189\n",
      "Epoch 3 num_samples 2900 loss 0.574878999737483\n",
      "Epoch 3 num_samples 3000 loss 0.8868720006372481\n",
      "Epoch 3 num_samples 3100 loss 0.6946972591604912\n",
      "Epoch 3 num_samples 3200 loss 0.6140409466197793\n",
      "Epoch 3 num_samples 3300 loss 0.6822021107526706\n",
      "Epoch 3 num_samples 3400 loss 0.5770626923083955\n",
      "Epoch 3 num_samples 3500 loss 0.49427714844581144\n",
      "Epoch 3 num_samples 3600 loss 0.6587518693997346\n",
      "Epoch 3 num_samples 3700 loss 0.5411337549317081\n",
      "Epoch 3 num_samples 3800 loss 0.6580564112312115\n",
      "Epoch 3 num_samples 3900 loss 0.5649084137050473\n",
      "Epoch 3 num_samples 4000 loss 0.6699102655389113\n",
      "Epoch 3 num_samples 4100 loss 0.48893639756541035\n",
      "Epoch 3 num_samples 4200 loss 0.5926819917782211\n",
      "Epoch 3 num_samples 4300 loss 0.6741421536040244\n",
      "Epoch 3 num_samples 4400 loss 0.5980982078681296\n",
      "Epoch 3 num_samples 4500 loss 0.8774564591709271\n",
      "Epoch 3 num_samples 4600 loss 0.6410482203583469\n",
      "Epoch 3 num_samples 4700 loss 0.6633828807426848\n",
      "Epoch 3 num_samples 4800 loss 0.5884573430068472\n",
      "Epoch 3 num_samples 4900 loss 0.7660618832871919\n",
      "Epoch 3 num_samples 5000 loss 0.817657824183494\n",
      "Epoch 3 num_samples 5100 loss 0.8454922015112065\n",
      "Epoch 3 num_samples 5200 loss 0.5927265029465024\n",
      "Epoch 3 num_samples 5300 loss 0.7489368107851084\n",
      "Epoch 3 num_samples 5400 loss 0.8118864998524805\n",
      "Epoch 3 num_samples 5500 loss 0.6290773335292925\n",
      "Epoch 3 num_samples 5600 loss 0.8744866080300295\n",
      "Epoch 3 num_samples 5700 loss 0.7646320887930497\n",
      "Epoch 3 num_samples 5800 loss 0.6649513886048051\n",
      "Epoch 3 num_samples 5900 loss 0.6361040917146897\n",
      "Epoch 3 num_samples 6000 loss 0.5088703318458857\n",
      "Epoch 3 num_samples 6100 loss 0.6857324934434003\n",
      "Epoch 3 num_samples 6200 loss 0.7708047141056449\n",
      "Epoch 3 num_samples 6300 loss 0.8194376640467812\n",
      "Epoch 3 num_samples 6400 loss 0.649562339597602\n",
      "Epoch 3 num_samples 6500 loss 0.7874784417047505\n",
      "Epoch 3 num_samples 6600 loss 0.4828301619722677\n",
      "Epoch 3 num_samples 6700 loss 0.7307220460349798\n",
      "Epoch 3 num_samples 6800 loss 0.7157948806176995\n",
      "Epoch 3 num_samples 6900 loss 0.5873916716448001\n",
      "Epoch 3 num_samples 7000 loss 0.6878541958343493\n",
      "Epoch 3 num_samples 7100 loss 0.741813717306701\n",
      "Epoch 3 num_samples 7200 loss 0.43502216350014467\n",
      "Epoch 3 num_samples 7300 loss 0.7710746953616535\n",
      "Epoch 3 num_samples 7400 loss 0.7395780990553669\n",
      "Epoch 3 num_samples 7500 loss 0.6556219554228381\n",
      "Epoch 3 num_samples 7600 loss 0.5992569854299563\n",
      "Epoch 3 num_samples 7700 loss 0.7351032519619067\n",
      "Epoch 3 num_samples 7800 loss 0.6207651722375193\n",
      "Epoch 3 num_samples 7900 loss 0.4090739483373608\n",
      "Epoch 3 num_samples 8000 loss 0.9127494917800822\n",
      "Epoch 3 num_samples 8100 loss 0.7219003329542951\n",
      "Epoch 3 num_samples 8200 loss 0.6350416148108852\n",
      "Epoch 3 num_samples 8300 loss 0.6221086208819346\n",
      "Epoch 3 num_samples 8400 loss 0.6823718052251593\n",
      "Epoch 3 num_samples 8500 loss 0.6699809807824813\n",
      "Epoch 3 num_samples 8600 loss 0.6661686563656605\n",
      "Epoch 3 num_samples 8700 loss 0.7826126175177219\n",
      "Epoch 3 num_samples 8800 loss 0.41779382256961495\n",
      "Epoch 3 num_samples 8900 loss 0.7257759318707049\n",
      "Epoch 3 num_samples 9000 loss 0.5791234241589994\n",
      "Epoch 3 num_samples 9100 loss 0.715438964540954\n",
      "Epoch 3 num_samples 9200 loss 0.7636557803474651\n",
      "Epoch 3 num_samples 9300 loss 0.5865333736017806\n",
      "Epoch 3 num_samples 9400 loss 0.6301016366302018\n",
      "Epoch 3 num_samples 9500 loss 0.7466261186552672\n",
      "Epoch 3 num_samples 9600 loss 0.5604320725375103\n",
      "Epoch 3 num_samples 9700 loss 0.7218389580422141\n",
      "Epoch 3 num_samples 9800 loss 0.47856326424625834\n",
      "Epoch 3 num_samples 9900 loss 0.7568848134060482\n",
      "Epoch 3 num_samples 10000 loss 0.5266265345075299\n",
      "Epoch 3 num_samples 10100 loss 0.80026581164264\n",
      "Epoch 3 num_samples 10200 loss 0.639322637061174\n",
      "Epoch 3 num_samples 10300 loss 0.5251722794244578\n",
      "Epoch 3 num_samples 10400 loss 0.7125443870042845\n",
      "Epoch 3 num_samples 10500 loss 0.639451086888484\n",
      "Epoch 3 num_samples 10600 loss 0.7492296442369601\n",
      "Epoch 3 num_samples 10700 loss 0.6314443472571241\n",
      "Epoch 3 num_samples 10800 loss 0.6342772729334422\n",
      "Epoch 3 num_samples 10900 loss 0.8363859025673992\n",
      "Epoch 3 num_samples 11000 loss 0.7087844301602207\n",
      "Epoch 3 num_samples 11100 loss 0.549028606210563\n",
      "Epoch 3 num_samples 11200 loss 0.9408459787707738\n",
      "Epoch 3 num_samples 11300 loss 0.7717832218049925\n",
      "Epoch 3 num_samples 11400 loss 0.7226945927202791\n",
      "Epoch 3 num_samples 11500 loss 0.7862810113593238\n",
      "Epoch 3 num_samples 11600 loss 0.7243683123251445\n",
      "Epoch 3 num_samples 11700 loss 0.48590155594806794\n",
      "Epoch 3 num_samples 11800 loss 0.6365728890540018\n",
      "Epoch 3 num_samples 11900 loss 0.4987816846841864\n",
      "Epoch 3 num_samples 12000 loss 0.8069746423387898\n",
      "Epoch 3 num_samples 12100 loss 0.5428470763550065\n",
      "Epoch 3 num_samples 12200 loss 0.625876546408135\n",
      "Epoch 3 num_samples 12300 loss 0.6722305794850719\n",
      "Epoch 3 num_samples 12400 loss 0.6985541963407913\n",
      "Epoch 3 num_samples 12500 loss 0.6144914556913377\n",
      "Epoch 3 num_samples 12600 loss 0.568798540194424\n",
      "Epoch 3 num_samples 12700 loss 0.6292276171053894\n",
      "Epoch 3 num_samples 12800 loss 0.5697244944480814\n",
      "Epoch 3 num_samples 12900 loss 0.6540413701395448\n",
      "Epoch 3 num_samples 13000 loss 0.5195577588205936\n",
      "Epoch 3 num_samples 13100 loss 0.43481818680076567\n",
      "Epoch 3 num_samples 13200 loss 0.7799704715550412\n",
      "Epoch 3 num_samples 13300 loss 0.5354671816737357\n",
      "Epoch 3 num_samples 13400 loss 0.515317078957859\n",
      "Epoch 3 num_samples 13500 loss 0.5915523405823869\n",
      "Epoch 3 num_samples 13600 loss 0.5549477752162876\n",
      "Epoch 3 num_samples 13700 loss 0.6978100630653619\n",
      "Epoch 3 num_samples 13800 loss 0.7581009014112924\n",
      "Epoch 3 num_samples 13900 loss 0.7020504123503752\n",
      "Epoch 3 num_samples 14000 loss 0.6431947399044889\n",
      "Epoch 3 num_samples 14100 loss 0.5877285115072938\n",
      "Epoch 3 num_samples 14200 loss 0.7535537801285827\n",
      "Epoch 3 num_samples 14300 loss 0.8001643400609331\n",
      "Epoch 3 num_samples 14400 loss 0.6090517921469334\n",
      "Epoch 3 num_samples 14500 loss 0.6441383349177532\n",
      "Epoch 3 num_samples 14600 loss 0.7000065714897943\n",
      "Epoch 3 num_samples 14700 loss 0.6176838642020881\n",
      "Epoch 3 num_samples 14800 loss 0.752935470438148\n",
      "Epoch 3 num_samples 14900 loss 0.6181889177015383\n",
      "Epoch 3 num_samples 15000 loss 0.6424712027910688\n",
      "Epoch 3 num_samples 15100 loss 0.4178342682743616\n",
      "Epoch 3 num_samples 15200 loss 0.6675933925862236\n",
      "Epoch 3 num_samples 15300 loss 0.6115544849767445\n",
      "Epoch 3 num_samples 15400 loss 0.5348490859084865\n",
      "Epoch 3 num_samples 15500 loss 0.5190553147616882\n",
      "Epoch 3 num_samples 15600 loss 0.6402053102635418\n",
      "Epoch 3 num_samples 15700 loss 0.6393973471856298\n",
      "Epoch 3 num_samples 15800 loss 0.6686308674529002\n",
      "Epoch 3 num_samples 15900 loss 0.8516012695572194\n",
      "Epoch 3 num_samples 16000 loss 0.7395925754840339\n",
      "Epoch 3 num_samples 16100 loss 0.6429156505673421\n",
      "Epoch 3 num_samples 16200 loss 0.709205662854584\n",
      "Epoch 3 num_samples 16300 loss 0.7924019249272581\n",
      "Epoch 3 num_samples 16400 loss 0.7808526604350048\n",
      "Epoch 3 num_samples 16500 loss 0.7504630973183494\n",
      "Epoch 3 num_samples 16600 loss 0.7210052388744408\n",
      "Epoch 3 num_samples 16700 loss 0.4557544007556425\n",
      "Epoch 3 num_samples 16800 loss 0.6888190618610546\n",
      "Epoch 3 num_samples 16900 loss 0.6951714200461904\n",
      "Epoch 3 num_samples 17000 loss 0.7484259078925128\n",
      "Epoch 3 num_samples 17100 loss 0.6000658513265481\n",
      "Epoch 3 num_samples 17200 loss 0.614052220054782\n",
      "Epoch 3 num_samples 17300 loss 0.5731733563003601\n",
      "Epoch 3 num_samples 17400 loss 0.6542313244665345\n",
      "Epoch 3 num_samples 17500 loss 0.5494288040694663\n",
      "Epoch 3 num_samples 17600 loss 0.5787082556091597\n",
      "Epoch 3 num_samples 17700 loss 0.6502283461243613\n",
      "Epoch 3 num_samples 17800 loss 0.5772912613054773\n",
      "Epoch 3 num_samples 17900 loss 0.5531700104445791\n",
      "Epoch 3 num_samples 18000 loss 0.4915919645221409\n",
      "Epoch 3 num_samples 18100 loss 0.6062692866603718\n",
      "Epoch 3 num_samples 18200 loss 0.4436691972194349\n",
      "Epoch 3 num_samples 18300 loss 0.6727054296098021\n",
      "Epoch 3 num_samples 18400 loss 0.7050848762220618\n",
      "Epoch 3 num_samples 18500 loss 0.665250288116189\n",
      "Epoch 4 num_samples 0 loss 0.56584428954729\n",
      "Epoch 4 num_samples 100 loss 0.633460103402197\n",
      "Epoch 4 num_samples 200 loss 0.4949569050077773\n",
      "Epoch 4 num_samples 300 loss 0.5282378456917495\n",
      "Epoch 4 num_samples 400 loss 0.39841085524892383\n",
      "Epoch 4 num_samples 500 loss 0.44725548827792067\n",
      "Epoch 4 num_samples 600 loss 0.7979712708723682\n",
      "Epoch 4 num_samples 700 loss 0.43605217954384384\n",
      "Epoch 4 num_samples 800 loss 0.6322274940196488\n",
      "Epoch 4 num_samples 900 loss 0.5565734259324447\n",
      "Epoch 4 num_samples 1000 loss 0.48998534830968266\n",
      "Epoch 4 num_samples 1100 loss 0.5212084373463561\n",
      "Epoch 4 num_samples 1200 loss 0.5574738619113448\n",
      "Epoch 4 num_samples 1300 loss 0.545248081720377\n",
      "Epoch 4 num_samples 1400 loss 0.5003132741568487\n",
      "Epoch 4 num_samples 1500 loss 0.6972537087759497\n",
      "Epoch 4 num_samples 1600 loss 0.5540454222843582\n",
      "Epoch 4 num_samples 1700 loss 0.8290056119931927\n",
      "Epoch 4 num_samples 1800 loss 0.6906360084444255\n",
      "Epoch 4 num_samples 1900 loss 0.7260517605969687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 num_samples 2000 loss 0.4538780155078115\n",
      "Epoch 4 num_samples 2100 loss 0.5592947300697403\n",
      "Epoch 4 num_samples 2200 loss 0.6085620466481878\n",
      "Epoch 4 num_samples 2300 loss 0.49344872074687485\n",
      "Epoch 4 num_samples 2400 loss 0.4946414735328116\n",
      "Epoch 4 num_samples 2500 loss 0.4717013085272724\n",
      "Epoch 4 num_samples 2600 loss 0.5334099278309171\n",
      "Epoch 4 num_samples 2700 loss 0.7035613856473796\n",
      "Epoch 4 num_samples 2800 loss 0.4837245239447043\n",
      "Epoch 4 num_samples 2900 loss 0.45291557041270486\n",
      "Epoch 4 num_samples 3000 loss 0.7349207345227273\n",
      "Epoch 4 num_samples 3100 loss 0.5853577471085935\n",
      "Epoch 4 num_samples 3200 loss 0.5026187087353026\n",
      "Epoch 4 num_samples 3300 loss 0.5669949675186393\n",
      "Epoch 4 num_samples 3400 loss 0.4713554118124283\n",
      "Epoch 4 num_samples 3500 loss 0.40125901603999\n",
      "Epoch 4 num_samples 3600 loss 0.537007387044663\n",
      "Epoch 4 num_samples 3700 loss 0.4518539299546558\n",
      "Epoch 4 num_samples 3800 loss 0.5679478422227493\n",
      "Epoch 4 num_samples 3900 loss 0.47631976573357476\n",
      "Epoch 4 num_samples 4000 loss 0.5180899869356018\n",
      "Epoch 4 num_samples 4100 loss 0.4173282475459784\n",
      "Epoch 4 num_samples 4200 loss 0.489110602506533\n",
      "Epoch 4 num_samples 4300 loss 0.5737992269180484\n",
      "Epoch 4 num_samples 4400 loss 0.4905760976874284\n",
      "Epoch 4 num_samples 4500 loss 0.7352430166708314\n",
      "Epoch 4 num_samples 4600 loss 0.5001095615218429\n",
      "Epoch 4 num_samples 4700 loss 0.5739835369184678\n",
      "Epoch 4 num_samples 4800 loss 0.4743856349915728\n",
      "Epoch 4 num_samples 4900 loss 0.6726946721577031\n",
      "Epoch 4 num_samples 5000 loss 0.6791529831551089\n",
      "Epoch 4 num_samples 5100 loss 0.7234720498636393\n",
      "Epoch 4 num_samples 5200 loss 0.49731086944876607\n",
      "Epoch 4 num_samples 5300 loss 0.6113578046165193\n",
      "Epoch 4 num_samples 5400 loss 0.6630713291966424\n",
      "Epoch 4 num_samples 5500 loss 0.5116460249367628\n",
      "Epoch 4 num_samples 5600 loss 0.7257436809256439\n",
      "Epoch 4 num_samples 5700 loss 0.6370465803949726\n",
      "Epoch 4 num_samples 5800 loss 0.5386782143585725\n",
      "Epoch 4 num_samples 5900 loss 0.5274317944598846\n",
      "Epoch 4 num_samples 6000 loss 0.4231584682729973\n",
      "Epoch 4 num_samples 6100 loss 0.5681829412230475\n",
      "Epoch 4 num_samples 6200 loss 0.6557826631479908\n",
      "Epoch 4 num_samples 6300 loss 0.6919089997003234\n",
      "Epoch 4 num_samples 6400 loss 0.5277869181909507\n",
      "Epoch 4 num_samples 6500 loss 0.6547931983023838\n",
      "Epoch 4 num_samples 6600 loss 0.39976412674951733\n",
      "Epoch 4 num_samples 6700 loss 0.6287000664272844\n",
      "Epoch 4 num_samples 6800 loss 0.5955456759595736\n",
      "Epoch 4 num_samples 6900 loss 0.4822173669540855\n",
      "Epoch 4 num_samples 7000 loss 0.5560747592464341\n",
      "Epoch 4 num_samples 7100 loss 0.6007719598359144\n",
      "Epoch 4 num_samples 7200 loss 0.3425831558212113\n",
      "Epoch 4 num_samples 7300 loss 0.6521423021599424\n",
      "Epoch 4 num_samples 7400 loss 0.6067809541008685\n",
      "Epoch 4 num_samples 7500 loss 0.5413553910459263\n",
      "Epoch 4 num_samples 7600 loss 0.47672288804651786\n",
      "Epoch 4 num_samples 7700 loss 0.6161501872359526\n",
      "Epoch 4 num_samples 7800 loss 0.5178874040306376\n",
      "Epoch 4 num_samples 7900 loss 0.33390031696617206\n",
      "Epoch 4 num_samples 8000 loss 0.7523945568689407\n",
      "Epoch 4 num_samples 8100 loss 0.6212922257197501\n",
      "Epoch 4 num_samples 8200 loss 0.5227376137518921\n",
      "Epoch 4 num_samples 8300 loss 0.5175288724950421\n",
      "Epoch 4 num_samples 8400 loss 0.5518052149339\n",
      "Epoch 4 num_samples 8500 loss 0.5490492394718822\n",
      "Epoch 4 num_samples 8600 loss 0.570520081729255\n",
      "Epoch 4 num_samples 8700 loss 0.6768297022447781\n",
      "Epoch 4 num_samples 8800 loss 0.34233467189119937\n",
      "Epoch 4 num_samples 8900 loss 0.6092740152638106\n",
      "Epoch 4 num_samples 9000 loss 0.47020869987675906\n",
      "Epoch 4 num_samples 9100 loss 0.6002207970875029\n",
      "Epoch 4 num_samples 9200 loss 0.6232159266550868\n",
      "Epoch 4 num_samples 9300 loss 0.5030291412079717\n",
      "Epoch 4 num_samples 9400 loss 0.5116012482930912\n",
      "Epoch 4 num_samples 9500 loss 0.6199667565235306\n",
      "Epoch 4 num_samples 9600 loss 0.4798181412649912\n",
      "Epoch 4 num_samples 9700 loss 0.6131937380924366\n",
      "Epoch 4 num_samples 9800 loss 0.37291844820430897\n",
      "Epoch 4 num_samples 9900 loss 0.6499831661954719\n",
      "Epoch 4 num_samples 10000 loss 0.4145585126231491\n",
      "Epoch 4 num_samples 10100 loss 0.6624679108679657\n",
      "Epoch 4 num_samples 10200 loss 0.5319915568256431\n",
      "Epoch 4 num_samples 10300 loss 0.4227064966281994\n",
      "Epoch 4 num_samples 10400 loss 0.6142386938824501\n",
      "Epoch 4 num_samples 10500 loss 0.5464792290313152\n",
      "Epoch 4 num_samples 10600 loss 0.6415631364155682\n",
      "Epoch 4 num_samples 10700 loss 0.5186368635218507\n",
      "Epoch 4 num_samples 10800 loss 0.5237764363550635\n",
      "Epoch 4 num_samples 10900 loss 0.7461189194976856\n",
      "Epoch 4 num_samples 11000 loss 0.6004069082265735\n",
      "Epoch 4 num_samples 11100 loss 0.46725202773696795\n",
      "Epoch 4 num_samples 11200 loss 0.8093610315737993\n",
      "Epoch 4 num_samples 11300 loss 0.6337583431278883\n",
      "Epoch 4 num_samples 11400 loss 0.6020881279757812\n",
      "Epoch 4 num_samples 11500 loss 0.6739555590026671\n",
      "Epoch 4 num_samples 11600 loss 0.6181781567277771\n",
      "Epoch 4 num_samples 11700 loss 0.38561388416885906\n",
      "Epoch 4 num_samples 11800 loss 0.5335441252213059\n",
      "Epoch 4 num_samples 11900 loss 0.4334204216549988\n",
      "Epoch 4 num_samples 12000 loss 0.6576258246035116\n",
      "Epoch 4 num_samples 12100 loss 0.4635346790274778\n",
      "Epoch 4 num_samples 12200 loss 0.5180541288351086\n",
      "Epoch 4 num_samples 12300 loss 0.571046136803572\n",
      "Epoch 4 num_samples 12400 loss 0.6067444207895526\n",
      "Epoch 4 num_samples 12500 loss 0.5122058884763794\n",
      "Epoch 4 num_samples 12600 loss 0.4809968588332217\n",
      "Epoch 4 num_samples 12700 loss 0.5438109940472904\n",
      "Epoch 4 num_samples 12800 loss 0.48947266124003136\n",
      "Epoch 4 num_samples 12900 loss 0.5607120140328348\n",
      "Epoch 4 num_samples 13000 loss 0.44272751043489494\n",
      "Epoch 4 num_samples 13100 loss 0.3580600922845319\n",
      "Epoch 4 num_samples 13200 loss 0.6981342755555153\n",
      "Epoch 4 num_samples 13300 loss 0.42969140623938173\n",
      "Epoch 4 num_samples 13400 loss 0.45769929573407664\n",
      "Epoch 4 num_samples 13500 loss 0.47035337894158447\n",
      "Epoch 4 num_samples 13600 loss 0.4819859226231344\n",
      "Epoch 4 num_samples 13700 loss 0.589542655431505\n",
      "Epoch 4 num_samples 13800 loss 0.6423565843378546\n",
      "Epoch 4 num_samples 13900 loss 0.5834323146850771\n",
      "Epoch 4 num_samples 14000 loss 0.5479934871707931\n",
      "Epoch 4 num_samples 14100 loss 0.5032775305113613\n",
      "Epoch 4 num_samples 14200 loss 0.6229456153984282\n",
      "Epoch 4 num_samples 14300 loss 0.6885563238801042\n",
      "Epoch 4 num_samples 14400 loss 0.5153312316593318\n",
      "Epoch 4 num_samples 14500 loss 0.5118868865533641\n",
      "Epoch 4 num_samples 14600 loss 0.6015896797280061\n",
      "Epoch 4 num_samples 14700 loss 0.5348088206284645\n",
      "Epoch 4 num_samples 14800 loss 0.6237320624960482\n",
      "Epoch 4 num_samples 14900 loss 0.4957926343840461\n",
      "Epoch 4 num_samples 15000 loss 0.5406990511982438\n",
      "Epoch 4 num_samples 15100 loss 0.35214277295691465\n",
      "Epoch 4 num_samples 15200 loss 0.592356833000606\n",
      "Epoch 4 num_samples 15300 loss 0.5181453879233215\n",
      "Epoch 4 num_samples 15400 loss 0.43446232908921345\n",
      "Epoch 4 num_samples 15500 loss 0.42819425010566364\n",
      "Epoch 4 num_samples 15600 loss 0.5256398401733616\n",
      "Epoch 4 num_samples 15700 loss 0.5339123258579463\n",
      "Epoch 4 num_samples 15800 loss 0.555461897789229\n",
      "Epoch 4 num_samples 15900 loss 0.7405671447137633\n",
      "Epoch 4 num_samples 16000 loss 0.6532950824743886\n",
      "Epoch 4 num_samples 16100 loss 0.5343036406612114\n",
      "Epoch 4 num_samples 16200 loss 0.6159104265498548\n",
      "Epoch 4 num_samples 16300 loss 0.6752981923821999\n",
      "Epoch 4 num_samples 16400 loss 0.6896406905687225\n",
      "Epoch 4 num_samples 16500 loss 0.6218401002867502\n",
      "Epoch 4 num_samples 16600 loss 0.6248514644140374\n",
      "Epoch 4 num_samples 16700 loss 0.38965790889927104\n",
      "Epoch 4 num_samples 16800 loss 0.5884745988797119\n",
      "Epoch 4 num_samples 16900 loss 0.6048534622329064\n",
      "Epoch 4 num_samples 17000 loss 0.6317253996491025\n",
      "Epoch 4 num_samples 17100 loss 0.5192135042503527\n",
      "Epoch 4 num_samples 17200 loss 0.5118894438243595\n",
      "Epoch 4 num_samples 17300 loss 0.4715408274541465\n",
      "Epoch 4 num_samples 17400 loss 0.564559787702243\n",
      "Epoch 4 num_samples 17500 loss 0.4756252530290442\n",
      "Epoch 4 num_samples 17600 loss 0.5112603052060534\n",
      "Epoch 4 num_samples 17700 loss 0.5259887965475306\n",
      "Epoch 4 num_samples 17800 loss 0.49647055263832046\n",
      "Epoch 4 num_samples 17900 loss 0.4608367587408654\n",
      "Epoch 4 num_samples 18000 loss 0.4226208784677214\n",
      "Epoch 4 num_samples 18100 loss 0.5425418468142835\n",
      "Epoch 4 num_samples 18200 loss 0.37174221073483266\n",
      "Epoch 4 num_samples 18300 loss 0.5939154778067965\n",
      "Epoch 4 num_samples 18400 loss 0.6109710097935909\n",
      "Epoch 4 num_samples 18500 loss 0.5415423560847141\n",
      "Epoch 5 num_samples 0 loss 0.493980734938702\n",
      "Epoch 5 num_samples 100 loss 0.5444178312219023\n",
      "Epoch 5 num_samples 200 loss 0.43060259747489965\n",
      "Epoch 5 num_samples 300 loss 0.4360858270160506\n",
      "Epoch 5 num_samples 400 loss 0.33834922615905166\n",
      "Epoch 5 num_samples 500 loss 0.36916821427094654\n",
      "Epoch 5 num_samples 600 loss 0.6843624793577725\n",
      "Epoch 5 num_samples 700 loss 0.37623247441327684\n",
      "Epoch 5 num_samples 800 loss 0.5280137447633506\n",
      "Epoch 5 num_samples 900 loss 0.448485787853849\n",
      "Epoch 5 num_samples 1000 loss 0.42254858266630035\n",
      "Epoch 5 num_samples 1100 loss 0.4436413128389917\n",
      "Epoch 5 num_samples 1200 loss 0.45849148626449276\n",
      "Epoch 5 num_samples 1300 loss 0.4715724788324265\n",
      "Epoch 5 num_samples 1400 loss 0.42009650253735314\n",
      "Epoch 5 num_samples 1500 loss 0.6191155729381763\n",
      "Epoch 5 num_samples 1600 loss 0.4770654988382384\n",
      "Epoch 5 num_samples 1700 loss 0.7064861213692609\n",
      "Epoch 5 num_samples 1800 loss 0.5883631331603069\n",
      "Epoch 5 num_samples 1900 loss 0.6449487284723157\n",
      "Epoch 5 num_samples 2000 loss 0.3815253913564656\n",
      "Epoch 5 num_samples 2100 loss 0.4911125682982157\n",
      "Epoch 5 num_samples 2200 loss 0.5098181941041305\n",
      "Epoch 5 num_samples 2300 loss 0.4122076736447518\n",
      "Epoch 5 num_samples 2400 loss 0.4015116273072605\n",
      "Epoch 5 num_samples 2500 loss 0.3862670009709902\n",
      "Epoch 5 num_samples 2600 loss 0.470140274030446\n",
      "Epoch 5 num_samples 2700 loss 0.601524789646329\n",
      "Epoch 5 num_samples 2800 loss 0.40574359749295286\n",
      "Epoch 5 num_samples 2900 loss 0.36576217864617433\n",
      "Epoch 5 num_samples 3000 loss 0.6282819313947341\n",
      "Epoch 5 num_samples 3100 loss 0.5024797257053668\n",
      "Epoch 5 num_samples 3200 loss 0.4240837512362893\n",
      "Epoch 5 num_samples 3300 loss 0.49185038181797563\n",
      "Epoch 5 num_samples 3400 loss 0.40605065854197486\n",
      "Epoch 5 num_samples 3500 loss 0.3412603391898655\n",
      "Epoch 5 num_samples 3600 loss 0.46701439847501286\n",
      "Epoch 5 num_samples 3700 loss 0.3835087428670862\n",
      "Epoch 5 num_samples 3800 loss 0.5012248152429033\n",
      "Epoch 5 num_samples 3900 loss 0.41700414732564595\n",
      "Epoch 5 num_samples 4000 loss 0.4182001210128331\n",
      "Epoch 5 num_samples 4100 loss 0.37087392156258053\n",
      "Epoch 5 num_samples 4200 loss 0.41500144475426887\n",
      "Epoch 5 num_samples 4300 loss 0.5001898498522724\n",
      "Epoch 5 num_samples 4400 loss 0.4155342039310257\n",
      "Epoch 5 num_samples 4500 loss 0.6257024625348141\n",
      "Epoch 5 num_samples 4600 loss 0.4099974691441696\n",
      "Epoch 5 num_samples 4700 loss 0.5120577284035187\n",
      "Epoch 5 num_samples 4800 loss 0.39102968643386554\n",
      "Epoch 5 num_samples 4900 loss 0.5888518653804882\n",
      "Epoch 5 num_samples 5000 loss 0.5732011999001174\n",
      "Epoch 5 num_samples 5100 loss 0.6391212952900144\n",
      "Epoch 5 num_samples 5200 loss 0.42968537492471753\n",
      "Epoch 5 num_samples 5300 loss 0.5186266796103811\n",
      "Epoch 5 num_samples 5400 loss 0.5548356826950022\n",
      "Epoch 5 num_samples 5500 loss 0.4387084624176317\n",
      "Epoch 5 num_samples 5600 loss 0.6148404881411038\n",
      "Epoch 5 num_samples 5700 loss 0.5585301436901706\n",
      "Epoch 5 num_samples 5800 loss 0.4612146218240211\n",
      "Epoch 5 num_samples 5900 loss 0.4579348093827733\n",
      "Epoch 5 num_samples 6000 loss 0.37521098229822675\n",
      "Epoch 5 num_samples 6100 loss 0.484072028075786\n",
      "Epoch 5 num_samples 6200 loss 0.5723803489463392\n",
      "Epoch 5 num_samples 6300 loss 0.6013332468341187\n",
      "Epoch 5 num_samples 6400 loss 0.4527686474287231\n",
      "Epoch 5 num_samples 6500 loss 0.5525877030457975\n",
      "Epoch 5 num_samples 6600 loss 0.343762978495562\n",
      "Epoch 5 num_samples 6700 loss 0.5505081404557326\n",
      "Epoch 5 num_samples 6800 loss 0.523635258437979\n",
      "Epoch 5 num_samples 6900 loss 0.41659558405249947\n",
      "Epoch 5 num_samples 7000 loss 0.47148485228001824\n",
      "Epoch 5 num_samples 7100 loss 0.5015309531363278\n",
      "Epoch 5 num_samples 7200 loss 0.28278790641462154\n",
      "Epoch 5 num_samples 7300 loss 0.573677240726725\n",
      "Epoch 5 num_samples 7400 loss 0.5034760721138443\n",
      "Epoch 5 num_samples 7500 loss 0.4642379401156599\n",
      "Epoch 5 num_samples 7600 loss 0.39710958398001994\n",
      "Epoch 5 num_samples 7700 loss 0.5272051989904567\n",
      "Epoch 5 num_samples 7800 loss 0.44086144713018405\n",
      "Epoch 5 num_samples 7900 loss 0.28105648651596077\n",
      "Epoch 5 num_samples 8000 loss 0.6310646677939795\n",
      "Epoch 5 num_samples 8100 loss 0.5383650378243552\n",
      "Epoch 5 num_samples 8200 loss 0.44407573320916427\n",
      "Epoch 5 num_samples 8300 loss 0.43669982613739505\n",
      "Epoch 5 num_samples 8400 loss 0.4542121827496935\n",
      "Epoch 5 num_samples 8500 loss 0.46276117393699306\n",
      "Epoch 5 num_samples 8600 loss 0.4988114942069145\n",
      "Epoch 5 num_samples 8700 loss 0.5937883789065926\n",
      "Epoch 5 num_samples 8800 loss 0.29195127015828864\n",
      "Epoch 5 num_samples 8900 loss 0.5344171761893689\n",
      "Epoch 5 num_samples 9000 loss 0.40244847274712453\n",
      "Epoch 5 num_samples 9100 loss 0.5111935193528481\n",
      "Epoch 5 num_samples 9200 loss 0.5192738241921824\n",
      "Epoch 5 num_samples 9300 loss 0.4355423673454075\n",
      "Epoch 5 num_samples 9400 loss 0.43199646889635596\n",
      "Epoch 5 num_samples 9500 loss 0.5306675055400396\n",
      "Epoch 5 num_samples 9600 loss 0.4058820135566922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 num_samples 9700 loss 0.5373812904676835\n",
      "Epoch 5 num_samples 9800 loss 0.2986148509515574\n",
      "Epoch 5 num_samples 9900 loss 0.5683386761014836\n",
      "Epoch 5 num_samples 10000 loss 0.3444956093429419\n",
      "Epoch 5 num_samples 10100 loss 0.563923983176752\n",
      "Epoch 5 num_samples 10200 loss 0.45961473365087246\n",
      "Epoch 5 num_samples 10300 loss 0.3489713096725579\n",
      "Epoch 5 num_samples 10400 loss 0.5443055295018105\n",
      "Epoch 5 num_samples 10500 loss 0.4763285213754094\n",
      "Epoch 5 num_samples 10600 loss 0.5586356714480736\n",
      "Epoch 5 num_samples 10700 loss 0.46511396306660324\n",
      "Epoch 5 num_samples 10800 loss 0.4467565309926971\n",
      "Epoch 5 num_samples 10900 loss 0.6844192476274338\n",
      "Epoch 5 num_samples 11000 loss 0.5272150070133199\n",
      "Epoch 5 num_samples 11100 loss 0.4040822972748293\n",
      "Epoch 5 num_samples 11200 loss 0.7236160425740193\n",
      "Epoch 5 num_samples 11300 loss 0.5350332176160307\n",
      "Epoch 5 num_samples 11400 loss 0.5094877907336093\n",
      "Epoch 5 num_samples 11500 loss 0.5781348478370183\n",
      "Epoch 5 num_samples 11600 loss 0.536725263432917\n",
      "Epoch 5 num_samples 11700 loss 0.32440574720351906\n",
      "Epoch 5 num_samples 11800 loss 0.45789439970994467\n",
      "Epoch 5 num_samples 11900 loss 0.3796978085118299\n",
      "Epoch 5 num_samples 12000 loss 0.5520201146829782\n",
      "Epoch 5 num_samples 12100 loss 0.4031945478272691\n",
      "Epoch 5 num_samples 12200 loss 0.4389078619777754\n",
      "Epoch 5 num_samples 12300 loss 0.5113319182690943\n",
      "Epoch 5 num_samples 12400 loss 0.5431839450007147\n",
      "Epoch 5 num_samples 12500 loss 0.42926764272961065\n",
      "Epoch 5 num_samples 12600 loss 0.4130296506225645\n",
      "Epoch 5 num_samples 12700 loss 0.470403343327166\n",
      "Epoch 5 num_samples 12800 loss 0.42046056504063967\n",
      "Epoch 5 num_samples 12900 loss 0.5000507056918053\n",
      "Epoch 5 num_samples 13000 loss 0.3877292797320175\n",
      "Epoch 5 num_samples 13100 loss 0.3064512694693182\n",
      "Epoch 5 num_samples 13200 loss 0.6247537294411643\n",
      "Epoch 5 num_samples 13300 loss 0.36071842228119094\n",
      "Epoch 5 num_samples 13400 loss 0.4104684121693441\n",
      "Epoch 5 num_samples 13500 loss 0.38530009577126195\n",
      "Epoch 5 num_samples 13600 loss 0.43102235973675496\n",
      "Epoch 5 num_samples 13700 loss 0.5049049923171454\n",
      "Epoch 5 num_samples 13800 loss 0.557443520499535\n",
      "Epoch 5 num_samples 13900 loss 0.5044839709241813\n",
      "Epoch 5 num_samples 14000 loss 0.4827998830967124\n",
      "Epoch 5 num_samples 14100 loss 0.4303915915378526\n",
      "Epoch 5 num_samples 14200 loss 0.5288688910161768\n",
      "Epoch 5 num_samples 14300 loss 0.6093394859949914\n",
      "Epoch 5 num_samples 14400 loss 0.4474165033889727\n",
      "Epoch 5 num_samples 14500 loss 0.42091990746286684\n",
      "Epoch 5 num_samples 14600 loss 0.5374020820224825\n",
      "Epoch 5 num_samples 14700 loss 0.4717823323249658\n",
      "Epoch 5 num_samples 14800 loss 0.535182944843773\n",
      "Epoch 5 num_samples 14900 loss 0.41586010948478974\n",
      "Epoch 5 num_samples 15000 loss 0.47093737644741396\n",
      "Epoch 5 num_samples 15100 loss 0.3075333523335557\n",
      "Epoch 5 num_samples 15200 loss 0.5267328879861437\n",
      "Epoch 5 num_samples 15300 loss 0.4417570287187963\n",
      "Epoch 5 num_samples 15400 loss 0.35871398724402526\n",
      "Epoch 5 num_samples 15500 loss 0.36655310420776643\n",
      "Epoch 5 num_samples 15600 loss 0.4450978407519017\n",
      "Epoch 5 num_samples 15700 loss 0.4580818087649668\n",
      "Epoch 5 num_samples 15800 loss 0.47045933537570755\n",
      "Epoch 5 num_samples 15900 loss 0.6574075203282723\n",
      "Epoch 5 num_samples 16000 loss 0.5880629327867218\n",
      "Epoch 5 num_samples 16100 loss 0.4591203289474348\n",
      "Epoch 5 num_samples 16200 loss 0.5409374148396645\n",
      "Epoch 5 num_samples 16300 loss 0.5944788114833087\n",
      "Epoch 5 num_samples 16400 loss 0.6297959287267212\n",
      "Epoch 5 num_samples 16500 loss 0.52877524348995\n",
      "Epoch 5 num_samples 16600 loss 0.5589019154724677\n",
      "Epoch 5 num_samples 16700 loss 0.3436579545632917\n",
      "Epoch 5 num_samples 16800 loss 0.5059057060018859\n",
      "Epoch 5 num_samples 16900 loss 0.531394980830809\n",
      "Epoch 5 num_samples 17000 loss 0.5447728660448198\n",
      "Epoch 5 num_samples 17100 loss 0.46027390126497103\n",
      "Epoch 5 num_samples 17200 loss 0.4311927935387929\n",
      "Epoch 5 num_samples 17300 loss 0.3984340969880473\n",
      "Epoch 5 num_samples 17400 loss 0.49440472604392466\n",
      "Epoch 5 num_samples 17500 loss 0.4110124247314353\n",
      "Epoch 5 num_samples 17600 loss 0.4414824082157011\n",
      "Epoch 5 num_samples 17700 loss 0.44361420350863345\n",
      "Epoch 5 num_samples 17800 loss 0.434487184337064\n",
      "Epoch 5 num_samples 17900 loss 0.4039544240911692\n",
      "Epoch 5 num_samples 18000 loss 0.365007538907435\n",
      "Epoch 5 num_samples 18100 loss 0.4869604223814081\n",
      "Epoch 5 num_samples 18200 loss 0.31206276672844496\n",
      "Epoch 5 num_samples 18300 loss 0.5306262843207338\n",
      "Epoch 5 num_samples 18400 loss 0.5375585093781504\n",
      "Epoch 5 num_samples 18500 loss 0.4507862549630817\n",
      "Epoch 6 num_samples 0 loss 0.43705800264013306\n",
      "Epoch 6 num_samples 100 loss 0.48794594070283\n",
      "Epoch 6 num_samples 200 loss 0.38589703068044506\n",
      "Epoch 6 num_samples 300 loss 0.3770175237138939\n",
      "Epoch 6 num_samples 400 loss 0.2934759085760741\n",
      "Epoch 6 num_samples 500 loss 0.3119954485625485\n",
      "Epoch 6 num_samples 600 loss 0.6035058052949759\n",
      "Epoch 6 num_samples 700 loss 0.33731889358382333\n",
      "Epoch 6 num_samples 800 loss 0.44639726211400443\n",
      "Epoch 6 num_samples 900 loss 0.3707549172668273\n",
      "Epoch 6 num_samples 1000 loss 0.36794707950352945\n",
      "Epoch 6 num_samples 1100 loss 0.3859387105777323\n",
      "Epoch 6 num_samples 1200 loss 0.3847984568010962\n",
      "Epoch 6 num_samples 1300 loss 0.4228306775363953\n",
      "Epoch 6 num_samples 1400 loss 0.36496860572041834\n",
      "Epoch 6 num_samples 1500 loss 0.5639648653592427\n",
      "Epoch 6 num_samples 1600 loss 0.4174273000299845\n",
      "Epoch 6 num_samples 1700 loss 0.6134748521033453\n",
      "Epoch 6 num_samples 1800 loss 0.5099108775305747\n",
      "Epoch 6 num_samples 1900 loss 0.5720454382826223\n",
      "Epoch 6 num_samples 2000 loss 0.3292389667457644\n",
      "Epoch 6 num_samples 2100 loss 0.43997805504435483\n",
      "Epoch 6 num_samples 2200 loss 0.4323687440360092\n",
      "Epoch 6 num_samples 2300 loss 0.34355764428253566\n",
      "Epoch 6 num_samples 2400 loss 0.3333699479736873\n",
      "Epoch 6 num_samples 2500 loss 0.3236709738569492\n",
      "Epoch 6 num_samples 2600 loss 0.4133144672715674\n",
      "Epoch 6 num_samples 2700 loss 0.5167949850250835\n",
      "Epoch 6 num_samples 2800 loss 0.3506922730269699\n",
      "Epoch 6 num_samples 2900 loss 0.30533689028438393\n",
      "Epoch 6 num_samples 3000 loss 0.537670832956076\n",
      "Epoch 6 num_samples 3100 loss 0.433132945329919\n",
      "Epoch 6 num_samples 3200 loss 0.371497363457009\n",
      "Epoch 6 num_samples 3300 loss 0.4399022320581904\n",
      "Epoch 6 num_samples 3400 loss 0.3604484136384831\n",
      "Epoch 6 num_samples 3500 loss 0.3025634864974989\n",
      "Epoch 6 num_samples 3600 loss 0.4145184253412171\n",
      "Epoch 6 num_samples 3700 loss 0.32893009535732515\n",
      "Epoch 6 num_samples 3800 loss 0.45175003482494885\n",
      "Epoch 6 num_samples 3900 loss 0.37684667604977873\n",
      "Epoch 6 num_samples 4000 loss 0.350815607178988\n",
      "Epoch 6 num_samples 4100 loss 0.33153757911713255\n",
      "Epoch 6 num_samples 4200 loss 0.36029207032034577\n",
      "Epoch 6 num_samples 4300 loss 0.43893571214499016\n",
      "Epoch 6 num_samples 4400 loss 0.35879611331221783\n",
      "Epoch 6 num_samples 4500 loss 0.5353138119097124\n",
      "Epoch 6 num_samples 4600 loss 0.34730821379448074\n",
      "Epoch 6 num_samples 4700 loss 0.4538501076178031\n",
      "Epoch 6 num_samples 4800 loss 0.33291068241954946\n",
      "Epoch 6 num_samples 4900 loss 0.5131970296197443\n",
      "Epoch 6 num_samples 5000 loss 0.5074870681111705\n",
      "Epoch 6 num_samples 5100 loss 0.5647116043151498\n",
      "Epoch 6 num_samples 5200 loss 0.3817427642898372\n",
      "Epoch 6 num_samples 5300 loss 0.44710690655520724\n",
      "Epoch 6 num_samples 5400 loss 0.46975861479254133\n",
      "Epoch 6 num_samples 5500 loss 0.38324815148248154\n",
      "Epoch 6 num_samples 5600 loss 0.5397788591152426\n",
      "Epoch 6 num_samples 5700 loss 0.49864107310849404\n",
      "Epoch 6 num_samples 5800 loss 0.4053480836871978\n",
      "Epoch 6 num_samples 5900 loss 0.4021674744316006\n",
      "Epoch 6 num_samples 6000 loss 0.3378365803793146\n",
      "Epoch 6 num_samples 6100 loss 0.4153271177534716\n",
      "Epoch 6 num_samples 6200 loss 0.512784591047886\n",
      "Epoch 6 num_samples 6300 loss 0.531093275388403\n",
      "Epoch 6 num_samples 6400 loss 0.3949608781245\n",
      "Epoch 6 num_samples 6500 loss 0.4696877448405988\n",
      "Epoch 6 num_samples 6600 loss 0.3051732618166345\n",
      "Epoch 6 num_samples 6700 loss 0.4925684079119959\n",
      "Epoch 6 num_samples 6800 loss 0.46610802361189285\n",
      "Epoch 6 num_samples 6900 loss 0.3691808129058387\n",
      "Epoch 6 num_samples 7000 loss 0.3985094944444172\n",
      "Epoch 6 num_samples 7100 loss 0.42845595364000316\n",
      "Epoch 6 num_samples 7200 loss 0.23893995640589452\n",
      "Epoch 6 num_samples 7300 loss 0.5156860488206142\n",
      "Epoch 6 num_samples 7400 loss 0.4350796066108684\n",
      "Epoch 6 num_samples 7500 loss 0.3992462924513947\n",
      "Epoch 6 num_samples 7600 loss 0.32992389687533275\n",
      "Epoch 6 num_samples 7700 loss 0.46180288470475483\n",
      "Epoch 6 num_samples 7800 loss 0.3812927793150894\n",
      "Epoch 6 num_samples 7900 loss 0.24250152984126744\n",
      "Epoch 6 num_samples 8000 loss 0.5534306359565591\n",
      "Epoch 6 num_samples 8100 loss 0.47285450694537373\n",
      "Epoch 6 num_samples 8200 loss 0.38154171557990046\n",
      "Epoch 6 num_samples 8300 loss 0.3788341524858269\n",
      "Epoch 6 num_samples 8400 loss 0.37935219210700893\n",
      "Epoch 6 num_samples 8500 loss 0.40008521374081335\n",
      "Epoch 6 num_samples 8600 loss 0.43703878039729477\n",
      "Epoch 6 num_samples 8700 loss 0.5382148547866826\n",
      "Epoch 6 num_samples 8800 loss 0.25503857335941993\n",
      "Epoch 6 num_samples 8900 loss 0.47225743174895485\n",
      "Epoch 6 num_samples 9000 loss 0.35004984376817033\n",
      "Epoch 6 num_samples 9100 loss 0.448658564814184\n",
      "Epoch 6 num_samples 9200 loss 0.4463171480676562\n",
      "Epoch 6 num_samples 9300 loss 0.38268164916656316\n",
      "Epoch 6 num_samples 9400 loss 0.3676015101859163\n",
      "Epoch 6 num_samples 9500 loss 0.46604402100855546\n",
      "Epoch 6 num_samples 9600 loss 0.3520482338315857\n",
      "Epoch 6 num_samples 9700 loss 0.4853510579190122\n",
      "Epoch 6 num_samples 9800 loss 0.24655630810177612\n",
      "Epoch 6 num_samples 9900 loss 0.5051855877404339\n",
      "Epoch 6 num_samples 10000 loss 0.2925759680232523\n",
      "Epoch 6 num_samples 10100 loss 0.49621954818563535\n",
      "Epoch 6 num_samples 10200 loss 0.4069953426135656\n",
      "Epoch 6 num_samples 10300 loss 0.294334424945072\n",
      "Epoch 6 num_samples 10400 loss 0.4878674308456901\n",
      "Epoch 6 num_samples 10500 loss 0.4178467102266596\n",
      "Epoch 6 num_samples 10600 loss 0.49409382767791316\n",
      "Epoch 6 num_samples 10700 loss 0.4136745365501189\n",
      "Epoch 6 num_samples 10800 loss 0.3837420262198919\n",
      "Epoch 6 num_samples 10900 loss 0.6241076683121203\n",
      "Epoch 6 num_samples 11000 loss 0.47268958254911525\n",
      "Epoch 6 num_samples 11100 loss 0.34824464379011355\n",
      "Epoch 6 num_samples 11200 loss 0.6503328288639841\n",
      "Epoch 6 num_samples 11300 loss 0.45852436262905166\n",
      "Epoch 6 num_samples 11400 loss 0.4409074364002207\n",
      "Epoch 6 num_samples 11500 loss 0.5063116530918017\n",
      "Epoch 6 num_samples 11600 loss 0.46565747932053836\n",
      "Epoch 6 num_samples 11700 loss 0.2764625973567969\n",
      "Epoch 6 num_samples 11800 loss 0.38545779822843357\n",
      "Epoch 6 num_samples 11900 loss 0.33089483495203054\n",
      "Epoch 6 num_samples 12000 loss 0.4722906717683745\n",
      "Epoch 6 num_samples 12100 loss 0.3566864650304473\n",
      "Epoch 6 num_samples 12200 loss 0.3806419672042031\n",
      "Epoch 6 num_samples 12300 loss 0.4602890147646532\n",
      "Epoch 6 num_samples 12400 loss 0.489633603686186\n",
      "Epoch 6 num_samples 12500 loss 0.3713251332210213\n",
      "Epoch 6 num_samples 12600 loss 0.3630624942026472\n",
      "Epoch 6 num_samples 12700 loss 0.41774083698224557\n",
      "Epoch 6 num_samples 12800 loss 0.3575212498232845\n",
      "Epoch 6 num_samples 12900 loss 0.44968216063972233\n",
      "Epoch 6 num_samples 13000 loss 0.34242472042176153\n",
      "Epoch 6 num_samples 13100 loss 0.2659847464768967\n",
      "Epoch 6 num_samples 13200 loss 0.5649162914936493\n",
      "Epoch 6 num_samples 13300 loss 0.3109388065722547\n",
      "Epoch 6 num_samples 13400 loss 0.37028880116967655\n",
      "Epoch 6 num_samples 13500 loss 0.32177772520846815\n",
      "Epoch 6 num_samples 13600 loss 0.39302397261491606\n",
      "Epoch 6 num_samples 13700 loss 0.43992255720039686\n",
      "Epoch 6 num_samples 13800 loss 0.4900232669985192\n",
      "Epoch 6 num_samples 13900 loss 0.44405380958715823\n",
      "Epoch 6 num_samples 14000 loss 0.42691435618927637\n",
      "Epoch 6 num_samples 14100 loss 0.3852282633204868\n",
      "Epoch 6 num_samples 14200 loss 0.4575243569414035\n",
      "Epoch 6 num_samples 14300 loss 0.5323880792802376\n",
      "Epoch 6 num_samples 14400 loss 0.3983054514182685\n",
      "Epoch 6 num_samples 14500 loss 0.35054141057391375\n",
      "Epoch 6 num_samples 14600 loss 0.4887446910284311\n",
      "Epoch 6 num_samples 14700 loss 0.4244523452389684\n",
      "Epoch 6 num_samples 14800 loss 0.46967796846313703\n",
      "Epoch 6 num_samples 14900 loss 0.3485268907227389\n",
      "Epoch 6 num_samples 15000 loss 0.4159156478477098\n",
      "Epoch 6 num_samples 15100 loss 0.2777844474695663\n",
      "Epoch 6 num_samples 15200 loss 0.4817889826722751\n",
      "Epoch 6 num_samples 15300 loss 0.3776593079595162\n",
      "Epoch 6 num_samples 15400 loss 0.30277551110902673\n",
      "Epoch 6 num_samples 15500 loss 0.31910212447823477\n",
      "Epoch 6 num_samples 15600 loss 0.3832523071422215\n",
      "Epoch 6 num_samples 15700 loss 0.3956379855908805\n",
      "Epoch 6 num_samples 15800 loss 0.41215542220780316\n",
      "Epoch 6 num_samples 15900 loss 0.5868590498566535\n",
      "Epoch 6 num_samples 16000 loss 0.5292224286372275\n",
      "Epoch 6 num_samples 16100 loss 0.39485810074618044\n",
      "Epoch 6 num_samples 16200 loss 0.47783488641087574\n",
      "Epoch 6 num_samples 16300 loss 0.5335966107064414\n",
      "Epoch 6 num_samples 16400 loss 0.5757278922775161\n",
      "Epoch 6 num_samples 16500 loss 0.4619625236620024\n",
      "Epoch 6 num_samples 16600 loss 0.5059704460480756\n",
      "Epoch 6 num_samples 16700 loss 0.3046945736637939\n",
      "Epoch 6 num_samples 16800 loss 0.4524938185683014\n",
      "Epoch 6 num_samples 16900 loss 0.47230272709685683\n",
      "Epoch 6 num_samples 17000 loss 0.47769722271098003\n",
      "Epoch 6 num_samples 17100 loss 0.3978700994116977\n",
      "Epoch 6 num_samples 17200 loss 0.38195438794332154\n",
      "Epoch 6 num_samples 17300 loss 0.3430176679112249\n",
      "Epoch 6 num_samples 17400 loss 0.4298002535641403\n",
      "Epoch 6 num_samples 17500 loss 0.36259648134247047\n",
      "Epoch 6 num_samples 17600 loss 0.38531664446748676\n",
      "Epoch 6 num_samples 17700 loss 0.3819832516164816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 num_samples 17800 loss 0.3873287267803305\n",
      "Epoch 6 num_samples 17900 loss 0.36785725472436\n",
      "Epoch 6 num_samples 18000 loss 0.3227949845070159\n",
      "Epoch 6 num_samples 18100 loss 0.43550083524678945\n",
      "Epoch 6 num_samples 18200 loss 0.26844458451342695\n",
      "Epoch 6 num_samples 18300 loss 0.4692617447631702\n",
      "Epoch 6 num_samples 18400 loss 0.4781219250842973\n",
      "Epoch 6 num_samples 18500 loss 0.38614629500146125\n",
      "Epoch 7 num_samples 0 loss 0.4002826605251441\n",
      "Epoch 7 num_samples 100 loss 0.4453257801920135\n",
      "Epoch 7 num_samples 200 loss 0.34870116815307706\n",
      "Epoch 7 num_samples 300 loss 0.32257285392136664\n",
      "Epoch 7 num_samples 400 loss 0.2610056288521303\n",
      "Epoch 7 num_samples 500 loss 0.27440426990698563\n",
      "Epoch 7 num_samples 600 loss 0.5416437773327271\n",
      "Epoch 7 num_samples 700 loss 0.307945570129567\n",
      "Epoch 7 num_samples 800 loss 0.390701851839331\n",
      "Epoch 7 num_samples 900 loss 0.3143412736781708\n",
      "Epoch 7 num_samples 1000 loss 0.3262392742631012\n",
      "Epoch 7 num_samples 1100 loss 0.34055667925398736\n",
      "Epoch 7 num_samples 1200 loss 0.322036838573111\n",
      "Epoch 7 num_samples 1300 loss 0.38354755291956316\n",
      "Epoch 7 num_samples 1400 loss 0.32670450602431006\n",
      "Epoch 7 num_samples 1500 loss 0.5152535684010437\n",
      "Epoch 7 num_samples 1600 loss 0.37632973934403424\n",
      "Epoch 7 num_samples 1700 loss 0.5304984964991973\n",
      "Epoch 7 num_samples 1800 loss 0.4464991647799643\n",
      "Epoch 7 num_samples 1900 loss 0.5143966463788167\n",
      "Epoch 7 num_samples 2000 loss 0.2832698898796259\n",
      "Epoch 7 num_samples 2100 loss 0.39876195711978696\n",
      "Epoch 7 num_samples 2200 loss 0.3717831181652232\n",
      "Epoch 7 num_samples 2300 loss 0.2981632355280905\n",
      "Epoch 7 num_samples 2400 loss 0.2836767471097034\n",
      "Epoch 7 num_samples 2500 loss 0.27443704389731943\n",
      "Epoch 7 num_samples 2600 loss 0.37265701215845703\n",
      "Epoch 7 num_samples 2700 loss 0.4497993949916166\n",
      "Epoch 7 num_samples 2800 loss 0.30748515370203283\n",
      "Epoch 7 num_samples 2900 loss 0.2637292036143187\n",
      "Epoch 7 num_samples 3000 loss 0.46649138916721244\n",
      "Epoch 7 num_samples 3100 loss 0.3820863733387275\n",
      "Epoch 7 num_samples 3200 loss 0.32671075615730094\n",
      "Epoch 7 num_samples 3300 loss 0.3922706341411212\n",
      "Epoch 7 num_samples 3400 loss 0.3244039266227179\n",
      "Epoch 7 num_samples 3500 loss 0.270306267889153\n",
      "Epoch 7 num_samples 3600 loss 0.36814400978276046\n",
      "Epoch 7 num_samples 3700 loss 0.27804826051827797\n",
      "Epoch 7 num_samples 3800 loss 0.41105920442028493\n",
      "Epoch 7 num_samples 3900 loss 0.33873421418054406\n",
      "Epoch 7 num_samples 4000 loss 0.2975410559764706\n",
      "Epoch 7 num_samples 4100 loss 0.30054295785570245\n",
      "Epoch 7 num_samples 4200 loss 0.313046999856935\n",
      "Epoch 7 num_samples 4300 loss 0.3892324466071953\n",
      "Epoch 7 num_samples 4400 loss 0.31101625325081717\n",
      "Epoch 7 num_samples 4500 loss 0.4693417372383709\n",
      "Epoch 7 num_samples 4600 loss 0.29793482599867394\n",
      "Epoch 7 num_samples 4700 loss 0.4159136205169415\n",
      "Epoch 7 num_samples 4800 loss 0.29060348362516886\n",
      "Epoch 7 num_samples 4900 loss 0.45593211830325886\n",
      "Epoch 7 num_samples 5000 loss 0.4563937325334406\n",
      "Epoch 7 num_samples 5100 loss 0.5065918693651416\n",
      "Epoch 7 num_samples 5200 loss 0.3422357235127208\n",
      "Epoch 7 num_samples 5300 loss 0.3889901166711236\n",
      "Epoch 7 num_samples 5400 loss 0.40515069469565745\n",
      "Epoch 7 num_samples 5500 loss 0.3425559522188501\n",
      "Epoch 7 num_samples 5600 loss 0.47903655262810196\n",
      "Epoch 7 num_samples 5700 loss 0.4568499427446692\n",
      "Epoch 7 num_samples 5800 loss 0.358587838393406\n",
      "Epoch 7 num_samples 5900 loss 0.34993809980076196\n",
      "Epoch 7 num_samples 6000 loss 0.30787925120674386\n",
      "Epoch 7 num_samples 6100 loss 0.36347817515615755\n",
      "Epoch 7 num_samples 6200 loss 0.4605684925043751\n",
      "Epoch 7 num_samples 6300 loss 0.4731601614314717\n",
      "Epoch 7 num_samples 6400 loss 0.34785726725320915\n",
      "Epoch 7 num_samples 6500 loss 0.4130958082357939\n",
      "Epoch 7 num_samples 6600 loss 0.2751061737457189\n",
      "Epoch 7 num_samples 6700 loss 0.4485991814520598\n",
      "Epoch 7 num_samples 6800 loss 0.4108565472256251\n",
      "Epoch 7 num_samples 6900 loss 0.3365934775661885\n",
      "Epoch 7 num_samples 7000 loss 0.33977884402068176\n",
      "Epoch 7 num_samples 7100 loss 0.37386217075308237\n",
      "Epoch 7 num_samples 7200 loss 0.20565558284794405\n",
      "Epoch 7 num_samples 7300 loss 0.46744202434455473\n",
      "Epoch 7 num_samples 7400 loss 0.3888689076265483\n",
      "Epoch 7 num_samples 7500 loss 0.3490687413750311\n",
      "Epoch 7 num_samples 7600 loss 0.28725965995849156\n",
      "Epoch 7 num_samples 7700 loss 0.4060633909708931\n",
      "Epoch 7 num_samples 7800 loss 0.33473066984631883\n",
      "Epoch 7 num_samples 7900 loss 0.21118990002905655\n",
      "Epoch 7 num_samples 8000 loss 0.4905046235994181\n",
      "Epoch 7 num_samples 8100 loss 0.42178552889577253\n",
      "Epoch 7 num_samples 8200 loss 0.33362801699231914\n",
      "Epoch 7 num_samples 8300 loss 0.3294571168739376\n",
      "Epoch 7 num_samples 8400 loss 0.3226650410111682\n",
      "Epoch 7 num_samples 8500 loss 0.3516777975249403\n",
      "Epoch 7 num_samples 8600 loss 0.38886618701490155\n",
      "Epoch 7 num_samples 8700 loss 0.4888359964729675\n",
      "Epoch 7 num_samples 8800 loss 0.223030908040867\n",
      "Epoch 7 num_samples 8900 loss 0.421835898477132\n",
      "Epoch 7 num_samples 9000 loss 0.3069311207566198\n",
      "Epoch 7 num_samples 9100 loss 0.4009215936859754\n",
      "Epoch 7 num_samples 9200 loss 0.38646615329503\n",
      "Epoch 7 num_samples 9300 loss 0.34230300130811786\n",
      "Epoch 7 num_samples 9400 loss 0.32124721372317466\n",
      "Epoch 7 num_samples 9500 loss 0.40471653528004803\n",
      "Epoch 7 num_samples 9600 loss 0.31007428160841116\n",
      "Epoch 7 num_samples 9700 loss 0.4439080883301675\n",
      "Epoch 7 num_samples 9800 loss 0.20627708028706998\n",
      "Epoch 7 num_samples 9900 loss 0.45120837286924514\n",
      "Epoch 7 num_samples 10000 loss 0.24967405152463043\n",
      "Epoch 7 num_samples 10100 loss 0.44455881365165417\n",
      "Epoch 7 num_samples 10200 loss 0.36880766646940283\n",
      "Epoch 7 num_samples 10300 loss 0.25024447118505033\n",
      "Epoch 7 num_samples 10400 loss 0.4446945939819457\n",
      "Epoch 7 num_samples 10500 loss 0.3748808133569056\n",
      "Epoch 7 num_samples 10600 loss 0.4369769522124783\n",
      "Epoch 7 num_samples 10700 loss 0.3708459825779098\n",
      "Epoch 7 num_samples 10800 loss 0.32997017484422303\n",
      "Epoch 7 num_samples 10900 loss 0.5610114960501237\n",
      "Epoch 7 num_samples 11000 loss 0.42495090787030193\n",
      "Epoch 7 num_samples 11100 loss 0.30487360822624326\n",
      "Epoch 7 num_samples 11200 loss 0.5939870054985615\n",
      "Epoch 7 num_samples 11300 loss 0.39314549876449767\n",
      "Epoch 7 num_samples 11400 loss 0.3835999098603655\n",
      "Epoch 7 num_samples 11500 loss 0.44721863719695976\n",
      "Epoch 7 num_samples 11600 loss 0.4042751308862338\n",
      "Epoch 7 num_samples 11700 loss 0.23976527192334465\n",
      "Epoch 7 num_samples 11800 loss 0.33178832062296115\n",
      "Epoch 7 num_samples 11900 loss 0.2927109514793607\n",
      "Epoch 7 num_samples 12000 loss 0.41341531997123754\n",
      "Epoch 7 num_samples 12100 loss 0.3155907622971845\n",
      "Epoch 7 num_samples 12200 loss 0.3348018555980872\n",
      "Epoch 7 num_samples 12300 loss 0.41149833399472535\n",
      "Epoch 7 num_samples 12400 loss 0.43952746774650675\n",
      "Epoch 7 num_samples 12500 loss 0.3217314307361088\n",
      "Epoch 7 num_samples 12600 loss 0.3245976208389753\n",
      "Epoch 7 num_samples 12700 loss 0.3742124476592247\n",
      "Epoch 7 num_samples 12800 loss 0.30783645204065285\n",
      "Epoch 7 num_samples 12900 loss 0.4111223889448108\n",
      "Epoch 7 num_samples 13000 loss 0.30836884357211825\n",
      "Epoch 7 num_samples 13100 loss 0.23341685219527897\n",
      "Epoch 7 num_samples 13200 loss 0.5158674188922193\n",
      "Epoch 7 num_samples 13300 loss 0.27255163867307175\n",
      "Epoch 7 num_samples 13400 loss 0.3391950786077393\n",
      "Epoch 7 num_samples 13500 loss 0.27457835387913226\n",
      "Epoch 7 num_samples 13600 loss 0.35322289425105086\n",
      "Epoch 7 num_samples 13700 loss 0.3943819969351615\n",
      "Epoch 7 num_samples 13800 loss 0.43903230673192906\n",
      "Epoch 7 num_samples 13900 loss 0.4003966774185727\n",
      "Epoch 7 num_samples 14000 loss 0.37964805870910395\n",
      "Epoch 7 num_samples 14100 loss 0.346546746785842\n",
      "Epoch 7 num_samples 14200 loss 0.4036268199873493\n",
      "Epoch 7 num_samples 14300 loss 0.4668826143454502\n",
      "Epoch 7 num_samples 14400 loss 0.3499748493331915\n",
      "Epoch 7 num_samples 14500 loss 0.2992243988337028\n",
      "Epoch 7 num_samples 14600 loss 0.44764292448096943\n",
      "Epoch 7 num_samples 14700 loss 0.38795690951317496\n",
      "Epoch 7 num_samples 14800 loss 0.41932241908559237\n",
      "Epoch 7 num_samples 14900 loss 0.2926154592930485\n",
      "Epoch 7 num_samples 15000 loss 0.3645118785175201\n",
      "Epoch 7 num_samples 15100 loss 0.248695047821303\n",
      "Epoch 7 num_samples 15200 loss 0.44951387541151716\n",
      "Epoch 7 num_samples 15300 loss 0.33216839768964107\n",
      "Epoch 7 num_samples 15400 loss 0.26233473173524396\n",
      "Epoch 7 num_samples 15500 loss 0.2841397063978133\n",
      "Epoch 7 num_samples 15600 loss 0.33224263494399836\n",
      "Epoch 7 num_samples 15700 loss 0.3504731173072665\n",
      "Epoch 7 num_samples 15800 loss 0.3663860041092667\n",
      "Epoch 7 num_samples 15900 loss 0.5355970602460879\n",
      "Epoch 7 num_samples 16000 loss 0.47462307796962505\n",
      "Epoch 7 num_samples 16100 loss 0.3445271306803202\n",
      "Epoch 7 num_samples 16200 loss 0.42540024116812775\n",
      "Epoch 7 num_samples 16300 loss 0.4854529563816395\n",
      "Epoch 7 num_samples 16400 loss 0.5280103231158697\n",
      "Epoch 7 num_samples 16500 loss 0.41444904972378965\n",
      "Epoch 7 num_samples 16600 loss 0.45593836062070037\n",
      "Epoch 7 num_samples 16700 loss 0.2776183391408529\n",
      "Epoch 7 num_samples 16800 loss 0.39276116170450837\n",
      "Epoch 7 num_samples 16900 loss 0.41876457355834057\n",
      "Epoch 7 num_samples 17000 loss 0.4230591616990683\n",
      "Epoch 7 num_samples 17100 loss 0.34391614403341175\n",
      "Epoch 7 num_samples 17200 loss 0.3305012080791178\n",
      "Epoch 7 num_samples 17300 loss 0.3034631532631537\n",
      "Epoch 7 num_samples 17400 loss 0.3779341094131474\n",
      "Epoch 7 num_samples 17500 loss 0.3206098386791713\n",
      "Epoch 7 num_samples 17600 loss 0.3423090396945982\n",
      "Epoch 7 num_samples 17700 loss 0.34164209810602164\n",
      "Epoch 7 num_samples 17800 loss 0.3482427136863352\n",
      "Epoch 7 num_samples 17900 loss 0.3359750949260388\n",
      "Epoch 7 num_samples 18000 loss 0.2796791627365566\n",
      "Epoch 7 num_samples 18100 loss 0.3895247324827599\n",
      "Epoch 7 num_samples 18200 loss 0.23577384715564878\n",
      "Epoch 7 num_samples 18300 loss 0.4180279338339073\n",
      "Epoch 7 num_samples 18400 loss 0.42243906125417974\n",
      "Epoch 7 num_samples 18500 loss 0.33729651025080587\n",
      "Epoch 8 num_samples 0 loss 0.36797684402401026\n",
      "Epoch 8 num_samples 100 loss 0.4105395933583522\n",
      "Epoch 8 num_samples 200 loss 0.31894417395612473\n",
      "Epoch 8 num_samples 300 loss 0.2788153814221592\n",
      "Epoch 8 num_samples 400 loss 0.23471643630026962\n",
      "Epoch 8 num_samples 500 loss 0.24250360981577138\n",
      "Epoch 8 num_samples 600 loss 0.48902953597080306\n",
      "Epoch 8 num_samples 700 loss 0.28489609935652316\n",
      "Epoch 8 num_samples 800 loss 0.351868221532862\n",
      "Epoch 8 num_samples 900 loss 0.2742200936223606\n",
      "Epoch 8 num_samples 1000 loss 0.29316988250731535\n",
      "Epoch 8 num_samples 1100 loss 0.300043243783824\n",
      "Epoch 8 num_samples 1200 loss 0.27479356697691437\n",
      "Epoch 8 num_samples 1300 loss 0.36022838728626283\n",
      "Epoch 8 num_samples 1400 loss 0.29565161244145377\n",
      "Epoch 8 num_samples 1500 loss 0.4787781000103986\n",
      "Epoch 8 num_samples 1600 loss 0.3398503345794083\n",
      "Epoch 8 num_samples 1700 loss 0.45606519251644934\n",
      "Epoch 8 num_samples 1800 loss 0.3933104632540585\n",
      "Epoch 8 num_samples 1900 loss 0.4658922181505256\n",
      "Epoch 8 num_samples 2000 loss 0.255044023895755\n",
      "Epoch 8 num_samples 2100 loss 0.3634556703613427\n",
      "Epoch 8 num_samples 2200 loss 0.3209852298851385\n",
      "Epoch 8 num_samples 2300 loss 0.2589956916697604\n",
      "Epoch 8 num_samples 2400 loss 0.24596622303941607\n",
      "Epoch 8 num_samples 2500 loss 0.23039457856564\n",
      "Epoch 8 num_samples 2600 loss 0.333580102006825\n",
      "Epoch 8 num_samples 2700 loss 0.3873428810728632\n",
      "Epoch 8 num_samples 2800 loss 0.2732031812075703\n",
      "Epoch 8 num_samples 2900 loss 0.23456758755245943\n",
      "Epoch 8 num_samples 3000 loss 0.40610625168501413\n",
      "Epoch 8 num_samples 3100 loss 0.3412458557635963\n",
      "Epoch 8 num_samples 3200 loss 0.2952745572255996\n",
      "Epoch 8 num_samples 3300 loss 0.3513425720264353\n",
      "Epoch 8 num_samples 3400 loss 0.29031402153902514\n",
      "Epoch 8 num_samples 3500 loss 0.24423507995339733\n",
      "Epoch 8 num_samples 3600 loss 0.3290837838845986\n",
      "Epoch 8 num_samples 3700 loss 0.2337325105662022\n",
      "Epoch 8 num_samples 3800 loss 0.3677020195351719\n",
      "Epoch 8 num_samples 3900 loss 0.3056097436298015\n",
      "Epoch 8 num_samples 4000 loss 0.2585614314806032\n",
      "Epoch 8 num_samples 4100 loss 0.27416941421905067\n",
      "Epoch 8 num_samples 4200 loss 0.2777640281729157\n",
      "Epoch 8 num_samples 4300 loss 0.3431833818967233\n",
      "Epoch 8 num_samples 4400 loss 0.2785049658612429\n",
      "Epoch 8 num_samples 4500 loss 0.4121427937557332\n",
      "Epoch 8 num_samples 4600 loss 0.26318096291121296\n",
      "Epoch 8 num_samples 4700 loss 0.3816517858081333\n",
      "Epoch 8 num_samples 4800 loss 0.2573473775938273\n",
      "Epoch 8 num_samples 4900 loss 0.4016968931486963\n",
      "Epoch 8 num_samples 5000 loss 0.4144911905007112\n",
      "Epoch 8 num_samples 5100 loss 0.45232746912558425\n",
      "Epoch 8 num_samples 5200 loss 0.31319684593472347\n",
      "Epoch 8 num_samples 5300 loss 0.34443933018460754\n",
      "Epoch 8 num_samples 5400 loss 0.3527748023611666\n",
      "Epoch 8 num_samples 5500 loss 0.31507680440676444\n",
      "Epoch 8 num_samples 5600 loss 0.4260913568604586\n",
      "Epoch 8 num_samples 5700 loss 0.4152047120016034\n",
      "Epoch 8 num_samples 5800 loss 0.32154411729033766\n",
      "Epoch 8 num_samples 5900 loss 0.31282510348981857\n",
      "Epoch 8 num_samples 6000 loss 0.28495323878848305\n",
      "Epoch 8 num_samples 6100 loss 0.31787974287113946\n",
      "Epoch 8 num_samples 6200 loss 0.4204278813681209\n",
      "Epoch 8 num_samples 6300 loss 0.4315671748220316\n",
      "Epoch 8 num_samples 6400 loss 0.3063094168653917\n",
      "Epoch 8 num_samples 6500 loss 0.36776508508686895\n",
      "Epoch 8 num_samples 6600 loss 0.24497584015161725\n",
      "Epoch 8 num_samples 6700 loss 0.4126948335885514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 num_samples 6800 loss 0.36775024479479335\n",
      "Epoch 8 num_samples 6900 loss 0.3075787095124787\n",
      "Epoch 8 num_samples 7000 loss 0.2978017759131848\n",
      "Epoch 8 num_samples 7100 loss 0.3354149564748343\n",
      "Epoch 8 num_samples 7200 loss 0.1829519915212606\n",
      "Epoch 8 num_samples 7300 loss 0.42286649488978056\n",
      "Epoch 8 num_samples 7400 loss 0.34653149964918284\n",
      "Epoch 8 num_samples 7500 loss 0.3073899300586022\n",
      "Epoch 8 num_samples 7600 loss 0.2564944234354455\n",
      "Epoch 8 num_samples 7700 loss 0.36060931633674614\n",
      "Epoch 8 num_samples 7800 loss 0.30080135968180605\n",
      "Epoch 8 num_samples 7900 loss 0.1894889460526959\n",
      "Epoch 8 num_samples 8000 loss 0.43721417615137625\n",
      "Epoch 8 num_samples 8100 loss 0.3764628805333401\n",
      "Epoch 8 num_samples 8200 loss 0.29009960450135713\n",
      "Epoch 8 num_samples 8300 loss 0.2870524783846743\n",
      "Epoch 8 num_samples 8400 loss 0.2725862397711567\n",
      "Epoch 8 num_samples 8500 loss 0.31485032074753994\n",
      "Epoch 8 num_samples 8600 loss 0.35087813692967984\n",
      "Epoch 8 num_samples 8700 loss 0.44954012774836927\n",
      "Epoch 8 num_samples 8800 loss 0.19795396762618503\n",
      "Epoch 8 num_samples 8900 loss 0.38349489854689445\n",
      "Epoch 8 num_samples 9000 loss 0.2741516619475812\n",
      "Epoch 8 num_samples 9100 loss 0.36343919055105445\n",
      "Epoch 8 num_samples 9200 loss 0.33898353402568593\n",
      "Epoch 8 num_samples 9300 loss 0.3070960105110238\n",
      "Epoch 8 num_samples 9400 loss 0.28378521114507504\n",
      "Epoch 8 num_samples 9500 loss 0.35905797502453546\n",
      "Epoch 8 num_samples 9600 loss 0.2731760326488875\n",
      "Epoch 8 num_samples 9700 loss 0.4115825895082063\n",
      "Epoch 8 num_samples 9800 loss 0.1736128042304054\n",
      "Epoch 8 num_samples 9900 loss 0.4078996609391545\n",
      "Epoch 8 num_samples 10000 loss 0.21713480392395496\n",
      "Epoch 8 num_samples 10100 loss 0.40039619630605716\n",
      "Epoch 8 num_samples 10200 loss 0.3333416533237205\n",
      "Epoch 8 num_samples 10300 loss 0.21695470705287934\n",
      "Epoch 8 num_samples 10400 loss 0.4066281340289043\n",
      "Epoch 8 num_samples 10500 loss 0.3425023556307576\n",
      "Epoch 8 num_samples 10600 loss 0.38590944983446834\n",
      "Epoch 8 num_samples 10700 loss 0.3396504804643947\n",
      "Epoch 8 num_samples 10800 loss 0.2905104995630153\n",
      "Epoch 8 num_samples 10900 loss 0.5087826455432325\n",
      "Epoch 8 num_samples 11000 loss 0.3808733766222787\n",
      "Epoch 8 num_samples 11100 loss 0.2681909571234106\n",
      "Epoch 8 num_samples 11200 loss 0.5430077442683375\n",
      "Epoch 8 num_samples 11300 loss 0.3392761178960575\n",
      "Epoch 8 num_samples 11400 loss 0.3357101139119315\n",
      "Epoch 8 num_samples 11500 loss 0.39981390584726334\n",
      "Epoch 8 num_samples 11600 loss 0.35313041916460736\n",
      "Epoch 8 num_samples 11700 loss 0.21439123288083478\n",
      "Epoch 8 num_samples 11800 loss 0.28863494950757507\n",
      "Epoch 8 num_samples 11900 loss 0.25969564316567445\n",
      "Epoch 8 num_samples 12000 loss 0.3650599299323576\n",
      "Epoch 8 num_samples 12100 loss 0.28555230642034785\n",
      "Epoch 8 num_samples 12200 loss 0.295472282843167\n",
      "Epoch 8 num_samples 12300 loss 0.37309271744562394\n",
      "Epoch 8 num_samples 12400 loss 0.3924754645511709\n",
      "Epoch 8 num_samples 12500 loss 0.2830447862125751\n",
      "Epoch 8 num_samples 12600 loss 0.2928109299005351\n",
      "Epoch 8 num_samples 12700 loss 0.3368113876073328\n",
      "Epoch 8 num_samples 12800 loss 0.27340028284126516\n",
      "Epoch 8 num_samples 12900 loss 0.3836378920953325\n",
      "Epoch 8 num_samples 13000 loss 0.27639530213395047\n",
      "Epoch 8 num_samples 13100 loss 0.2073968363440984\n",
      "Epoch 8 num_samples 13200 loss 0.46396306814428656\n",
      "Epoch 8 num_samples 13300 loss 0.2447320095916227\n",
      "Epoch 8 num_samples 13400 loss 0.30820436968821746\n",
      "Epoch 8 num_samples 13500 loss 0.232945162027211\n",
      "Epoch 8 num_samples 13600 loss 0.3217651531155413\n",
      "Epoch 8 num_samples 13700 loss 0.35295801465783583\n",
      "Epoch 8 num_samples 13800 loss 0.39927700999778454\n",
      "Epoch 8 num_samples 13900 loss 0.3615311005912467\n",
      "Epoch 8 num_samples 14000 loss 0.33903999048694955\n",
      "Epoch 8 num_samples 14100 loss 0.31076564286775704\n",
      "Epoch 8 num_samples 14200 loss 0.3590397360204255\n",
      "Epoch 8 num_samples 14300 loss 0.4090044868080706\n",
      "Epoch 8 num_samples 14400 loss 0.30897221230918537\n",
      "Epoch 8 num_samples 14500 loss 0.2589412139752137\n",
      "Epoch 8 num_samples 14600 loss 0.41372740892205534\n",
      "Epoch 8 num_samples 14700 loss 0.35401497779147806\n",
      "Epoch 8 num_samples 14800 loss 0.3771677815376597\n",
      "Epoch 8 num_samples 14900 loss 0.24704959714931454\n",
      "Epoch 8 num_samples 15000 loss 0.32393047633885574\n",
      "Epoch 8 num_samples 15100 loss 0.22705689933155745\n",
      "Epoch 8 num_samples 15200 loss 0.41873103415321955\n",
      "Epoch 8 num_samples 15300 loss 0.29147872153345517\n",
      "Epoch 8 num_samples 15400 loss 0.22752712391652188\n",
      "Epoch 8 num_samples 15500 loss 0.24840740228119693\n",
      "Epoch 8 num_samples 15600 loss 0.28957682256315936\n",
      "Epoch 8 num_samples 15700 loss 0.31272766457797224\n",
      "Epoch 8 num_samples 15800 loss 0.3300207523342229\n",
      "Epoch 8 num_samples 15900 loss 0.4898486689875308\n",
      "Epoch 8 num_samples 16000 loss 0.42568249373397216\n",
      "Epoch 8 num_samples 16100 loss 0.2998672756885902\n",
      "Epoch 8 num_samples 16200 loss 0.3802245563590013\n",
      "Epoch 8 num_samples 16300 loss 0.4430024651635079\n",
      "Epoch 8 num_samples 16400 loss 0.4818453104350735\n",
      "Epoch 8 num_samples 16500 loss 0.3693239972613174\n",
      "Epoch 8 num_samples 16600 loss 0.4185264444182993\n",
      "Epoch 8 num_samples 16700 loss 0.25405615641296286\n",
      "Epoch 8 num_samples 16800 loss 0.349594128007723\n",
      "Epoch 8 num_samples 16900 loss 0.3694300865396023\n",
      "Epoch 8 num_samples 17000 loss 0.375127980603516\n",
      "Epoch 8 num_samples 17100 loss 0.2999899469414497\n",
      "Epoch 8 num_samples 17200 loss 0.29314199892746784\n",
      "Epoch 8 num_samples 17300 loss 0.2722642089240806\n",
      "Epoch 8 num_samples 17400 loss 0.3357162513930124\n",
      "Epoch 8 num_samples 17500 loss 0.28428926553204364\n",
      "Epoch 8 num_samples 17600 loss 0.30657630439031264\n",
      "Epoch 8 num_samples 17700 loss 0.30620742983187604\n",
      "Epoch 8 num_samples 17800 loss 0.310670574660478\n",
      "Epoch 8 num_samples 17900 loss 0.30699959822427475\n",
      "Epoch 8 num_samples 18000 loss 0.24333819367378418\n",
      "Epoch 8 num_samples 18100 loss 0.34953656303419467\n",
      "Epoch 8 num_samples 18200 loss 0.21050983216439215\n",
      "Epoch 8 num_samples 18300 loss 0.37255133826456466\n",
      "Epoch 8 num_samples 18400 loss 0.3791750033422883\n",
      "Epoch 8 num_samples 18500 loss 0.29669653941106644\n",
      "Epoch 9 num_samples 0 loss 0.33591850411958135\n",
      "Epoch 9 num_samples 100 loss 0.38232576099442256\n",
      "Epoch 9 num_samples 200 loss 0.29399414696918486\n",
      "Epoch 9 num_samples 300 loss 0.24683077715152155\n",
      "Epoch 9 num_samples 400 loss 0.21347404998569747\n",
      "Epoch 9 num_samples 500 loss 0.21903127043107962\n",
      "Epoch 9 num_samples 600 loss 0.4399982071908355\n",
      "Epoch 9 num_samples 700 loss 0.25979863033088785\n",
      "Epoch 9 num_samples 800 loss 0.3178811200745721\n",
      "Epoch 9 num_samples 900 loss 0.2444869062388221\n",
      "Epoch 9 num_samples 1000 loss 0.2639992599411366\n",
      "Epoch 9 num_samples 1100 loss 0.2665812440747586\n",
      "Epoch 9 num_samples 1200 loss 0.2412892395052515\n",
      "Epoch 9 num_samples 1300 loss 0.3346661088086803\n",
      "Epoch 9 num_samples 1400 loss 0.2724770587628111\n",
      "Epoch 9 num_samples 1500 loss 0.4481089649735756\n",
      "Epoch 9 num_samples 1600 loss 0.30555660660654604\n",
      "Epoch 9 num_samples 1700 loss 0.4050241027419466\n",
      "Epoch 9 num_samples 1800 loss 0.338989027399708\n",
      "Epoch 9 num_samples 1900 loss 0.4277552854776957\n",
      "Epoch 9 num_samples 2000 loss 0.23250556455056903\n",
      "Epoch 9 num_samples 2100 loss 0.33417560878502656\n",
      "Epoch 9 num_samples 2200 loss 0.2789789712676457\n",
      "Epoch 9 num_samples 2300 loss 0.22170301143412197\n",
      "Epoch 9 num_samples 2400 loss 0.210415657555354\n",
      "Epoch 9 num_samples 2500 loss 0.19698015476818423\n",
      "Epoch 9 num_samples 2600 loss 0.3019631734449654\n",
      "Epoch 9 num_samples 2700 loss 0.3423418057044034\n",
      "Epoch 9 num_samples 2800 loss 0.24233846689570168\n",
      "Epoch 9 num_samples 2900 loss 0.20939843062458086\n",
      "Epoch 9 num_samples 3000 loss 0.3564804392501479\n",
      "Epoch 9 num_samples 3100 loss 0.3038710769244255\n",
      "Epoch 9 num_samples 3200 loss 0.2701518369368066\n",
      "Epoch 9 num_samples 3300 loss 0.3142352667824452\n",
      "Epoch 9 num_samples 3400 loss 0.2631207392749866\n",
      "Epoch 9 num_samples 3500 loss 0.22277721664500538\n",
      "Epoch 9 num_samples 3600 loss 0.29112589269505657\n",
      "Epoch 9 num_samples 3700 loss 0.1994721086886042\n",
      "Epoch 9 num_samples 3800 loss 0.3326873620460307\n",
      "Epoch 9 num_samples 3900 loss 0.2771226621785383\n",
      "Epoch 9 num_samples 4000 loss 0.2235832863435706\n",
      "Epoch 9 num_samples 4100 loss 0.2546640469824187\n",
      "Epoch 9 num_samples 4200 loss 0.2410694362053932\n",
      "Epoch 9 num_samples 4300 loss 0.30564546586788943\n",
      "Epoch 9 num_samples 4400 loss 0.24812459496144623\n",
      "Epoch 9 num_samples 4500 loss 0.3694069111397116\n",
      "Epoch 9 num_samples 4600 loss 0.2328729931572257\n",
      "Epoch 9 num_samples 4700 loss 0.3514464438008541\n",
      "Epoch 9 num_samples 4800 loss 0.2286540254553298\n",
      "Epoch 9 num_samples 4900 loss 0.3598609819177287\n",
      "Epoch 9 num_samples 5000 loss 0.3786673149749627\n",
      "Epoch 9 num_samples 5100 loss 0.4048584433954456\n",
      "Epoch 9 num_samples 5200 loss 0.29065595363209445\n",
      "Epoch 9 num_samples 5300 loss 0.3104746293287908\n",
      "Epoch 9 num_samples 5400 loss 0.3102103133241412\n",
      "Epoch 9 num_samples 5500 loss 0.2890442676545997\n",
      "Epoch 9 num_samples 5600 loss 0.38478699273291156\n",
      "Epoch 9 num_samples 5700 loss 0.3770450768842713\n",
      "Epoch 9 num_samples 5800 loss 0.28866727178616963\n",
      "Epoch 9 num_samples 5900 loss 0.2802535353962132\n",
      "Epoch 9 num_samples 6000 loss 0.2627571089073257\n",
      "Epoch 9 num_samples 6100 loss 0.27976898160396724\n",
      "Epoch 9 num_samples 6200 loss 0.38382515242137655\n",
      "Epoch 9 num_samples 6300 loss 0.39430157620635625\n",
      "Epoch 9 num_samples 6400 loss 0.2791218455383466\n",
      "Epoch 9 num_samples 6500 loss 0.32790632480156906\n",
      "Epoch 9 num_samples 6600 loss 0.21948798116373525\n",
      "Epoch 9 num_samples 6700 loss 0.37924773540298573\n",
      "Epoch 9 num_samples 6800 loss 0.3285127544416001\n",
      "Epoch 9 num_samples 6900 loss 0.27736747286360985\n",
      "Epoch 9 num_samples 7000 loss 0.2627813849258432\n",
      "Epoch 9 num_samples 7100 loss 0.29769127907945253\n",
      "Epoch 9 num_samples 7200 loss 0.16184164302464146\n",
      "Epoch 9 num_samples 7300 loss 0.3870766386920838\n",
      "Epoch 9 num_samples 7400 loss 0.32007067943340706\n",
      "Epoch 9 num_samples 7500 loss 0.2690748318107353\n",
      "Epoch 9 num_samples 7600 loss 0.22898318663467043\n",
      "Epoch 9 num_samples 7700 loss 0.32149948786933985\n",
      "Epoch 9 num_samples 7800 loss 0.2731077183245647\n",
      "Epoch 9 num_samples 7900 loss 0.16974963446855504\n",
      "Epoch 9 num_samples 8000 loss 0.3890196656928384\n",
      "Epoch 9 num_samples 8100 loss 0.34133213118549405\n",
      "Epoch 9 num_samples 8200 loss 0.25137243858842395\n",
      "Epoch 9 num_samples 8300 loss 0.24882817201127827\n",
      "Epoch 9 num_samples 8400 loss 0.23632906345971447\n",
      "Epoch 9 num_samples 8500 loss 0.28553507528986743\n",
      "Epoch 9 num_samples 8600 loss 0.3203098666695706\n",
      "Epoch 9 num_samples 8700 loss 0.41567943901952176\n",
      "Epoch 9 num_samples 8800 loss 0.1782809450593373\n",
      "Epoch 9 num_samples 8900 loss 0.34826441955545767\n",
      "Epoch 9 num_samples 9000 loss 0.24226244430344462\n",
      "Epoch 9 num_samples 9100 loss 0.3338195932485731\n",
      "Epoch 9 num_samples 9200 loss 0.2996201923421345\n",
      "Epoch 9 num_samples 9300 loss 0.27736386938538127\n",
      "Epoch 9 num_samples 9400 loss 0.2550419384719574\n",
      "Epoch 9 num_samples 9500 loss 0.3203451661080572\n",
      "Epoch 9 num_samples 9600 loss 0.2433439466374994\n",
      "Epoch 9 num_samples 9700 loss 0.378562768621926\n",
      "Epoch 9 num_samples 9800 loss 0.1512030017585499\n",
      "Epoch 9 num_samples 9900 loss 0.37441364753040957\n",
      "Epoch 9 num_samples 10000 loss 0.19380962648085862\n",
      "Epoch 9 num_samples 10100 loss 0.3620664785640136\n",
      "Epoch 9 num_samples 10200 loss 0.3038208699778799\n",
      "Epoch 9 num_samples 10300 loss 0.18989597923389567\n",
      "Epoch 9 num_samples 10400 loss 0.3724856217563618\n",
      "Epoch 9 num_samples 10500 loss 0.3135800949753604\n",
      "Epoch 9 num_samples 10600 loss 0.34696857677393117\n",
      "Epoch 9 num_samples 10700 loss 0.3109048416405696\n",
      "Epoch 9 num_samples 10800 loss 0.2537260371158168\n",
      "Epoch 9 num_samples 10900 loss 0.4614522321546539\n",
      "Epoch 9 num_samples 11000 loss 0.3399732252293124\n",
      "Epoch 9 num_samples 11100 loss 0.2379347970008295\n",
      "Epoch 9 num_samples 11200 loss 0.4989514196620821\n",
      "Epoch 9 num_samples 11300 loss 0.2879540378535378\n",
      "Epoch 9 num_samples 11400 loss 0.2990059114253657\n",
      "Epoch 9 num_samples 11500 loss 0.35876608329465154\n",
      "Epoch 9 num_samples 11600 loss 0.3145529899867617\n",
      "Epoch 9 num_samples 11700 loss 0.19172224056866125\n",
      "Epoch 9 num_samples 11800 loss 0.25578186297419864\n",
      "Epoch 9 num_samples 11900 loss 0.23047367555265297\n",
      "Epoch 9 num_samples 12000 loss 0.3245131059236719\n",
      "Epoch 9 num_samples 12100 loss 0.25710638544611475\n",
      "Epoch 9 num_samples 12200 loss 0.26027050485751796\n",
      "Epoch 9 num_samples 12300 loss 0.34184071882153866\n",
      "Epoch 9 num_samples 12400 loss 0.35399123486348216\n",
      "Epoch 9 num_samples 12500 loss 0.2469413892425365\n",
      "Epoch 9 num_samples 12600 loss 0.2664008639767852\n",
      "Epoch 9 num_samples 12700 loss 0.30761431860624977\n",
      "Epoch 9 num_samples 12800 loss 0.24267142781382606\n",
      "Epoch 9 num_samples 12900 loss 0.3609868980106711\n",
      "Epoch 9 num_samples 13000 loss 0.24964889039314628\n",
      "Epoch 9 num_samples 13100 loss 0.19030539754786033\n",
      "Epoch 9 num_samples 13200 loss 0.4209394638403471\n",
      "Epoch 9 num_samples 13300 loss 0.22133239737199228\n",
      "Epoch 9 num_samples 13400 loss 0.28139331354711283\n",
      "Epoch 9 num_samples 13500 loss 0.20155941755046633\n",
      "Epoch 9 num_samples 13600 loss 0.289280615533612\n",
      "Epoch 9 num_samples 13700 loss 0.31394797296278604\n",
      "Epoch 9 num_samples 13800 loss 0.35887083977357853\n",
      "Epoch 9 num_samples 13900 loss 0.3288566153478496\n",
      "Epoch 9 num_samples 14000 loss 0.3048981390677001\n",
      "Epoch 9 num_samples 14100 loss 0.2826287174699752\n",
      "Epoch 9 num_samples 14200 loss 0.3251718386765672\n",
      "Epoch 9 num_samples 14300 loss 0.356982668479878\n",
      "Epoch 9 num_samples 14400 loss 0.27621354179443386\n",
      "Epoch 9 num_samples 14500 loss 0.22758488521182024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 num_samples 14600 loss 0.3817110497223426\n",
      "Epoch 9 num_samples 14700 loss 0.3238091259681235\n",
      "Epoch 9 num_samples 14800 loss 0.3428451536939577\n",
      "Epoch 9 num_samples 14900 loss 0.21206506210214784\n",
      "Epoch 9 num_samples 15000 loss 0.29169123477045544\n",
      "Epoch 9 num_samples 15100 loss 0.21096219599597646\n",
      "Epoch 9 num_samples 15200 loss 0.39365507756490736\n",
      "Epoch 9 num_samples 15300 loss 0.2546594147377004\n",
      "Epoch 9 num_samples 15400 loss 0.20358318210213394\n",
      "Epoch 9 num_samples 15500 loss 0.21611863226843245\n",
      "Epoch 9 num_samples 15600 loss 0.25373046061431537\n",
      "Epoch 9 num_samples 15700 loss 0.27990406122794637\n",
      "Epoch 9 num_samples 15800 loss 0.2935802855246574\n",
      "Epoch 9 num_samples 15900 loss 0.4454843112840288\n",
      "Epoch 9 num_samples 16000 loss 0.38716844052519994\n",
      "Epoch 9 num_samples 16100 loss 0.26456694729826064\n",
      "Epoch 9 num_samples 16200 loss 0.3370033911348348\n",
      "Epoch 9 num_samples 16300 loss 0.4035515313069567\n",
      "Epoch 9 num_samples 16400 loss 0.44217855896251657\n",
      "Epoch 9 num_samples 16500 loss 0.33301891259984784\n",
      "Epoch 9 num_samples 16600 loss 0.3794365248799289\n",
      "Epoch 9 num_samples 16700 loss 0.23692028176279034\n",
      "Epoch 9 num_samples 16800 loss 0.31527253993358106\n",
      "Epoch 9 num_samples 16900 loss 0.3285078160780979\n",
      "Epoch 9 num_samples 17000 loss 0.3399460572674207\n",
      "Epoch 9 num_samples 17100 loss 0.2633274050181671\n",
      "Epoch 9 num_samples 17200 loss 0.2614497576786989\n",
      "Epoch 9 num_samples 17300 loss 0.24667916977906515\n",
      "Epoch 9 num_samples 17400 loss 0.3049364323030219\n",
      "Epoch 9 num_samples 17500 loss 0.25391870684286544\n",
      "Epoch 9 num_samples 17600 loss 0.2723007249558515\n",
      "Epoch 9 num_samples 17700 loss 0.27941410762281005\n",
      "Epoch 9 num_samples 17800 loss 0.2784979289090122\n",
      "Epoch 9 num_samples 17900 loss 0.2858758558027877\n",
      "Epoch 9 num_samples 18000 loss 0.21303765826545226\n",
      "Epoch 9 num_samples 18100 loss 0.31567606557830125\n",
      "Epoch 9 num_samples 18200 loss 0.19095854530911807\n",
      "Epoch 9 num_samples 18300 loss 0.33497917165631463\n",
      "Epoch 9 num_samples 18400 loss 0.33525837632811945\n",
      "Epoch 9 num_samples 18500 loss 0.26122043396654304\n",
      "Epoch 10 num_samples 0 loss 0.31214948538677423\n",
      "Epoch 10 num_samples 100 loss 0.357363240304361\n",
      "Epoch 10 num_samples 200 loss 0.27048737950738677\n",
      "Epoch 10 num_samples 300 loss 0.22011713727272053\n",
      "Epoch 10 num_samples 400 loss 0.19860474036451148\n",
      "Epoch 10 num_samples 500 loss 0.2011014790838107\n",
      "Epoch 10 num_samples 600 loss 0.39325212400560117\n",
      "Epoch 10 num_samples 700 loss 0.24226148886425217\n",
      "Epoch 10 num_samples 800 loss 0.28713419403341206\n",
      "Epoch 10 num_samples 900 loss 0.2189623785122427\n",
      "Epoch 10 num_samples 1000 loss 0.24039896574145359\n",
      "Epoch 10 num_samples 1100 loss 0.23879375701275707\n",
      "Epoch 10 num_samples 1200 loss 0.20781685913464756\n",
      "Epoch 10 num_samples 1300 loss 0.3093691285745213\n",
      "Epoch 10 num_samples 1400 loss 0.2515617867217957\n",
      "Epoch 10 num_samples 1500 loss 0.41381935189701513\n",
      "Epoch 10 num_samples 1600 loss 0.2718307585024579\n",
      "Epoch 10 num_samples 1700 loss 0.36164363690757795\n",
      "Epoch 10 num_samples 1800 loss 0.2956458436458497\n",
      "Epoch 10 num_samples 1900 loss 0.3870049425721032\n",
      "Epoch 10 num_samples 2000 loss 0.21017033132466556\n",
      "Epoch 10 num_samples 2100 loss 0.30442140715256194\n",
      "Epoch 10 num_samples 2200 loss 0.24177866097706738\n",
      "Epoch 10 num_samples 2300 loss 0.19047156849084204\n",
      "Epoch 10 num_samples 2400 loss 0.1804376696917828\n",
      "Epoch 10 num_samples 2500 loss 0.16836453382408054\n",
      "Epoch 10 num_samples 2600 loss 0.2774350201990015\n",
      "Epoch 10 num_samples 2700 loss 0.3037194778610705\n",
      "Epoch 10 num_samples 2800 loss 0.21654877785130364\n",
      "Epoch 10 num_samples 2900 loss 0.19044689956903751\n",
      "Epoch 10 num_samples 3000 loss 0.3107114911237619\n",
      "Epoch 10 num_samples 3100 loss 0.268079023745404\n",
      "Epoch 10 num_samples 3200 loss 0.24952290664856475\n",
      "Epoch 10 num_samples 3300 loss 0.2817674769622517\n",
      "Epoch 10 num_samples 3400 loss 0.2354889349539505\n",
      "Epoch 10 num_samples 3500 loss 0.20284603214691152\n",
      "Epoch 10 num_samples 3600 loss 0.25828661569206945\n",
      "Epoch 10 num_samples 3700 loss 0.17481794793407032\n",
      "Epoch 10 num_samples 3800 loss 0.30341622964177156\n",
      "Epoch 10 num_samples 3900 loss 0.2532035697224485\n",
      "Epoch 10 num_samples 4000 loss 0.20076056911851217\n",
      "Epoch 10 num_samples 4100 loss 0.23389279036008886\n",
      "Epoch 10 num_samples 4200 loss 0.21120899849920718\n",
      "Epoch 10 num_samples 4300 loss 0.27218896407071497\n",
      "Epoch 10 num_samples 4400 loss 0.2191974439938096\n",
      "Epoch 10 num_samples 4500 loss 0.3273046629627349\n",
      "Epoch 10 num_samples 4600 loss 0.20877688908237055\n",
      "Epoch 10 num_samples 4700 loss 0.32564694277377165\n",
      "Epoch 10 num_samples 4800 loss 0.20524101274298176\n",
      "Epoch 10 num_samples 4900 loss 0.3206334180102472\n",
      "Epoch 10 num_samples 5000 loss 0.34231563226407485\n",
      "Epoch 10 num_samples 5100 loss 0.36210847862260387\n",
      "Epoch 10 num_samples 5200 loss 0.26999292974824246\n",
      "Epoch 10 num_samples 5300 loss 0.2808550585794857\n",
      "Epoch 10 num_samples 5400 loss 0.27030877076531024\n",
      "Epoch 10 num_samples 5500 loss 0.26891446316359263\n",
      "Epoch 10 num_samples 5600 loss 0.3544320862927097\n",
      "Epoch 10 num_samples 5700 loss 0.34427551419540153\n",
      "Epoch 10 num_samples 5800 loss 0.262930604439485\n",
      "Epoch 10 num_samples 5900 loss 0.2502732949828509\n",
      "Epoch 10 num_samples 6000 loss 0.24501269600068354\n",
      "Epoch 10 num_samples 6100 loss 0.2512466184196402\n",
      "Epoch 10 num_samples 6200 loss 0.3460074047931998\n",
      "Epoch 10 num_samples 6300 loss 0.35866689269881286\n",
      "Epoch 10 num_samples 6400 loss 0.2568365490243785\n",
      "Epoch 10 num_samples 6500 loss 0.29402795474312404\n",
      "Epoch 10 num_samples 6600 loss 0.19346710664841346\n",
      "Epoch 10 num_samples 6700 loss 0.35053785119610686\n",
      "Epoch 10 num_samples 6800 loss 0.29591573406985827\n",
      "Epoch 10 num_samples 6900 loss 0.25377788403176\n",
      "Epoch 10 num_samples 7000 loss 0.23603244341370214\n",
      "Epoch 10 num_samples 7100 loss 0.2691247426696322\n",
      "Epoch 10 num_samples 7200 loss 0.147639041437378\n",
      "Epoch 10 num_samples 7300 loss 0.3583696721650834\n",
      "Epoch 10 num_samples 7400 loss 0.29238733231465863\n",
      "Epoch 10 num_samples 7500 loss 0.23849747206304137\n",
      "Epoch 10 num_samples 7600 loss 0.20463323873618738\n",
      "Epoch 10 num_samples 7700 loss 0.29111192405698366\n",
      "Epoch 10 num_samples 7800 loss 0.2491396572404959\n",
      "Epoch 10 num_samples 7900 loss 0.1524850510741545\n",
      "Epoch 10 num_samples 8000 loss 0.34871390201030833\n",
      "Epoch 10 num_samples 8100 loss 0.30620972833494925\n",
      "Epoch 10 num_samples 8200 loss 0.21941591112847533\n",
      "Epoch 10 num_samples 8300 loss 0.22043710305872233\n",
      "Epoch 10 num_samples 8400 loss 0.20446035373874216\n",
      "Epoch 10 num_samples 8500 loss 0.2599526798750447\n",
      "Epoch 10 num_samples 8600 loss 0.29068846394965425\n",
      "Epoch 10 num_samples 8700 loss 0.38750903180699764\n",
      "Epoch 10 num_samples 8800 loss 0.16051512790507744\n",
      "Epoch 10 num_samples 8900 loss 0.31948432615898137\n",
      "Epoch 10 num_samples 9000 loss 0.21535613917347093\n",
      "Epoch 10 num_samples 9100 loss 0.3071901889042121\n",
      "Epoch 10 num_samples 9200 loss 0.2632117145063791\n",
      "Epoch 10 num_samples 9300 loss 0.25006803129939104\n",
      "Epoch 10 num_samples 9400 loss 0.23125717445414046\n",
      "Epoch 10 num_samples 9500 loss 0.28611489631645465\n",
      "Epoch 10 num_samples 9600 loss 0.2214449414226198\n",
      "Epoch 10 num_samples 9700 loss 0.34967339180213236\n",
      "Epoch 10 num_samples 9800 loss 0.13136289243653812\n",
      "Epoch 10 num_samples 9900 loss 0.34569823710996134\n",
      "Epoch 10 num_samples 10000 loss 0.17475101998281142\n",
      "Epoch 10 num_samples 10100 loss 0.32679350374341576\n",
      "Epoch 10 num_samples 10200 loss 0.27622592489394754\n",
      "Epoch 10 num_samples 10300 loss 0.1667607563679162\n",
      "Epoch 10 num_samples 10400 loss 0.3414611880288983\n",
      "Epoch 10 num_samples 10500 loss 0.2872927891563297\n",
      "Epoch 10 num_samples 10600 loss 0.31770944226513775\n",
      "Epoch 10 num_samples 10700 loss 0.2850614090171433\n",
      "Epoch 10 num_samples 10800 loss 0.22068409001794376\n",
      "Epoch 10 num_samples 10900 loss 0.4218021133015047\n",
      "Epoch 10 num_samples 11000 loss 0.3050926166628562\n",
      "Epoch 10 num_samples 11100 loss 0.21375975311191836\n",
      "Epoch 10 num_samples 11200 loss 0.4582737779065998\n",
      "Epoch 10 num_samples 11300 loss 0.25014908275258946\n",
      "Epoch 10 num_samples 11400 loss 0.2659526581702847\n",
      "Epoch 10 num_samples 11500 loss 0.32201440958807653\n",
      "Epoch 10 num_samples 11600 loss 0.28124301803637874\n",
      "Epoch 10 num_samples 11700 loss 0.17322647276210767\n",
      "Epoch 10 num_samples 11800 loss 0.22608133603113703\n",
      "Epoch 10 num_samples 11900 loss 0.2069221844525877\n",
      "Epoch 10 num_samples 12000 loss 0.291430539265207\n",
      "Epoch 10 num_samples 12100 loss 0.23206389112584852\n",
      "Epoch 10 num_samples 12200 loss 0.22341922205514603\n",
      "Epoch 10 num_samples 12300 loss 0.31547571912402633\n",
      "Epoch 10 num_samples 12400 loss 0.31667311113219426\n",
      "Epoch 10 num_samples 12500 loss 0.2182510578232694\n",
      "Epoch 10 num_samples 12600 loss 0.24575392553430042\n",
      "Epoch 10 num_samples 12700 loss 0.2824357881158811\n",
      "Epoch 10 num_samples 12800 loss 0.21963172384509033\n",
      "Epoch 10 num_samples 12900 loss 0.3409767996032816\n",
      "Epoch 10 num_samples 13000 loss 0.22511044934535188\n",
      "Epoch 10 num_samples 13100 loss 0.17096583500087084\n",
      "Epoch 10 num_samples 13200 loss 0.38208187325215776\n",
      "Epoch 10 num_samples 13300 loss 0.20295606914421924\n",
      "Epoch 10 num_samples 13400 loss 0.2594897766462625\n",
      "Epoch 10 num_samples 13500 loss 0.1758303149106282\n",
      "Epoch 10 num_samples 13600 loss 0.2620815171855742\n",
      "Epoch 10 num_samples 13700 loss 0.2823411716791574\n",
      "Epoch 10 num_samples 13800 loss 0.32192499641518635\n",
      "Epoch 10 num_samples 13900 loss 0.2953318739889189\n",
      "Epoch 10 num_samples 14000 loss 0.2773082303560031\n",
      "Epoch 10 num_samples 14100 loss 0.25654841836723696\n",
      "Epoch 10 num_samples 14200 loss 0.29764006361432527\n",
      "Epoch 10 num_samples 14300 loss 0.31408957106426155\n",
      "Epoch 10 num_samples 14400 loss 0.2484413809128762\n",
      "Epoch 10 num_samples 14500 loss 0.2031110153462567\n",
      "Epoch 10 num_samples 14600 loss 0.3524816556652067\n",
      "Epoch 10 num_samples 14700 loss 0.2957159284667049\n",
      "Epoch 10 num_samples 14800 loss 0.31154278949663256\n",
      "Epoch 10 num_samples 14900 loss 0.1844846404388685\n",
      "Epoch 10 num_samples 15000 loss 0.2608396847019678\n",
      "Epoch 10 num_samples 15100 loss 0.19614401166130818\n",
      "Epoch 10 num_samples 15200 loss 0.36822138705187457\n",
      "Epoch 10 num_samples 15300 loss 0.23099070955011414\n",
      "Epoch 10 num_samples 15400 loss 0.18161125693758337\n",
      "Epoch 10 num_samples 15500 loss 0.19168289286010565\n",
      "Epoch 10 num_samples 15600 loss 0.2225186620945719\n",
      "Epoch 10 num_samples 15700 loss 0.25015003938201247\n",
      "Epoch 10 num_samples 15800 loss 0.26631405078401316\n",
      "Epoch 10 num_samples 15900 loss 0.40327915464490827\n",
      "Epoch 10 num_samples 16000 loss 0.3495997715864563\n",
      "Epoch 10 num_samples 16100 loss 0.23498826444875248\n",
      "Epoch 10 num_samples 16200 loss 0.29553697429095827\n",
      "Epoch 10 num_samples 16300 loss 0.3692699140014295\n",
      "Epoch 10 num_samples 16400 loss 0.4072599899343304\n",
      "Epoch 10 num_samples 16500 loss 0.3045075947726349\n",
      "Epoch 10 num_samples 16600 loss 0.3427583563145674\n",
      "Epoch 10 num_samples 16700 loss 0.2170282794133908\n",
      "Epoch 10 num_samples 16800 loss 0.28441977069045576\n",
      "Epoch 10 num_samples 16900 loss 0.2952319096156459\n",
      "Epoch 10 num_samples 17000 loss 0.30592931896126146\n",
      "Epoch 10 num_samples 17100 loss 0.2341774826065165\n",
      "Epoch 10 num_samples 17200 loss 0.23444080488965383\n",
      "Epoch 10 num_samples 17300 loss 0.22199001860134943\n",
      "Epoch 10 num_samples 17400 loss 0.2744146575996046\n",
      "Epoch 10 num_samples 17500 loss 0.23163010371722245\n",
      "Epoch 10 num_samples 17600 loss 0.2486658367584608\n",
      "Epoch 10 num_samples 17700 loss 0.2549819080426674\n",
      "Epoch 10 num_samples 17800 loss 0.2489906149802787\n",
      "Epoch 10 num_samples 17900 loss 0.2593520001727495\n",
      "Epoch 10 num_samples 18000 loss 0.1905356861971703\n",
      "Epoch 10 num_samples 18100 loss 0.2874517581333777\n",
      "Epoch 10 num_samples 18200 loss 0.175970922681326\n",
      "Epoch 10 num_samples 18300 loss 0.30220078852908694\n",
      "Epoch 10 num_samples 18400 loss 0.29448923618971656\n",
      "Epoch 10 num_samples 18500 loss 0.2293062188752255\n",
      "Epoch 11 num_samples 0 loss 0.2926852258982075\n",
      "Epoch 11 num_samples 100 loss 0.32901861770931506\n",
      "Epoch 11 num_samples 200 loss 0.25177462984976023\n",
      "Epoch 11 num_samples 300 loss 0.2002975823304097\n",
      "Epoch 11 num_samples 400 loss 0.18661078882492604\n",
      "Epoch 11 num_samples 500 loss 0.18606041073751953\n",
      "Epoch 11 num_samples 600 loss 0.35551653276159867\n",
      "Epoch 11 num_samples 700 loss 0.22632955909056476\n",
      "Epoch 11 num_samples 800 loss 0.25988273728609146\n",
      "Epoch 11 num_samples 900 loss 0.19834120947373612\n",
      "Epoch 11 num_samples 1000 loss 0.21676900114935344\n",
      "Epoch 11 num_samples 1100 loss 0.2122523791599283\n",
      "Epoch 11 num_samples 1200 loss 0.18279454658039776\n",
      "Epoch 11 num_samples 1300 loss 0.28482285771818966\n",
      "Epoch 11 num_samples 1400 loss 0.23631396815862316\n",
      "Epoch 11 num_samples 1500 loss 0.38394163806503\n",
      "Epoch 11 num_samples 1600 loss 0.24524214277479356\n",
      "Epoch 11 num_samples 1700 loss 0.31996906554278515\n",
      "Epoch 11 num_samples 1800 loss 0.26259117312552144\n",
      "Epoch 11 num_samples 1900 loss 0.35183042392924657\n",
      "Epoch 11 num_samples 2000 loss 0.19147019024320167\n",
      "Epoch 11 num_samples 2100 loss 0.27858744433010457\n",
      "Epoch 11 num_samples 2200 loss 0.2160321926922836\n",
      "Epoch 11 num_samples 2300 loss 0.16586227894404523\n",
      "Epoch 11 num_samples 2400 loss 0.15718005143462704\n",
      "Epoch 11 num_samples 2500 loss 0.14923937133639328\n",
      "Epoch 11 num_samples 2600 loss 0.25562830991506513\n",
      "Epoch 11 num_samples 2700 loss 0.2731190542938611\n",
      "Epoch 11 num_samples 2800 loss 0.1944645689480999\n",
      "Epoch 11 num_samples 2900 loss 0.17097630922416576\n",
      "Epoch 11 num_samples 3000 loss 0.27517969201692394\n",
      "Epoch 11 num_samples 3100 loss 0.23616005256130493\n",
      "Epoch 11 num_samples 3200 loss 0.23012315817958537\n",
      "Epoch 11 num_samples 3300 loss 0.25300073306347054\n",
      "Epoch 11 num_samples 3400 loss 0.213832937910742\n",
      "Epoch 11 num_samples 3500 loss 0.18567812621433588\n",
      "Epoch 11 num_samples 3600 loss 0.23281287210664137\n",
      "Epoch 11 num_samples 3700 loss 0.15391221791569035\n",
      "Epoch 11 num_samples 3800 loss 0.2776443122838035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 num_samples 3900 loss 0.23297658867905244\n",
      "Epoch 11 num_samples 4000 loss 0.1793719576086659\n",
      "Epoch 11 num_samples 4100 loss 0.2180720711203813\n",
      "Epoch 11 num_samples 4200 loss 0.1859597374657104\n",
      "Epoch 11 num_samples 4300 loss 0.24348112491674473\n",
      "Epoch 11 num_samples 4400 loss 0.2013729962367109\n",
      "Epoch 11 num_samples 4500 loss 0.298657263194674\n",
      "Epoch 11 num_samples 4600 loss 0.1880286712315039\n",
      "Epoch 11 num_samples 4700 loss 0.2965680031237046\n",
      "Epoch 11 num_samples 4800 loss 0.18263138271942134\n",
      "Epoch 11 num_samples 4900 loss 0.28866144222277107\n",
      "Epoch 11 num_samples 5000 loss 0.30834378719284156\n",
      "Epoch 11 num_samples 5100 loss 0.3258543257967493\n",
      "Epoch 11 num_samples 5200 loss 0.24668820221034105\n",
      "Epoch 11 num_samples 5300 loss 0.2557007239214959\n",
      "Epoch 11 num_samples 5400 loss 0.23602238916940238\n",
      "Epoch 11 num_samples 5500 loss 0.2497161693074269\n",
      "Epoch 11 num_samples 5600 loss 0.32543253275301454\n",
      "Epoch 11 num_samples 5700 loss 0.31194990041686826\n",
      "Epoch 11 num_samples 5800 loss 0.24141029418373466\n",
      "Epoch 11 num_samples 5900 loss 0.2223799959687036\n",
      "Epoch 11 num_samples 6000 loss 0.2318775175767138\n",
      "Epoch 11 num_samples 6100 loss 0.22771350134901508\n",
      "Epoch 11 num_samples 6200 loss 0.31232101430055986\n",
      "Epoch 11 num_samples 6300 loss 0.32756849277863814\n",
      "Epoch 11 num_samples 6400 loss 0.23324001073924466\n",
      "Epoch 11 num_samples 6500 loss 0.26058616241016724\n",
      "Epoch 11 num_samples 6600 loss 0.17260376493443627\n",
      "Epoch 11 num_samples 6700 loss 0.32401199932110464\n",
      "Epoch 11 num_samples 6800 loss 0.26430058230285647\n",
      "Epoch 11 num_samples 6900 loss 0.23038119553230277\n",
      "Epoch 11 num_samples 7000 loss 0.21106251642284132\n",
      "Epoch 11 num_samples 7100 loss 0.24754230502518726\n",
      "Epoch 11 num_samples 7200 loss 0.13409315270649255\n",
      "Epoch 11 num_samples 7300 loss 0.335631067219264\n",
      "Epoch 11 num_samples 7400 loss 0.27246333638356035\n",
      "Epoch 11 num_samples 7500 loss 0.2153304211523561\n",
      "Epoch 11 num_samples 7600 loss 0.1838234977493616\n",
      "Epoch 11 num_samples 7700 loss 0.2653333295801215\n",
      "Epoch 11 num_samples 7800 loss 0.2267740000881927\n",
      "Epoch 11 num_samples 7900 loss 0.139484717542654\n",
      "Epoch 11 num_samples 8000 loss 0.31646142743518707\n",
      "Epoch 11 num_samples 8100 loss 0.27405452586706913\n",
      "Epoch 11 num_samples 8200 loss 0.19408768210561506\n",
      "Epoch 11 num_samples 8300 loss 0.19614978685276005\n",
      "Epoch 11 num_samples 8400 loss 0.179808256553291\n",
      "Epoch 11 num_samples 8500 loss 0.23724723510890847\n",
      "Epoch 11 num_samples 8600 loss 0.2635922498495375\n",
      "Epoch 11 num_samples 8700 loss 0.36004267606188256\n",
      "Epoch 11 num_samples 8800 loss 0.14685981856989208\n",
      "Epoch 11 num_samples 8900 loss 0.2966641085683285\n",
      "Epoch 11 num_samples 9000 loss 0.1924715466547572\n",
      "Epoch 11 num_samples 9100 loss 0.28207819090256664\n",
      "Epoch 11 num_samples 9200 loss 0.23392450837990936\n",
      "Epoch 11 num_samples 9300 loss 0.22705025061099143\n",
      "Epoch 11 num_samples 9400 loss 0.2104097736089699\n",
      "Epoch 11 num_samples 9500 loss 0.25336244544851766\n",
      "Epoch 11 num_samples 9600 loss 0.1983177870185601\n",
      "Epoch 11 num_samples 9700 loss 0.32356569528210644\n",
      "Epoch 11 num_samples 9800 loss 0.11889671539380174\n",
      "Epoch 11 num_samples 9900 loss 0.32150772583601994\n",
      "Epoch 11 num_samples 10000 loss 0.1585535396404911\n",
      "Epoch 11 num_samples 10100 loss 0.29692940249879485\n",
      "Epoch 11 num_samples 10200 loss 0.2523217096978684\n",
      "Epoch 11 num_samples 10300 loss 0.15011513885984576\n",
      "Epoch 11 num_samples 10400 loss 0.3122712769332318\n",
      "Epoch 11 num_samples 10500 loss 0.26159755416535974\n",
      "Epoch 11 num_samples 10600 loss 0.28792782837214\n",
      "Epoch 11 num_samples 10700 loss 0.26177310724300024\n",
      "Epoch 11 num_samples 10800 loss 0.19144572480362868\n",
      "Epoch 11 num_samples 10900 loss 0.38878715291415417\n",
      "Epoch 11 num_samples 11000 loss 0.27646137336810556\n",
      "Epoch 11 num_samples 11100 loss 0.18899993009559724\n",
      "Epoch 11 num_samples 11200 loss 0.4178009238646911\n",
      "Epoch 11 num_samples 11300 loss 0.2189449806778425\n",
      "Epoch 11 num_samples 11400 loss 0.23901486984793807\n",
      "Epoch 11 num_samples 11500 loss 0.28829306562032253\n",
      "Epoch 11 num_samples 11600 loss 0.2544675411131416\n",
      "Epoch 11 num_samples 11700 loss 0.1578239602979519\n",
      "Epoch 11 num_samples 11800 loss 0.19939744028922932\n",
      "Epoch 11 num_samples 11900 loss 0.18958919963139256\n",
      "Epoch 11 num_samples 12000 loss 0.26234876015204844\n",
      "Epoch 11 num_samples 12100 loss 0.2124902095790535\n",
      "Epoch 11 num_samples 12200 loss 0.19514940201620504\n",
      "Epoch 11 num_samples 12300 loss 0.2885465896311777\n",
      "Epoch 11 num_samples 12400 loss 0.28636161591093634\n",
      "Epoch 11 num_samples 12500 loss 0.19030751922295736\n",
      "Epoch 11 num_samples 12600 loss 0.2277969411641153\n",
      "Epoch 11 num_samples 12700 loss 0.2592305192035556\n",
      "Epoch 11 num_samples 12800 loss 0.19677897065869535\n",
      "Epoch 11 num_samples 12900 loss 0.32131875271396493\n",
      "Epoch 11 num_samples 13000 loss 0.20699623062502032\n",
      "Epoch 11 num_samples 13100 loss 0.15416431890138457\n",
      "Epoch 11 num_samples 13200 loss 0.34692642797621387\n",
      "Epoch 11 num_samples 13300 loss 0.18510191196639927\n",
      "Epoch 11 num_samples 13400 loss 0.24096304235102575\n",
      "Epoch 11 num_samples 13500 loss 0.1516268866703051\n",
      "Epoch 11 num_samples 13600 loss 0.23859506860243576\n",
      "Epoch 11 num_samples 13700 loss 0.2556449579579597\n",
      "Epoch 11 num_samples 13800 loss 0.29465675810663355\n",
      "Epoch 11 num_samples 13900 loss 0.2669418109881858\n",
      "Epoch 11 num_samples 14000 loss 0.25336681778549014\n",
      "Epoch 11 num_samples 14100 loss 0.2344566162311346\n",
      "Epoch 11 num_samples 14200 loss 0.2699055782660915\n",
      "Epoch 11 num_samples 14300 loss 0.27884983582226913\n",
      "Epoch 11 num_samples 14400 loss 0.22745059504735118\n",
      "Epoch 11 num_samples 14500 loss 0.1810684948232511\n",
      "Epoch 11 num_samples 14600 loss 0.32612139561222775\n",
      "Epoch 11 num_samples 14700 loss 0.2724508830549198\n",
      "Epoch 11 num_samples 14800 loss 0.2844641678172868\n",
      "Epoch 11 num_samples 14900 loss 0.16007956745069207\n",
      "Epoch 11 num_samples 15000 loss 0.23243146248822144\n",
      "Epoch 11 num_samples 15100 loss 0.18362919426778782\n",
      "Epoch 11 num_samples 15200 loss 0.3450790847438407\n",
      "Epoch 11 num_samples 15300 loss 0.2084485042521066\n",
      "Epoch 11 num_samples 15400 loss 0.16371637918419019\n",
      "Epoch 11 num_samples 15500 loss 0.16937996115580378\n",
      "Epoch 11 num_samples 15600 loss 0.19723854657562737\n",
      "Epoch 11 num_samples 15700 loss 0.22614534268093578\n",
      "Epoch 11 num_samples 15800 loss 0.24274641954000686\n",
      "Epoch 11 num_samples 15900 loss 0.3644731966272541\n",
      "Epoch 11 num_samples 16000 loss 0.3176872913752208\n",
      "Epoch 11 num_samples 16100 loss 0.21168257587675204\n",
      "Epoch 11 num_samples 16200 loss 0.26377580483125196\n",
      "Epoch 11 num_samples 16300 loss 0.3400159386976344\n",
      "Epoch 11 num_samples 16400 loss 0.3742572601315351\n",
      "Epoch 11 num_samples 16500 loss 0.2812936902502006\n",
      "Epoch 11 num_samples 16600 loss 0.3109696314865479\n",
      "Epoch 11 num_samples 16700 loss 0.20160608502655003\n",
      "Epoch 11 num_samples 16800 loss 0.2589113785507953\n",
      "Epoch 11 num_samples 16900 loss 0.2661210787479631\n",
      "Epoch 11 num_samples 17000 loss 0.27829749689444105\n",
      "Epoch 11 num_samples 17100 loss 0.20911275761289475\n",
      "Epoch 11 num_samples 17200 loss 0.2094551848154813\n",
      "Epoch 11 num_samples 17300 loss 0.20170228891806682\n",
      "Epoch 11 num_samples 17400 loss 0.24647697179335637\n",
      "Epoch 11 num_samples 17500 loss 0.20989272975553916\n",
      "Epoch 11 num_samples 17600 loss 0.2244806678565682\n",
      "Epoch 11 num_samples 17700 loss 0.23329502705051197\n",
      "Epoch 11 num_samples 17800 loss 0.2211126356530784\n",
      "Epoch 11 num_samples 17900 loss 0.23579773737601123\n",
      "Epoch 11 num_samples 18000 loss 0.16935728067118258\n",
      "Epoch 11 num_samples 18100 loss 0.26196119906828375\n",
      "Epoch 11 num_samples 18200 loss 0.16225696545443224\n",
      "Epoch 11 num_samples 18300 loss 0.2737749801326376\n",
      "Epoch 11 num_samples 18400 loss 0.25939182118609727\n",
      "Epoch 11 num_samples 18500 loss 0.20447686447168661\n",
      "Epoch 12 num_samples 0 loss 0.2737350848520533\n",
      "Epoch 12 num_samples 100 loss 0.305161300923633\n",
      "Epoch 12 num_samples 200 loss 0.2313911192133031\n",
      "Epoch 12 num_samples 300 loss 0.18372210469201788\n",
      "Epoch 12 num_samples 400 loss 0.17177753915453217\n",
      "Epoch 12 num_samples 500 loss 0.17346931741350421\n",
      "Epoch 12 num_samples 600 loss 0.3224647336950733\n",
      "Epoch 12 num_samples 700 loss 0.21087786859330301\n",
      "Epoch 12 num_samples 800 loss 0.23562596028676663\n",
      "Epoch 12 num_samples 900 loss 0.17674161649430326\n",
      "Epoch 12 num_samples 1000 loss 0.195491624544984\n",
      "Epoch 12 num_samples 1100 loss 0.19066139147090802\n",
      "Epoch 12 num_samples 1200 loss 0.165724528255364\n",
      "Epoch 12 num_samples 1300 loss 0.26277714058698465\n",
      "Epoch 12 num_samples 1400 loss 0.2223872573641659\n",
      "Epoch 12 num_samples 1500 loss 0.356323228904408\n",
      "Epoch 12 num_samples 1600 loss 0.219097630047441\n",
      "Epoch 12 num_samples 1700 loss 0.28752547312841076\n",
      "Epoch 12 num_samples 1800 loss 0.2339631917000807\n",
      "Epoch 12 num_samples 1900 loss 0.3194939396221296\n",
      "Epoch 12 num_samples 2000 loss 0.1762217215433718\n",
      "Epoch 12 num_samples 2100 loss 0.25520374393591816\n",
      "Epoch 12 num_samples 2200 loss 0.1979160374583787\n",
      "Epoch 12 num_samples 2300 loss 0.14406818347011532\n",
      "Epoch 12 num_samples 2400 loss 0.13713061582335942\n",
      "Epoch 12 num_samples 2500 loss 0.1331266709686315\n",
      "Epoch 12 num_samples 2600 loss 0.23278181042397336\n",
      "Epoch 12 num_samples 2700 loss 0.2456130331915503\n",
      "Epoch 12 num_samples 2800 loss 0.17875360206992677\n",
      "Epoch 12 num_samples 2900 loss 0.15555786982533723\n",
      "Epoch 12 num_samples 3000 loss 0.24773579959419856\n",
      "Epoch 12 num_samples 3100 loss 0.20907074916435192\n",
      "Epoch 12 num_samples 3200 loss 0.21221084236453944\n",
      "Epoch 12 num_samples 3300 loss 0.22757044083158576\n",
      "Epoch 12 num_samples 3400 loss 0.19304214689675148\n",
      "Epoch 12 num_samples 3500 loss 0.17260074573431924\n",
      "Epoch 12 num_samples 3600 loss 0.2100088104118394\n",
      "Epoch 12 num_samples 3700 loss 0.1372642632150163\n",
      "Epoch 12 num_samples 3800 loss 0.25412863047102563\n",
      "Epoch 12 num_samples 3900 loss 0.21403912648476797\n",
      "Epoch 12 num_samples 4000 loss 0.16186392877110684\n",
      "Epoch 12 num_samples 4100 loss 0.2027071681230773\n",
      "Epoch 12 num_samples 4200 loss 0.1668162728493644\n",
      "Epoch 12 num_samples 4300 loss 0.217467417714895\n",
      "Epoch 12 num_samples 4400 loss 0.18303475733308816\n",
      "Epoch 12 num_samples 4500 loss 0.27017469814980777\n",
      "Epoch 12 num_samples 4600 loss 0.1667880567513795\n",
      "Epoch 12 num_samples 4700 loss 0.27009582388916725\n",
      "Epoch 12 num_samples 4800 loss 0.15982517343657554\n",
      "Epoch 12 num_samples 4900 loss 0.25816714080261205\n",
      "Epoch 12 num_samples 5000 loss 0.2793715331950307\n",
      "Epoch 12 num_samples 5100 loss 0.29441832229656667\n",
      "Epoch 12 num_samples 5200 loss 0.22487555325817005\n",
      "Epoch 12 num_samples 5300 loss 0.23455535154586749\n",
      "Epoch 12 num_samples 5400 loss 0.20695542778693052\n",
      "Epoch 12 num_samples 5500 loss 0.23544232797791456\n",
      "Epoch 12 num_samples 5600 loss 0.3003045297784085\n",
      "Epoch 12 num_samples 5700 loss 0.28548841338708836\n",
      "Epoch 12 num_samples 5800 loss 0.21955037662565136\n",
      "Epoch 12 num_samples 5900 loss 0.2003258158154351\n",
      "Epoch 12 num_samples 6000 loss 0.21877445596441142\n",
      "Epoch 12 num_samples 6100 loss 0.20569532642404126\n",
      "Epoch 12 num_samples 6200 loss 0.2817725886623941\n",
      "Epoch 12 num_samples 6300 loss 0.29867983591232045\n",
      "Epoch 12 num_samples 6400 loss 0.21252826183886595\n",
      "Epoch 12 num_samples 6500 loss 0.23748624874219765\n",
      "Epoch 12 num_samples 6600 loss 0.15471616007552239\n",
      "Epoch 12 num_samples 6700 loss 0.29875161754350343\n",
      "Epoch 12 num_samples 6800 loss 0.23514998171220608\n",
      "Epoch 12 num_samples 6900 loss 0.21108487536096046\n",
      "Epoch 12 num_samples 7000 loss 0.19208162994149305\n",
      "Epoch 12 num_samples 7100 loss 0.22511152283954622\n",
      "Epoch 12 num_samples 7200 loss 0.12147951916825406\n",
      "Epoch 12 num_samples 7300 loss 0.30983386217515907\n",
      "Epoch 12 num_samples 7400 loss 0.25496872216795596\n",
      "Epoch 12 num_samples 7500 loss 0.19960476269348554\n",
      "Epoch 12 num_samples 7600 loss 0.1688065438372604\n",
      "Epoch 12 num_samples 7700 loss 0.2410560517807837\n",
      "Epoch 12 num_samples 7800 loss 0.20740092816260242\n",
      "Epoch 12 num_samples 7900 loss 0.127957217306078\n",
      "Epoch 12 num_samples 8000 loss 0.28859441965732374\n",
      "Epoch 12 num_samples 8100 loss 0.24767051429877504\n",
      "Epoch 12 num_samples 8200 loss 0.17099075629695654\n",
      "Epoch 12 num_samples 8300 loss 0.1735927777254615\n",
      "Epoch 12 num_samples 8400 loss 0.15819362063629636\n",
      "Epoch 12 num_samples 8500 loss 0.2164587446543374\n",
      "Epoch 12 num_samples 8600 loss 0.24419853020256532\n",
      "Epoch 12 num_samples 8700 loss 0.3343320210866479\n",
      "Epoch 12 num_samples 8800 loss 0.132671448625507\n",
      "Epoch 12 num_samples 8900 loss 0.277420326259538\n",
      "Epoch 12 num_samples 9000 loss 0.17322208936353664\n",
      "Epoch 12 num_samples 9100 loss 0.2590687075274581\n",
      "Epoch 12 num_samples 9200 loss 0.2088947034449299\n",
      "Epoch 12 num_samples 9300 loss 0.20704349091627003\n",
      "Epoch 12 num_samples 9400 loss 0.19145197161200517\n",
      "Epoch 12 num_samples 9500 loss 0.22483852101080865\n",
      "Epoch 12 num_samples 9600 loss 0.1763299681248879\n",
      "Epoch 12 num_samples 9700 loss 0.299642414002072\n",
      "Epoch 12 num_samples 9800 loss 0.108585030561333\n",
      "Epoch 12 num_samples 9900 loss 0.2965480955691707\n",
      "Epoch 12 num_samples 10000 loss 0.14526784857629996\n",
      "Epoch 12 num_samples 10100 loss 0.26929367643803603\n",
      "Epoch 12 num_samples 10200 loss 0.22850785042452962\n",
      "Epoch 12 num_samples 10300 loss 0.13690642638820175\n",
      "Epoch 12 num_samples 10400 loss 0.2835871734157756\n",
      "Epoch 12 num_samples 10500 loss 0.24134620673222273\n",
      "Epoch 12 num_samples 10600 loss 0.2606389805892945\n",
      "Epoch 12 num_samples 10700 loss 0.2403238464536437\n",
      "Epoch 12 num_samples 10800 loss 0.16866974473920698\n",
      "Epoch 12 num_samples 10900 loss 0.3563388683692057\n",
      "Epoch 12 num_samples 11000 loss 0.2517401331240627\n",
      "Epoch 12 num_samples 11100 loss 0.16965096128679605\n",
      "Epoch 12 num_samples 11200 loss 0.3822737460598298\n",
      "Epoch 12 num_samples 11300 loss 0.19154504694982702\n",
      "Epoch 12 num_samples 11400 loss 0.21533227070104574\n",
      "Epoch 12 num_samples 11500 loss 0.26074466457585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 num_samples 11600 loss 0.2289703566578429\n",
      "Epoch 12 num_samples 11700 loss 0.14500024734335298\n",
      "Epoch 12 num_samples 11800 loss 0.1787096385118469\n",
      "Epoch 12 num_samples 11900 loss 0.17282595975772835\n",
      "Epoch 12 num_samples 12000 loss 0.23706529905262413\n",
      "Epoch 12 num_samples 12100 loss 0.19051538389116382\n",
      "Epoch 12 num_samples 12200 loss 0.17268710912283886\n",
      "Epoch 12 num_samples 12300 loss 0.2619578520950826\n",
      "Epoch 12 num_samples 12400 loss 0.25885293996628667\n",
      "Epoch 12 num_samples 12500 loss 0.1688040915418607\n",
      "Epoch 12 num_samples 12600 loss 0.2149369937342421\n",
      "Epoch 12 num_samples 12700 loss 0.2366195409488975\n",
      "Epoch 12 num_samples 12800 loss 0.18007658977523625\n",
      "Epoch 12 num_samples 12900 loss 0.30487019980383745\n",
      "Epoch 12 num_samples 13000 loss 0.18685875354734066\n",
      "Epoch 12 num_samples 13100 loss 0.13929858470785128\n",
      "Epoch 12 num_samples 13200 loss 0.31597675840045353\n",
      "Epoch 12 num_samples 13300 loss 0.1696620818380528\n",
      "Epoch 12 num_samples 13400 loss 0.22559528539155302\n",
      "Epoch 12 num_samples 13500 loss 0.13287540748936577\n",
      "Epoch 12 num_samples 13600 loss 0.21756479290074737\n",
      "Epoch 12 num_samples 13700 loss 0.23103353615753489\n",
      "Epoch 12 num_samples 13800 loss 0.27204825554249046\n",
      "Epoch 12 num_samples 13900 loss 0.24248869719330446\n",
      "Epoch 12 num_samples 14000 loss 0.23228599478088136\n",
      "Epoch 12 num_samples 14100 loss 0.21745851575649916\n",
      "Epoch 12 num_samples 14200 loss 0.246294653571455\n",
      "Epoch 12 num_samples 14300 loss 0.24811930541628271\n",
      "Epoch 12 num_samples 14400 loss 0.20965605197514614\n",
      "Epoch 12 num_samples 14500 loss 0.1632736561395376\n",
      "Epoch 12 num_samples 14600 loss 0.30553039574697943\n",
      "Epoch 12 num_samples 14700 loss 0.24784467118355122\n",
      "Epoch 12 num_samples 14800 loss 0.2592703474798667\n",
      "Epoch 12 num_samples 14900 loss 0.14051169904503363\n",
      "Epoch 12 num_samples 15000 loss 0.2101180503675762\n",
      "Epoch 12 num_samples 15100 loss 0.1714713651844771\n",
      "Epoch 12 num_samples 15200 loss 0.3214382923547197\n",
      "Epoch 12 num_samples 15300 loss 0.18845849330044348\n",
      "Epoch 12 num_samples 15400 loss 0.1470310025200588\n",
      "Epoch 12 num_samples 15500 loss 0.15065408018617285\n",
      "Epoch 12 num_samples 15600 loss 0.17355413356457014\n",
      "Epoch 12 num_samples 15700 loss 0.2044079890942701\n",
      "Epoch 12 num_samples 15800 loss 0.22128098588183398\n",
      "Epoch 12 num_samples 15900 loss 0.3285193961409326\n",
      "Epoch 12 num_samples 16000 loss 0.28824224624843925\n",
      "Epoch 12 num_samples 16100 loss 0.19110685066802385\n",
      "Epoch 12 num_samples 16200 loss 0.23640632811257892\n",
      "Epoch 12 num_samples 16300 loss 0.31259665423104804\n",
      "Epoch 12 num_samples 16400 loss 0.3410979930738211\n",
      "Epoch 12 num_samples 16500 loss 0.2618777826921759\n",
      "Epoch 12 num_samples 16600 loss 0.2813261720996406\n",
      "Epoch 12 num_samples 16700 loss 0.18779975398065268\n",
      "Epoch 12 num_samples 16800 loss 0.23245455298332465\n",
      "Epoch 12 num_samples 16900 loss 0.2422003259782705\n",
      "Epoch 12 num_samples 17000 loss 0.256630135601858\n",
      "Epoch 12 num_samples 17100 loss 0.18738484409417017\n",
      "Epoch 12 num_samples 17200 loss 0.1881499682583147\n",
      "Epoch 12 num_samples 17300 loss 0.18239313833908694\n",
      "Epoch 12 num_samples 17400 loss 0.2225773884986701\n",
      "Epoch 12 num_samples 17500 loss 0.19240011605117024\n",
      "Epoch 12 num_samples 17600 loss 0.20497498257400498\n",
      "Epoch 12 num_samples 17700 loss 0.21526157499800364\n",
      "Epoch 12 num_samples 17800 loss 0.1960359579740433\n",
      "Epoch 12 num_samples 17900 loss 0.21354028986495596\n",
      "Epoch 12 num_samples 18000 loss 0.15500141241021076\n",
      "Epoch 12 num_samples 18100 loss 0.23905735155767535\n",
      "Epoch 12 num_samples 18200 loss 0.1494389099557589\n",
      "Epoch 12 num_samples 18300 loss 0.24842003927369252\n",
      "Epoch 12 num_samples 18400 loss 0.23053880526643944\n",
      "Epoch 12 num_samples 18500 loss 0.18284215615368335\n",
      "Epoch 13 num_samples 0 loss 0.2586281905669573\n",
      "Epoch 13 num_samples 100 loss 0.28247818250313267\n",
      "Epoch 13 num_samples 200 loss 0.2139581746683607\n",
      "Epoch 13 num_samples 300 loss 0.17102614578235034\n",
      "Epoch 13 num_samples 400 loss 0.1610611018125831\n",
      "Epoch 13 num_samples 500 loss 0.1619114411744002\n",
      "Epoch 13 num_samples 600 loss 0.2917609237786408\n",
      "Epoch 13 num_samples 700 loss 0.19604650797533285\n",
      "Epoch 13 num_samples 800 loss 0.21665955380573731\n",
      "Epoch 13 num_samples 900 loss 0.15988409944917972\n",
      "Epoch 13 num_samples 1000 loss 0.17828907441547714\n",
      "Epoch 13 num_samples 1100 loss 0.16999435527059828\n",
      "Epoch 13 num_samples 1200 loss 0.14929844120869482\n",
      "Epoch 13 num_samples 1300 loss 0.24299446016802617\n",
      "Epoch 13 num_samples 1400 loss 0.20925446499698974\n",
      "Epoch 13 num_samples 1500 loss 0.3316085993179761\n",
      "Epoch 13 num_samples 1600 loss 0.1965631001109217\n",
      "Epoch 13 num_samples 1700 loss 0.2625911796774036\n",
      "Epoch 13 num_samples 1800 loss 0.2147254039898522\n",
      "Epoch 13 num_samples 1900 loss 0.2907318462371473\n",
      "Epoch 13 num_samples 2000 loss 0.16190486296330234\n",
      "Epoch 13 num_samples 2100 loss 0.2322389819326156\n",
      "Epoch 13 num_samples 2200 loss 0.1836368589508528\n",
      "Epoch 13 num_samples 2300 loss 0.1269542766759406\n",
      "Epoch 13 num_samples 2400 loss 0.12193142019586647\n",
      "Epoch 13 num_samples 2500 loss 0.12059163125715112\n",
      "Epoch 13 num_samples 2600 loss 0.21094248509276145\n",
      "Epoch 13 num_samples 2700 loss 0.2213169305667158\n",
      "Epoch 13 num_samples 2800 loss 0.16710531965885472\n",
      "Epoch 13 num_samples 2900 loss 0.14006515194600858\n",
      "Epoch 13 num_samples 3000 loss 0.22360130519734794\n",
      "Epoch 13 num_samples 3100 loss 0.18665991940996104\n",
      "Epoch 13 num_samples 3200 loss 0.20011928728905645\n",
      "Epoch 13 num_samples 3300 loss 0.2096893179087027\n",
      "Epoch 13 num_samples 3400 loss 0.17606834594893556\n",
      "Epoch 13 num_samples 3500 loss 0.16034642394581763\n",
      "Epoch 13 num_samples 3600 loss 0.19024542831368854\n",
      "Epoch 13 num_samples 3700 loss 0.12227878530599968\n",
      "Epoch 13 num_samples 3800 loss 0.2335863526119136\n",
      "Epoch 13 num_samples 3900 loss 0.19885228613547917\n",
      "Epoch 13 num_samples 4000 loss 0.14634614217501007\n",
      "Epoch 13 num_samples 4100 loss 0.18879199935757346\n",
      "Epoch 13 num_samples 4200 loss 0.15070208321758802\n",
      "Epoch 13 num_samples 4300 loss 0.19646581094397228\n",
      "Epoch 13 num_samples 4400 loss 0.1696068362720363\n",
      "Epoch 13 num_samples 4500 loss 0.24666638554098783\n",
      "Epoch 13 num_samples 4600 loss 0.14997538696804782\n",
      "Epoch 13 num_samples 4700 loss 0.2478188781092216\n",
      "Epoch 13 num_samples 4800 loss 0.14222330580813408\n",
      "Epoch 13 num_samples 4900 loss 0.23347535567123542\n",
      "Epoch 13 num_samples 5000 loss 0.2529722809473081\n",
      "Epoch 13 num_samples 5100 loss 0.2662576322862505\n",
      "Epoch 13 num_samples 5200 loss 0.20482515908653695\n",
      "Epoch 13 num_samples 5300 loss 0.21829135828220558\n",
      "Epoch 13 num_samples 5400 loss 0.18269594843160225\n",
      "Epoch 13 num_samples 5500 loss 0.2228392744854829\n",
      "Epoch 13 num_samples 5600 loss 0.2797218262200588\n",
      "Epoch 13 num_samples 5700 loss 0.25938259660488056\n",
      "Epoch 13 num_samples 5800 loss 0.20158374787920638\n",
      "Epoch 13 num_samples 5900 loss 0.18062533571384853\n",
      "Epoch 13 num_samples 6000 loss 0.2077761072019961\n",
      "Epoch 13 num_samples 6100 loss 0.1873166324285738\n",
      "Epoch 13 num_samples 6200 loss 0.2530437198286094\n",
      "Epoch 13 num_samples 6300 loss 0.27596323251366756\n",
      "Epoch 13 num_samples 6400 loss 0.19182616350479653\n",
      "Epoch 13 num_samples 6500 loss 0.2158606538626654\n",
      "Epoch 13 num_samples 6600 loss 0.1402104279030706\n",
      "Epoch 13 num_samples 6700 loss 0.27629119212991765\n",
      "Epoch 13 num_samples 6800 loss 0.21054168007798402\n",
      "Epoch 13 num_samples 6900 loss 0.1937680028769859\n",
      "Epoch 13 num_samples 7000 loss 0.17656596007367686\n",
      "Epoch 13 num_samples 7100 loss 0.2080030831271192\n",
      "Epoch 13 num_samples 7200 loss 0.11111770889666964\n",
      "Epoch 13 num_samples 7300 loss 0.28579882303514076\n",
      "Epoch 13 num_samples 7400 loss 0.23981227297942023\n",
      "Epoch 13 num_samples 7500 loss 0.18560813422955746\n",
      "Epoch 13 num_samples 7600 loss 0.15573026397720993\n",
      "Epoch 13 num_samples 7700 loss 0.22232749744858546\n",
      "Epoch 13 num_samples 7800 loss 0.1866981991876102\n",
      "Epoch 13 num_samples 7900 loss 0.11771101354113594\n",
      "Epoch 13 num_samples 8000 loss 0.2639798710276489\n",
      "Epoch 13 num_samples 8100 loss 0.22283390579617227\n",
      "Epoch 13 num_samples 8200 loss 0.15157098595594207\n",
      "Epoch 13 num_samples 8300 loss 0.15716105571358416\n",
      "Epoch 13 num_samples 8400 loss 0.13809123347178012\n",
      "Epoch 13 num_samples 8500 loss 0.19837364833726004\n",
      "Epoch 13 num_samples 8600 loss 0.22251460600847323\n",
      "Epoch 13 num_samples 8700 loss 0.3111526595522005\n",
      "Epoch 13 num_samples 8800 loss 0.12073711319761977\n",
      "Epoch 13 num_samples 8900 loss 0.26085687737212476\n",
      "Epoch 13 num_samples 9000 loss 0.15650305698051592\n",
      "Epoch 13 num_samples 9100 loss 0.23861590429999166\n",
      "Epoch 13 num_samples 9200 loss 0.1892620909361466\n",
      "Epoch 13 num_samples 9300 loss 0.18793118420696317\n",
      "Epoch 13 num_samples 9400 loss 0.1806316338895308\n",
      "Epoch 13 num_samples 9500 loss 0.1997673935564167\n",
      "Epoch 13 num_samples 9600 loss 0.1591151693298021\n",
      "Epoch 13 num_samples 9700 loss 0.27716580887118336\n",
      "Epoch 13 num_samples 9800 loss 0.09833404054476835\n",
      "Epoch 13 num_samples 9900 loss 0.2764164009562775\n",
      "Epoch 13 num_samples 10000 loss 0.1314022980188013\n",
      "Epoch 13 num_samples 10100 loss 0.24047225298264088\n",
      "Epoch 13 num_samples 10200 loss 0.20499681759992924\n",
      "Epoch 13 num_samples 10300 loss 0.1259129561490423\n",
      "Epoch 13 num_samples 10400 loss 0.25869149288440496\n",
      "Epoch 13 num_samples 10500 loss 0.2250852054898605\n",
      "Epoch 13 num_samples 10600 loss 0.23471625286511455\n",
      "Epoch 13 num_samples 10700 loss 0.22106319347983627\n",
      "Epoch 13 num_samples 10800 loss 0.14863904359114938\n",
      "Epoch 13 num_samples 10900 loss 0.32767837212247536\n",
      "Epoch 13 num_samples 11000 loss 0.23358568290838144\n",
      "Epoch 13 num_samples 11100 loss 0.15249205449333167\n",
      "Epoch 13 num_samples 11200 loss 0.35054315302475786\n",
      "Epoch 13 num_samples 11300 loss 0.17046663453443256\n",
      "Epoch 13 num_samples 11400 loss 0.19585265063352125\n",
      "Epoch 13 num_samples 11500 loss 0.2372831329631481\n",
      "Epoch 13 num_samples 11600 loss 0.20639896155863538\n",
      "Epoch 13 num_samples 11700 loss 0.1331323856639348\n",
      "Epoch 13 num_samples 11800 loss 0.15941586055009677\n",
      "Epoch 13 num_samples 11900 loss 0.16100844791803862\n",
      "Epoch 13 num_samples 12000 loss 0.21658308150555222\n",
      "Epoch 13 num_samples 12100 loss 0.17044120313278477\n",
      "Epoch 13 num_samples 12200 loss 0.15401015453049274\n",
      "Epoch 13 num_samples 12300 loss 0.2433329965701811\n",
      "Epoch 13 num_samples 12400 loss 0.23466812355218675\n",
      "Epoch 13 num_samples 12500 loss 0.14968141307952065\n",
      "Epoch 13 num_samples 12600 loss 0.2047528151649864\n",
      "Epoch 13 num_samples 12700 loss 0.21980908852714187\n",
      "Epoch 13 num_samples 12800 loss 0.1662573993096566\n",
      "Epoch 13 num_samples 12900 loss 0.2905280899465865\n",
      "Epoch 13 num_samples 13000 loss 0.16880379382056204\n",
      "Epoch 13 num_samples 13100 loss 0.12481732755636475\n",
      "Epoch 13 num_samples 13200 loss 0.2841721985244169\n",
      "Epoch 13 num_samples 13300 loss 0.15646458970435564\n",
      "Epoch 13 num_samples 13400 loss 0.2120372048451039\n",
      "Epoch 13 num_samples 13500 loss 0.11707862562257089\n",
      "Epoch 13 num_samples 13600 loss 0.19713700624652447\n",
      "Epoch 13 num_samples 13700 loss 0.20797925804953807\n",
      "Epoch 13 num_samples 13800 loss 0.2500620499897955\n",
      "Epoch 13 num_samples 13900 loss 0.21869843293673583\n",
      "Epoch 13 num_samples 14000 loss 0.2118711744702459\n",
      "Epoch 13 num_samples 14100 loss 0.20051080518874131\n",
      "Epoch 13 num_samples 14200 loss 0.22524086396304036\n",
      "Epoch 13 num_samples 14300 loss 0.22014704473301733\n",
      "Epoch 13 num_samples 14400 loss 0.19613382675251098\n",
      "Epoch 13 num_samples 14500 loss 0.14491484694732898\n",
      "Epoch 13 num_samples 14600 loss 0.28493813945366875\n",
      "Epoch 13 num_samples 14700 loss 0.22798672611553175\n",
      "Epoch 13 num_samples 14800 loss 0.23666634142219586\n",
      "Epoch 13 num_samples 14900 loss 0.12348411824378706\n",
      "Epoch 13 num_samples 15000 loss 0.1898729800097022\n",
      "Epoch 13 num_samples 15100 loss 0.1589361698728341\n",
      "Epoch 13 num_samples 15200 loss 0.3001603383215889\n",
      "Epoch 13 num_samples 15300 loss 0.1703107065463111\n",
      "Epoch 13 num_samples 15400 loss 0.1332108199569339\n",
      "Epoch 13 num_samples 15500 loss 0.13454063464047356\n",
      "Epoch 13 num_samples 15600 loss 0.15194356189816763\n",
      "Epoch 13 num_samples 15700 loss 0.1842127461139834\n",
      "Epoch 13 num_samples 15800 loss 0.20150382075739934\n",
      "Epoch 13 num_samples 15900 loss 0.30141723310364926\n",
      "Epoch 13 num_samples 16000 loss 0.25653637203468066\n",
      "Epoch 13 num_samples 16100 loss 0.1744459464085839\n",
      "Epoch 13 num_samples 16200 loss 0.2123337655910478\n",
      "Epoch 13 num_samples 16300 loss 0.28632680625050855\n",
      "Epoch 13 num_samples 16400 loss 0.3089973389385778\n",
      "Epoch 13 num_samples 16500 loss 0.24512380826291852\n",
      "Epoch 13 num_samples 16600 loss 0.25733209429764353\n",
      "Epoch 13 num_samples 16700 loss 0.17167880098090102\n",
      "Epoch 13 num_samples 16800 loss 0.21282106432793774\n",
      "Epoch 13 num_samples 16900 loss 0.21978338603669986\n",
      "Epoch 13 num_samples 17000 loss 0.2340190219527069\n",
      "Epoch 13 num_samples 17100 loss 0.1662437991511957\n",
      "Epoch 13 num_samples 17200 loss 0.16916491490027347\n",
      "Epoch 13 num_samples 17300 loss 0.16704608049936873\n",
      "Epoch 13 num_samples 17400 loss 0.2000322892670995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 num_samples 17500 loss 0.17586220363326163\n",
      "Epoch 13 num_samples 17600 loss 0.18979095266041926\n",
      "Epoch 13 num_samples 17700 loss 0.19910701397523653\n",
      "Epoch 13 num_samples 17800 loss 0.17732828090291758\n",
      "Epoch 13 num_samples 17900 loss 0.19459330884040793\n",
      "Epoch 13 num_samples 18000 loss 0.14150375087056236\n",
      "Epoch 13 num_samples 18100 loss 0.21680050280881225\n",
      "Epoch 13 num_samples 18200 loss 0.13802024693146303\n",
      "Epoch 13 num_samples 18300 loss 0.22534287374389528\n",
      "Epoch 13 num_samples 18400 loss 0.20598463887384072\n",
      "Epoch 13 num_samples 18500 loss 0.1648779130400806\n",
      "Epoch 14 num_samples 0 loss 0.24250283470881925\n",
      "Epoch 14 num_samples 100 loss 0.26173881762615975\n",
      "Epoch 14 num_samples 200 loss 0.1952321904525772\n",
      "Epoch 14 num_samples 300 loss 0.15590725260653576\n",
      "Epoch 14 num_samples 400 loss 0.15182585041015495\n",
      "Epoch 14 num_samples 500 loss 0.15109964363000472\n",
      "Epoch 14 num_samples 600 loss 0.26677565732771347\n",
      "Epoch 14 num_samples 700 loss 0.1821584918564487\n",
      "Epoch 14 num_samples 800 loss 0.196380946522117\n",
      "Epoch 14 num_samples 900 loss 0.1447668026861597\n",
      "Epoch 14 num_samples 1000 loss 0.16311796807093837\n",
      "Epoch 14 num_samples 1100 loss 0.15364041635383813\n",
      "Epoch 14 num_samples 1200 loss 0.13728033027683315\n",
      "Epoch 14 num_samples 1300 loss 0.2249748909309276\n",
      "Epoch 14 num_samples 1400 loss 0.1965260107116514\n",
      "Epoch 14 num_samples 1500 loss 0.3062671404956639\n",
      "Epoch 14 num_samples 1600 loss 0.1782588327130379\n",
      "Epoch 14 num_samples 1700 loss 0.24106920197733314\n",
      "Epoch 14 num_samples 1800 loss 0.19786119709899982\n",
      "Epoch 14 num_samples 1900 loss 0.26492569940551053\n",
      "Epoch 14 num_samples 2000 loss 0.14833504803877212\n",
      "Epoch 14 num_samples 2100 loss 0.21199085212781366\n",
      "Epoch 14 num_samples 2200 loss 0.16790706840263286\n",
      "Epoch 14 num_samples 2300 loss 0.11277112044739074\n",
      "Epoch 14 num_samples 2400 loss 0.10887092966076374\n",
      "Epoch 14 num_samples 2500 loss 0.11123268030846278\n",
      "Epoch 14 num_samples 2600 loss 0.19341059687098233\n",
      "Epoch 14 num_samples 2700 loss 0.19886134254720567\n",
      "Epoch 14 num_samples 2800 loss 0.15543135320699064\n",
      "Epoch 14 num_samples 2900 loss 0.12859457797224838\n",
      "Epoch 14 num_samples 3000 loss 0.2044932915446364\n",
      "Epoch 14 num_samples 3100 loss 0.1670010118195467\n",
      "Epoch 14 num_samples 3200 loss 0.1868710325342656\n",
      "Epoch 14 num_samples 3300 loss 0.1902847271049251\n",
      "Epoch 14 num_samples 3400 loss 0.1597723056435581\n",
      "Epoch 14 num_samples 3500 loss 0.1485683304176737\n",
      "Epoch 14 num_samples 3600 loss 0.17179080053832663\n",
      "Epoch 14 num_samples 3700 loss 0.1112947446434389\n",
      "Epoch 14 num_samples 3800 loss 0.21436716088236232\n",
      "Epoch 14 num_samples 3900 loss 0.18458785711285855\n",
      "Epoch 14 num_samples 4000 loss 0.13466207870461366\n",
      "Epoch 14 num_samples 4100 loss 0.17882369972447554\n",
      "Epoch 14 num_samples 4200 loss 0.13650447295485096\n",
      "Epoch 14 num_samples 4300 loss 0.17973313249130451\n",
      "Epoch 14 num_samples 4400 loss 0.15851037366911988\n",
      "Epoch 14 num_samples 4500 loss 0.22456205786726616\n",
      "Epoch 14 num_samples 4600 loss 0.1352777040580216\n",
      "Epoch 14 num_samples 4700 loss 0.22907933746492637\n",
      "Epoch 14 num_samples 4800 loss 0.1290075548439147\n",
      "Epoch 14 num_samples 4900 loss 0.21180498342998866\n",
      "Epoch 14 num_samples 5000 loss 0.23017001179418237\n",
      "Epoch 14 num_samples 5100 loss 0.23911304753751103\n",
      "Epoch 14 num_samples 5200 loss 0.18610174470258184\n",
      "Epoch 14 num_samples 5300 loss 0.20329580360449817\n",
      "Epoch 14 num_samples 5400 loss 0.1590030519514931\n",
      "Epoch 14 num_samples 5500 loss 0.21098772412411504\n",
      "Epoch 14 num_samples 5600 loss 0.26109687360560957\n",
      "Epoch 14 num_samples 5700 loss 0.23602706715582072\n",
      "Epoch 14 num_samples 5800 loss 0.18482662142334733\n",
      "Epoch 14 num_samples 5900 loss 0.16544161355433007\n",
      "Epoch 14 num_samples 6000 loss 0.19789730811501421\n",
      "Epoch 14 num_samples 6100 loss 0.1703660637361667\n",
      "Epoch 14 num_samples 6200 loss 0.22958427950584123\n",
      "Epoch 14 num_samples 6300 loss 0.2542464569572383\n",
      "Epoch 14 num_samples 6400 loss 0.17582950838934203\n",
      "Epoch 14 num_samples 6500 loss 0.20033007631584915\n",
      "Epoch 14 num_samples 6600 loss 0.12663207312302802\n",
      "Epoch 14 num_samples 6700 loss 0.25350545153433646\n",
      "Epoch 14 num_samples 6800 loss 0.1906692075311025\n",
      "Epoch 14 num_samples 6900 loss 0.18033573274287118\n",
      "Epoch 14 num_samples 7000 loss 0.1587301100741479\n",
      "Epoch 14 num_samples 7100 loss 0.19256277259661914\n",
      "Epoch 14 num_samples 7200 loss 0.1018540792696881\n",
      "Epoch 14 num_samples 7300 loss 0.26150490459849307\n",
      "Epoch 14 num_samples 7400 loss 0.22752814415305117\n",
      "Epoch 14 num_samples 7500 loss 0.17082109723319883\n",
      "Epoch 14 num_samples 7600 loss 0.14377374200109233\n",
      "Epoch 14 num_samples 7700 loss 0.20579317124550997\n",
      "Epoch 14 num_samples 7800 loss 0.17009880488631676\n",
      "Epoch 14 num_samples 7900 loss 0.10979505763125426\n",
      "Epoch 14 num_samples 8000 loss 0.2394869407368329\n",
      "Epoch 14 num_samples 8100 loss 0.20305146009618297\n",
      "Epoch 14 num_samples 8200 loss 0.13457902299859792\n",
      "Epoch 14 num_samples 8300 loss 0.14130333507058204\n",
      "Epoch 14 num_samples 8400 loss 0.11929798074405581\n",
      "Epoch 14 num_samples 8500 loss 0.18262419615854955\n",
      "Epoch 14 num_samples 8600 loss 0.2037044977959846\n",
      "Epoch 14 num_samples 8700 loss 0.2887943057692479\n",
      "Epoch 14 num_samples 8800 loss 0.10945914033560414\n",
      "Epoch 14 num_samples 8900 loss 0.24378659208745768\n",
      "Epoch 14 num_samples 9000 loss 0.14186084678911537\n",
      "Epoch 14 num_samples 9100 loss 0.22162816550182654\n",
      "Epoch 14 num_samples 9200 loss 0.17045899088074254\n",
      "Epoch 14 num_samples 9300 loss 0.16902583089496667\n",
      "Epoch 14 num_samples 9400 loss 0.16396291519251294\n",
      "Epoch 14 num_samples 9500 loss 0.178682934721196\n",
      "Epoch 14 num_samples 9600 loss 0.14632278603765425\n",
      "Epoch 14 num_samples 9700 loss 0.2582837335800142\n",
      "Epoch 14 num_samples 9800 loss 0.09033613928377505\n",
      "Epoch 14 num_samples 9900 loss 0.25855478169072343\n",
      "Epoch 14 num_samples 10000 loss 0.12065306477926983\n",
      "Epoch 14 num_samples 10100 loss 0.21596422842922203\n",
      "Epoch 14 num_samples 10200 loss 0.1857549753792701\n",
      "Epoch 14 num_samples 10300 loss 0.11686894912739959\n",
      "Epoch 14 num_samples 10400 loss 0.2339117791146754\n",
      "Epoch 14 num_samples 10500 loss 0.2071411427630092\n",
      "Epoch 14 num_samples 10600 loss 0.2116227460723774\n",
      "Epoch 14 num_samples 10700 loss 0.20720759271731698\n",
      "Epoch 14 num_samples 10800 loss 0.13554572127998485\n",
      "Epoch 14 num_samples 10900 loss 0.3017474171605712\n",
      "Epoch 14 num_samples 11000 loss 0.21861394243372792\n",
      "Epoch 14 num_samples 11100 loss 0.13831416613573452\n",
      "Epoch 14 num_samples 11200 loss 0.32088018344425784\n",
      "Epoch 14 num_samples 11300 loss 0.15267192133057111\n",
      "Epoch 14 num_samples 11400 loss 0.17651540017976283\n",
      "Epoch 14 num_samples 11500 loss 0.21610122666975365\n",
      "Epoch 14 num_samples 11600 loss 0.1890737194400179\n",
      "Epoch 14 num_samples 11700 loss 0.12387206177380794\n",
      "Epoch 14 num_samples 11800 loss 0.14391025891053583\n",
      "Epoch 14 num_samples 11900 loss 0.14980313370980827\n",
      "Epoch 14 num_samples 12000 loss 0.1941668000477249\n",
      "Epoch 14 num_samples 12100 loss 0.1525588076729498\n",
      "Epoch 14 num_samples 12200 loss 0.13987714372266144\n",
      "Epoch 14 num_samples 12300 loss 0.22473537934369922\n",
      "Epoch 14 num_samples 12400 loss 0.21228764224763186\n",
      "Epoch 14 num_samples 12500 loss 0.1362473974676843\n",
      "Epoch 14 num_samples 12600 loss 0.19397355985239095\n",
      "Epoch 14 num_samples 12700 loss 0.20320232646577702\n",
      "Epoch 14 num_samples 12800 loss 0.1530284570737339\n",
      "Epoch 14 num_samples 12900 loss 0.2766895174572405\n",
      "Epoch 14 num_samples 13000 loss 0.15062500185728347\n",
      "Epoch 14 num_samples 13100 loss 0.11298257702544141\n",
      "Epoch 14 num_samples 13200 loss 0.2595736694613436\n",
      "Epoch 14 num_samples 13300 loss 0.14400438967469353\n",
      "Epoch 14 num_samples 13400 loss 0.1977425854371384\n",
      "Epoch 14 num_samples 13500 loss 0.10560926021384347\n",
      "Epoch 14 num_samples 13600 loss 0.17984839762497656\n",
      "Epoch 14 num_samples 13700 loss 0.18862704312836684\n",
      "Epoch 14 num_samples 13800 loss 0.23092824141945034\n",
      "Epoch 14 num_samples 13900 loss 0.19771630099306997\n",
      "Epoch 14 num_samples 14000 loss 0.19253444725939256\n",
      "Epoch 14 num_samples 14100 loss 0.1865924287344763\n",
      "Epoch 14 num_samples 14200 loss 0.20962262248301136\n",
      "Epoch 14 num_samples 14300 loss 0.19717257070453426\n",
      "Epoch 14 num_samples 14400 loss 0.18300251792244737\n",
      "Epoch 14 num_samples 14500 loss 0.13126125314431988\n",
      "Epoch 14 num_samples 14600 loss 0.26365732712600026\n",
      "Epoch 14 num_samples 14700 loss 0.2077382565380416\n",
      "Epoch 14 num_samples 14800 loss 0.2167825421824947\n",
      "Epoch 14 num_samples 14900 loss 0.10976505836901765\n",
      "Epoch 14 num_samples 15000 loss 0.1719817680484104\n",
      "Epoch 14 num_samples 15100 loss 0.14927409238135583\n",
      "Epoch 14 num_samples 15200 loss 0.2815544585348166\n",
      "Epoch 14 num_samples 15300 loss 0.15453124852246916\n",
      "Epoch 14 num_samples 15400 loss 0.120868916354303\n",
      "Epoch 14 num_samples 15500 loss 0.12090519353604559\n",
      "Epoch 14 num_samples 15600 loss 0.13494790663106168\n",
      "Epoch 14 num_samples 15700 loss 0.16789866124003225\n",
      "Epoch 14 num_samples 15800 loss 0.18300190872919836\n",
      "Epoch 14 num_samples 15900 loss 0.27582687249832055\n",
      "Epoch 14 num_samples 16000 loss 0.233366836579435\n",
      "Epoch 14 num_samples 16100 loss 0.16045506613795113\n",
      "Epoch 14 num_samples 16200 loss 0.19217496525571945\n",
      "Epoch 14 num_samples 16300 loss 0.2635728340940828\n",
      "Epoch 14 num_samples 16400 loss 0.2785483935245842\n",
      "Epoch 14 num_samples 16500 loss 0.22863326519069688\n",
      "Epoch 14 num_samples 16600 loss 0.2320877373900231\n",
      "Epoch 14 num_samples 16700 loss 0.15959953740378338\n",
      "Epoch 14 num_samples 16800 loss 0.19604682938591744\n",
      "Epoch 14 num_samples 16900 loss 0.20016249087021315\n",
      "Epoch 14 num_samples 17000 loss 0.2136606899652709\n",
      "Epoch 14 num_samples 17100 loss 0.14640859326265634\n",
      "Epoch 14 num_samples 17200 loss 0.15404687071085205\n",
      "Epoch 14 num_samples 17300 loss 0.15202699894157487\n",
      "Epoch 14 num_samples 17400 loss 0.17991422291290235\n",
      "Epoch 14 num_samples 17500 loss 0.159054479251752\n",
      "Epoch 14 num_samples 17600 loss 0.17493811304259702\n",
      "Epoch 14 num_samples 17700 loss 0.18576203902400049\n",
      "Epoch 14 num_samples 17800 loss 0.16100940045531018\n",
      "Epoch 14 num_samples 17900 loss 0.1794388839068371\n",
      "Epoch 14 num_samples 18000 loss 0.13079975892924278\n",
      "Epoch 14 num_samples 18100 loss 0.19805664759634767\n",
      "Epoch 14 num_samples 18200 loss 0.1287635405342376\n",
      "Epoch 14 num_samples 18300 loss 0.20451229074995214\n",
      "Epoch 14 num_samples 18400 loss 0.18452034785052512\n",
      "Epoch 14 num_samples 18500 loss 0.14775915727491953\n",
      "Epoch 15 num_samples 0 loss 0.22802114624382064\n",
      "Epoch 15 num_samples 100 loss 0.2427705469611368\n",
      "Epoch 15 num_samples 200 loss 0.18106334667061713\n",
      "Epoch 15 num_samples 300 loss 0.14550274472930527\n",
      "Epoch 15 num_samples 400 loss 0.14054495787655866\n",
      "Epoch 15 num_samples 500 loss 0.14205690525945008\n",
      "Epoch 15 num_samples 600 loss 0.243030502082485\n",
      "Epoch 15 num_samples 700 loss 0.16771256157743178\n",
      "Epoch 15 num_samples 800 loss 0.18133929326140272\n",
      "Epoch 15 num_samples 900 loss 0.1341064790599484\n",
      "Epoch 15 num_samples 1000 loss 0.151107164541445\n",
      "Epoch 15 num_samples 1100 loss 0.13729427032815056\n",
      "Epoch 15 num_samples 1200 loss 0.12662844200172738\n",
      "Epoch 15 num_samples 1300 loss 0.21043758910758764\n",
      "Epoch 15 num_samples 1400 loss 0.18574935294333617\n",
      "Epoch 15 num_samples 1500 loss 0.28074299512863987\n",
      "Epoch 15 num_samples 1600 loss 0.16136301029366415\n",
      "Epoch 15 num_samples 1700 loss 0.22208739175069142\n",
      "Epoch 15 num_samples 1800 loss 0.18313670081958194\n",
      "Epoch 15 num_samples 1900 loss 0.24127634358591807\n",
      "Epoch 15 num_samples 2000 loss 0.13568863831477956\n",
      "Epoch 15 num_samples 2100 loss 0.19202200814021356\n",
      "Epoch 15 num_samples 2200 loss 0.15359629081641832\n",
      "Epoch 15 num_samples 2300 loss 0.09977276940769034\n",
      "Epoch 15 num_samples 2400 loss 0.09691600164653352\n",
      "Epoch 15 num_samples 2500 loss 0.10274349197388773\n",
      "Epoch 15 num_samples 2600 loss 0.17661368033582753\n",
      "Epoch 15 num_samples 2700 loss 0.1798569651284005\n",
      "Epoch 15 num_samples 2800 loss 0.14694629400278864\n",
      "Epoch 15 num_samples 2900 loss 0.11807039587496225\n",
      "Epoch 15 num_samples 3000 loss 0.18720912903910025\n",
      "Epoch 15 num_samples 3100 loss 0.14881551888431616\n",
      "Epoch 15 num_samples 3200 loss 0.1765180510084071\n",
      "Epoch 15 num_samples 3300 loss 0.17269464768269796\n",
      "Epoch 15 num_samples 3400 loss 0.1424788692563868\n",
      "Epoch 15 num_samples 3500 loss 0.13759055897471476\n",
      "Epoch 15 num_samples 3600 loss 0.1571691555915579\n",
      "Epoch 15 num_samples 3700 loss 0.10237165484887402\n",
      "Epoch 15 num_samples 3800 loss 0.19858798663119018\n",
      "Epoch 15 num_samples 3900 loss 0.1725089057234751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 num_samples 4000 loss 0.1252872885157777\n",
      "Epoch 15 num_samples 4100 loss 0.16694916214281894\n",
      "Epoch 15 num_samples 4200 loss 0.12492658422788513\n",
      "Epoch 15 num_samples 4300 loss 0.16468784849713394\n",
      "Epoch 15 num_samples 4400 loss 0.14914153316130277\n",
      "Epoch 15 num_samples 4500 loss 0.20519347799094753\n",
      "Epoch 15 num_samples 4600 loss 0.12186285213967775\n",
      "Epoch 15 num_samples 4700 loss 0.21151817471310658\n",
      "Epoch 15 num_samples 4800 loss 0.1175187113552613\n",
      "Epoch 15 num_samples 4900 loss 0.18856503504843766\n",
      "Epoch 15 num_samples 5000 loss 0.21113771978914164\n",
      "Epoch 15 num_samples 5100 loss 0.21619084825455065\n",
      "Epoch 15 num_samples 5200 loss 0.16717691991260694\n",
      "Epoch 15 num_samples 5300 loss 0.1885554217245607\n",
      "Epoch 15 num_samples 5400 loss 0.14103265125629919\n",
      "Epoch 15 num_samples 5500 loss 0.19888201882559875\n",
      "Epoch 15 num_samples 5600 loss 0.242193996311641\n",
      "Epoch 15 num_samples 5700 loss 0.21672975849103623\n",
      "Epoch 15 num_samples 5800 loss 0.17115634362917487\n",
      "Epoch 15 num_samples 5900 loss 0.1495219810645957\n",
      "Epoch 15 num_samples 6000 loss 0.1895828346784488\n",
      "Epoch 15 num_samples 6100 loss 0.15562296079587157\n",
      "Epoch 15 num_samples 6200 loss 0.2092501054878425\n",
      "Epoch 15 num_samples 6300 loss 0.23460511571251924\n",
      "Epoch 15 num_samples 6400 loss 0.16154014865198468\n",
      "Epoch 15 num_samples 6500 loss 0.18594034826314293\n",
      "Epoch 15 num_samples 6600 loss 0.11510693972613967\n",
      "Epoch 15 num_samples 6700 loss 0.22818833694285665\n",
      "Epoch 15 num_samples 6800 loss 0.1714464410359318\n",
      "Epoch 15 num_samples 6900 loss 0.1669798711404273\n",
      "Epoch 15 num_samples 7000 loss 0.14419753705508356\n",
      "Epoch 15 num_samples 7100 loss 0.1768099476865834\n",
      "Epoch 15 num_samples 7200 loss 0.0942321493611761\n",
      "Epoch 15 num_samples 7300 loss 0.24184371569259208\n",
      "Epoch 15 num_samples 7400 loss 0.21463466159955494\n",
      "Epoch 15 num_samples 7500 loss 0.15497244842889535\n",
      "Epoch 15 num_samples 7600 loss 0.13506864698036214\n",
      "Epoch 15 num_samples 7700 loss 0.1911302329944921\n",
      "Epoch 15 num_samples 7800 loss 0.1528295970318838\n",
      "Epoch 15 num_samples 7900 loss 0.1009976458126825\n",
      "Epoch 15 num_samples 8000 loss 0.21948060584571102\n",
      "Epoch 15 num_samples 8100 loss 0.1836475283018502\n",
      "Epoch 15 num_samples 8200 loss 0.1201968185578463\n",
      "Epoch 15 num_samples 8300 loss 0.1288783757837043\n",
      "Epoch 15 num_samples 8400 loss 0.10491142309461689\n",
      "Epoch 15 num_samples 8500 loss 0.1686613476095544\n",
      "Epoch 15 num_samples 8600 loss 0.18727520479210838\n",
      "Epoch 15 num_samples 8700 loss 0.26724012413280557\n",
      "Epoch 15 num_samples 8800 loss 0.09823978915679075\n",
      "Epoch 15 num_samples 8900 loss 0.22736356648552727\n",
      "Epoch 15 num_samples 9000 loss 0.12875259971386893\n",
      "Epoch 15 num_samples 9100 loss 0.20611538334850643\n",
      "Epoch 15 num_samples 9200 loss 0.15473458848694666\n",
      "Epoch 15 num_samples 9300 loss 0.15498558138127227\n",
      "Epoch 15 num_samples 9400 loss 0.14991997411612476\n",
      "Epoch 15 num_samples 9500 loss 0.15906480837547146\n",
      "Epoch 15 num_samples 9600 loss 0.13209993518842797\n",
      "Epoch 15 num_samples 9700 loss 0.24064864789429458\n",
      "Epoch 15 num_samples 9800 loss 0.08300978856362555\n",
      "Epoch 15 num_samples 9900 loss 0.23912014633711212\n",
      "Epoch 15 num_samples 10000 loss 0.11031116937075568\n",
      "Epoch 15 num_samples 10100 loss 0.19411393135552668\n",
      "Epoch 15 num_samples 10200 loss 0.16627490657366714\n",
      "Epoch 15 num_samples 10300 loss 0.10777293158466851\n",
      "Epoch 15 num_samples 10400 loss 0.21146132204397092\n",
      "Epoch 15 num_samples 10500 loss 0.1952971161056294\n",
      "Epoch 15 num_samples 10600 loss 0.19025069511743664\n",
      "Epoch 15 num_samples 10700 loss 0.1915981357500742\n",
      "Epoch 15 num_samples 10800 loss 0.12261560828543999\n",
      "Epoch 15 num_samples 10900 loss 0.27928700381276067\n",
      "Epoch 15 num_samples 11000 loss 0.20436912615751154\n",
      "Epoch 15 num_samples 11100 loss 0.1282268719995877\n",
      "Epoch 15 num_samples 11200 loss 0.29359412185193634\n",
      "Epoch 15 num_samples 11300 loss 0.1376358206353804\n",
      "Epoch 15 num_samples 11400 loss 0.1603699342989392\n",
      "Epoch 15 num_samples 11500 loss 0.19707502909736285\n",
      "Epoch 15 num_samples 11600 loss 0.17418736983868327\n",
      "Epoch 15 num_samples 11700 loss 0.11485050756918716\n",
      "Epoch 15 num_samples 11800 loss 0.13007723004578403\n",
      "Epoch 15 num_samples 11900 loss 0.13940134559570655\n",
      "Epoch 15 num_samples 12000 loss 0.17487989835898674\n",
      "Epoch 15 num_samples 12100 loss 0.13885550479059597\n",
      "Epoch 15 num_samples 12200 loss 0.12723291848658577\n",
      "Epoch 15 num_samples 12300 loss 0.20750889859786248\n",
      "Epoch 15 num_samples 12400 loss 0.1911649397828119\n",
      "Epoch 15 num_samples 12500 loss 0.12439519667558968\n",
      "Epoch 15 num_samples 12600 loss 0.18425567132759027\n",
      "Epoch 15 num_samples 12700 loss 0.19344227416012102\n",
      "Epoch 15 num_samples 12800 loss 0.13881145452077082\n",
      "Epoch 15 num_samples 12900 loss 0.26637989677898516\n",
      "Epoch 15 num_samples 13000 loss 0.1364113363752906\n",
      "Epoch 15 num_samples 13100 loss 0.1033584056261954\n",
      "Epoch 15 num_samples 13200 loss 0.23634866713239888\n",
      "Epoch 15 num_samples 13300 loss 0.13318439503500865\n",
      "Epoch 15 num_samples 13400 loss 0.18706219351989317\n",
      "Epoch 15 num_samples 13500 loss 0.09465151744081114\n",
      "Epoch 15 num_samples 13600 loss 0.16426782173384036\n",
      "Epoch 15 num_samples 13700 loss 0.17297729557425393\n",
      "Epoch 15 num_samples 13800 loss 0.21321893785147666\n",
      "Epoch 15 num_samples 13900 loss 0.17919821990106766\n",
      "Epoch 15 num_samples 14000 loss 0.1773759156876271\n",
      "Epoch 15 num_samples 14100 loss 0.17203934113940952\n",
      "Epoch 15 num_samples 14200 loss 0.19112395568612603\n",
      "Epoch 15 num_samples 14300 loss 0.17710560133361924\n",
      "Epoch 15 num_samples 14400 loss 0.1712411301773927\n",
      "Epoch 15 num_samples 14500 loss 0.12020772305659513\n",
      "Epoch 15 num_samples 14600 loss 0.24516059688932718\n",
      "Epoch 15 num_samples 14700 loss 0.18821951767635725\n",
      "Epoch 15 num_samples 14800 loss 0.1990932390838182\n",
      "Epoch 15 num_samples 14900 loss 0.09750831638595006\n",
      "Epoch 15 num_samples 15000 loss 0.15982339304044352\n",
      "Epoch 15 num_samples 15100 loss 0.13813700733377454\n",
      "Epoch 15 num_samples 15200 loss 0.2625238540295247\n",
      "Epoch 15 num_samples 15300 loss 0.1434980150681002\n",
      "Epoch 15 num_samples 15400 loss 0.1099482774828522\n",
      "Epoch 15 num_samples 15500 loss 0.10892379975844896\n",
      "Epoch 15 num_samples 15600 loss 0.12003689434876609\n",
      "Epoch 15 num_samples 15700 loss 0.15191943768249405\n",
      "Epoch 15 num_samples 15800 loss 0.16482196121836876\n",
      "Epoch 15 num_samples 15900 loss 0.25104911646096306\n",
      "Epoch 15 num_samples 16000 loss 0.21266356194368444\n",
      "Epoch 15 num_samples 16100 loss 0.14716774104016028\n",
      "Epoch 15 num_samples 16200 loss 0.1759438406047153\n",
      "Epoch 15 num_samples 16300 loss 0.24296829166909878\n",
      "Epoch 15 num_samples 16400 loss 0.2531404131146001\n",
      "Epoch 15 num_samples 16500 loss 0.21206789778573615\n",
      "Epoch 15 num_samples 16600 loss 0.2087162972189419\n",
      "Epoch 15 num_samples 16700 loss 0.14770239794576373\n",
      "Epoch 15 num_samples 16800 loss 0.18129721702732282\n",
      "Epoch 15 num_samples 16900 loss 0.1816625146865689\n",
      "Epoch 15 num_samples 17000 loss 0.1953571885271678\n",
      "Epoch 15 num_samples 17100 loss 0.13043001918389116\n",
      "Epoch 15 num_samples 17200 loss 0.14005657451498596\n",
      "Epoch 15 num_samples 17300 loss 0.1403838937633311\n",
      "Epoch 15 num_samples 17400 loss 0.1623852721919988\n",
      "Epoch 15 num_samples 17500 loss 0.14408006530351958\n",
      "Epoch 15 num_samples 17600 loss 0.16254342827360962\n",
      "Epoch 15 num_samples 17700 loss 0.17286811479858938\n",
      "Epoch 15 num_samples 17800 loss 0.14550276077793708\n",
      "Epoch 15 num_samples 17900 loss 0.16285068827095073\n",
      "Epoch 15 num_samples 18000 loss 0.12027147219802249\n",
      "Epoch 15 num_samples 18100 loss 0.17831480195349708\n",
      "Epoch 15 num_samples 18200 loss 0.12068692435318812\n",
      "Epoch 15 num_samples 18300 loss 0.18794296496705676\n",
      "Epoch 15 num_samples 18400 loss 0.1642291373519211\n",
      "Epoch 15 num_samples 18500 loss 0.13590180200261265\n",
      "Epoch 16 num_samples 0 loss 0.21356557941672066\n",
      "Epoch 16 num_samples 100 loss 0.22444425346363808\n",
      "Epoch 16 num_samples 200 loss 0.16785358801762468\n",
      "Epoch 16 num_samples 300 loss 0.13682501852812198\n",
      "Epoch 16 num_samples 400 loss 0.13049898605446142\n",
      "Epoch 16 num_samples 500 loss 0.1326778594596944\n",
      "Epoch 16 num_samples 600 loss 0.22176938571349858\n",
      "Epoch 16 num_samples 700 loss 0.15492053450228876\n",
      "Epoch 16 num_samples 800 loss 0.16729174559469162\n",
      "Epoch 16 num_samples 900 loss 0.12383454707781436\n",
      "Epoch 16 num_samples 1000 loss 0.1397324738946722\n",
      "Epoch 16 num_samples 1100 loss 0.12367484713197484\n",
      "Epoch 16 num_samples 1200 loss 0.11520743553546615\n",
      "Epoch 16 num_samples 1300 loss 0.19442772068180075\n",
      "Epoch 16 num_samples 1400 loss 0.17580222602061063\n",
      "Epoch 16 num_samples 1500 loss 0.2561838504948297\n",
      "Epoch 16 num_samples 1600 loss 0.14447230286297877\n",
      "Epoch 16 num_samples 1700 loss 0.20448233019211984\n",
      "Epoch 16 num_samples 1800 loss 0.16781296279632207\n",
      "Epoch 16 num_samples 1900 loss 0.22162531326841783\n",
      "Epoch 16 num_samples 2000 loss 0.12351341422095129\n",
      "Epoch 16 num_samples 2100 loss 0.17277114680945985\n",
      "Epoch 16 num_samples 2200 loss 0.14163507775130763\n",
      "Epoch 16 num_samples 2300 loss 0.08903489352778234\n",
      "Epoch 16 num_samples 2400 loss 0.08744549785090747\n",
      "Epoch 16 num_samples 2500 loss 0.09414899166352769\n",
      "Epoch 16 num_samples 2600 loss 0.1599397971418593\n",
      "Epoch 16 num_samples 2700 loss 0.1650373743696609\n",
      "Epoch 16 num_samples 2800 loss 0.13682206931145352\n",
      "Epoch 16 num_samples 2900 loss 0.10820795073888047\n",
      "Epoch 16 num_samples 3000 loss 0.17217119950612872\n",
      "Epoch 16 num_samples 3100 loss 0.13300292765885527\n",
      "Epoch 16 num_samples 3200 loss 0.16644431143339947\n",
      "Epoch 16 num_samples 3300 loss 0.15958344332905358\n",
      "Epoch 16 num_samples 3400 loss 0.13042469325541067\n",
      "Epoch 16 num_samples 3500 loss 0.1281074118371312\n",
      "Epoch 16 num_samples 3600 loss 0.14270839072388103\n",
      "Epoch 16 num_samples 3700 loss 0.09416831529794123\n",
      "Epoch 16 num_samples 3800 loss 0.18254175226361297\n",
      "Epoch 16 num_samples 3900 loss 0.161479295203835\n",
      "Epoch 16 num_samples 4000 loss 0.11740621304438015\n",
      "Epoch 16 num_samples 4100 loss 0.1562589667287326\n",
      "Epoch 16 num_samples 4200 loss 0.11459571232043843\n",
      "Epoch 16 num_samples 4300 loss 0.15024993445707055\n",
      "Epoch 16 num_samples 4400 loss 0.1417278395813503\n",
      "Epoch 16 num_samples 4500 loss 0.1905609678151523\n",
      "Epoch 16 num_samples 4600 loss 0.10963594552737532\n",
      "Epoch 16 num_samples 4700 loss 0.19150726784822197\n",
      "Epoch 16 num_samples 4800 loss 0.10600710186359619\n",
      "Epoch 16 num_samples 4900 loss 0.16830137228586764\n",
      "Epoch 16 num_samples 5000 loss 0.1947526698151953\n",
      "Epoch 16 num_samples 5100 loss 0.197918617285912\n",
      "Epoch 16 num_samples 5200 loss 0.15027508480863866\n",
      "Epoch 16 num_samples 5300 loss 0.17299041487287758\n",
      "Epoch 16 num_samples 5400 loss 0.12672676645104267\n",
      "Epoch 16 num_samples 5500 loss 0.18749253577349337\n",
      "Epoch 16 num_samples 5600 loss 0.225480855426324\n",
      "Epoch 16 num_samples 5700 loss 0.19862139839215295\n",
      "Epoch 16 num_samples 5800 loss 0.15915334838203832\n",
      "Epoch 16 num_samples 5900 loss 0.13809574096649446\n",
      "Epoch 16 num_samples 6000 loss 0.18141447200122088\n",
      "Epoch 16 num_samples 6100 loss 0.14043707710802206\n",
      "Epoch 16 num_samples 6200 loss 0.19076958837031413\n",
      "Epoch 16 num_samples 6300 loss 0.2163287019545974\n",
      "Epoch 16 num_samples 6400 loss 0.14709376121545564\n",
      "Epoch 16 num_samples 6500 loss 0.172959207342674\n",
      "Epoch 16 num_samples 6600 loss 0.10351459324999795\n",
      "Epoch 16 num_samples 6700 loss 0.20782166448284647\n",
      "Epoch 16 num_samples 6800 loss 0.15530587110734576\n",
      "Epoch 16 num_samples 6900 loss 0.15436108198117704\n",
      "Epoch 16 num_samples 7000 loss 0.13128671128763103\n",
      "Epoch 16 num_samples 7100 loss 0.16283306028727004\n",
      "Epoch 16 num_samples 7200 loss 0.08686488572796805\n",
      "Epoch 16 num_samples 7300 loss 0.2244406187319801\n",
      "Epoch 16 num_samples 7400 loss 0.201190037381485\n",
      "Epoch 16 num_samples 7500 loss 0.14232755266243635\n",
      "Epoch 16 num_samples 7600 loss 0.12439750903524721\n",
      "Epoch 16 num_samples 7700 loss 0.17984565457529356\n",
      "Epoch 16 num_samples 7800 loss 0.1382923410516867\n",
      "Epoch 16 num_samples 7900 loss 0.09411276729206726\n",
      "Epoch 16 num_samples 8000 loss 0.19896650245667863\n",
      "Epoch 16 num_samples 8100 loss 0.16597754993150038\n",
      "Epoch 16 num_samples 8200 loss 0.10696653824390844\n",
      "Epoch 16 num_samples 8300 loss 0.11869420194309707\n",
      "Epoch 16 num_samples 8400 loss 0.09213998548763147\n",
      "Epoch 16 num_samples 8500 loss 0.15485953764235094\n",
      "Epoch 16 num_samples 8600 loss 0.17262547932547107\n",
      "Epoch 16 num_samples 8700 loss 0.2467134176505634\n",
      "Epoch 16 num_samples 8800 loss 0.08702017963434341\n",
      "Epoch 16 num_samples 8900 loss 0.21506821487033748\n",
      "Epoch 16 num_samples 9000 loss 0.11741644355581564\n",
      "Epoch 16 num_samples 9100 loss 0.18915438521172534\n",
      "Epoch 16 num_samples 9200 loss 0.13967855722747013\n",
      "Epoch 16 num_samples 9300 loss 0.1421212306503793\n",
      "Epoch 16 num_samples 9400 loss 0.1391606923582326\n",
      "Epoch 16 num_samples 9500 loss 0.14135843218889577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 num_samples 9600 loss 0.1198174226415426\n",
      "Epoch 16 num_samples 9700 loss 0.2237171457197982\n",
      "Epoch 16 num_samples 9800 loss 0.07713164963341569\n",
      "Epoch 16 num_samples 9900 loss 0.22241137615458817\n",
      "Epoch 16 num_samples 10000 loss 0.1033163774160739\n",
      "Epoch 16 num_samples 10100 loss 0.1791597606250469\n",
      "Epoch 16 num_samples 10200 loss 0.14863535020686225\n",
      "Epoch 16 num_samples 10300 loss 0.09935758205584207\n",
      "Epoch 16 num_samples 10400 loss 0.19280927284635896\n",
      "Epoch 16 num_samples 10500 loss 0.18294828805368263\n",
      "Epoch 16 num_samples 10600 loss 0.1703776962264088\n",
      "Epoch 16 num_samples 10700 loss 0.18152335342663356\n",
      "Epoch 16 num_samples 10800 loss 0.11380446345880625\n",
      "Epoch 16 num_samples 10900 loss 0.25894117352269824\n",
      "Epoch 16 num_samples 11000 loss 0.1915541202566452\n",
      "Epoch 16 num_samples 11100 loss 0.11764689422557406\n",
      "Epoch 16 num_samples 11200 loss 0.2679446152221758\n",
      "Epoch 16 num_samples 11300 loss 0.12448733529647715\n",
      "Epoch 16 num_samples 11400 loss 0.14704815969269616\n",
      "Epoch 16 num_samples 11500 loss 0.18003863884088664\n",
      "Epoch 16 num_samples 11600 loss 0.15962712001369347\n",
      "Epoch 16 num_samples 11700 loss 0.10466267055763233\n",
      "Epoch 16 num_samples 11800 loss 0.11842095244186857\n",
      "Epoch 16 num_samples 11900 loss 0.1270484142762598\n",
      "Epoch 16 num_samples 12000 loss 0.15888545368980697\n",
      "Epoch 16 num_samples 12100 loss 0.12708841634979387\n",
      "Epoch 16 num_samples 12200 loss 0.11492524988061549\n",
      "Epoch 16 num_samples 12300 loss 0.1927780754839697\n",
      "Epoch 16 num_samples 12400 loss 0.17510567288067336\n",
      "Epoch 16 num_samples 12500 loss 0.11615418495941894\n",
      "Epoch 16 num_samples 12600 loss 0.1728513042005993\n",
      "Epoch 16 num_samples 12700 loss 0.18342111290255647\n",
      "Epoch 16 num_samples 12800 loss 0.1281035543331581\n",
      "Epoch 16 num_samples 12900 loss 0.2536398253024489\n",
      "Epoch 16 num_samples 13000 loss 0.12045460267260864\n",
      "Epoch 16 num_samples 13100 loss 0.09517142387268995\n",
      "Epoch 16 num_samples 13200 loss 0.21782527344068936\n",
      "Epoch 16 num_samples 13300 loss 0.12219707063192654\n",
      "Epoch 16 num_samples 13400 loss 0.1754972614374775\n",
      "Epoch 16 num_samples 13500 loss 0.08495360387112569\n",
      "Epoch 16 num_samples 13600 loss 0.14757703508023456\n",
      "Epoch 16 num_samples 13700 loss 0.1570401898984546\n",
      "Epoch 16 num_samples 13800 loss 0.19715678659333397\n",
      "Epoch 16 num_samples 13900 loss 0.1628451741009708\n",
      "Epoch 16 num_samples 14000 loss 0.16432160368838122\n",
      "Epoch 16 num_samples 14100 loss 0.15828212648104256\n",
      "Epoch 16 num_samples 14200 loss 0.17326404890837607\n",
      "Epoch 16 num_samples 14300 loss 0.15897578981098068\n",
      "Epoch 16 num_samples 14400 loss 0.16210208420022873\n",
      "Epoch 16 num_samples 14500 loss 0.11095960122515967\n",
      "Epoch 16 num_samples 14600 loss 0.22842116720795033\n",
      "Epoch 16 num_samples 14700 loss 0.17185830267579852\n",
      "Epoch 16 num_samples 14800 loss 0.18058912379430225\n",
      "Epoch 16 num_samples 14900 loss 0.08630594709731759\n",
      "Epoch 16 num_samples 15000 loss 0.14730509116053886\n",
      "Epoch 16 num_samples 15100 loss 0.128518205305354\n",
      "Epoch 16 num_samples 15200 loss 0.24644659335606373\n",
      "Epoch 16 num_samples 15300 loss 0.1318009068606026\n",
      "Epoch 16 num_samples 15400 loss 0.10019915715303826\n",
      "Epoch 16 num_samples 15500 loss 0.1000072550137612\n",
      "Epoch 16 num_samples 15600 loss 0.10868906364831091\n",
      "Epoch 16 num_samples 15700 loss 0.13845605696575158\n",
      "Epoch 16 num_samples 15800 loss 0.1481856229413886\n",
      "Epoch 16 num_samples 15900 loss 0.23097120974210078\n",
      "Epoch 16 num_samples 16000 loss 0.19379610533821662\n",
      "Epoch 16 num_samples 16100 loss 0.13486156322016515\n",
      "Epoch 16 num_samples 16200 loss 0.15986231078494165\n",
      "Epoch 16 num_samples 16300 loss 0.22465435404178347\n",
      "Epoch 16 num_samples 16400 loss 0.22960286962091203\n",
      "Epoch 16 num_samples 16500 loss 0.199435460146664\n",
      "Epoch 16 num_samples 16600 loss 0.18984568115763742\n",
      "Epoch 16 num_samples 16700 loss 0.13658523155078858\n",
      "Epoch 16 num_samples 16800 loss 0.16622921730860454\n",
      "Epoch 16 num_samples 16900 loss 0.16438187631964582\n",
      "Epoch 16 num_samples 17000 loss 0.1805243503982136\n",
      "Epoch 16 num_samples 17100 loss 0.11726017862612775\n",
      "Epoch 16 num_samples 17200 loss 0.12740160105242676\n",
      "Epoch 16 num_samples 17300 loss 0.12873750988499047\n",
      "Epoch 16 num_samples 17400 loss 0.14554275647897938\n",
      "Epoch 16 num_samples 17500 loss 0.1305441094793843\n",
      "Epoch 16 num_samples 17600 loss 0.15042228016242398\n",
      "Epoch 16 num_samples 17700 loss 0.1605971885694875\n",
      "Epoch 16 num_samples 17800 loss 0.13204370558418843\n",
      "Epoch 16 num_samples 17900 loss 0.14777103830012275\n",
      "Epoch 16 num_samples 18000 loss 0.11060456775199225\n",
      "Epoch 16 num_samples 18100 loss 0.16165781740167412\n",
      "Epoch 16 num_samples 18200 loss 0.11331034374773717\n",
      "Epoch 16 num_samples 18300 loss 0.1707338423066183\n",
      "Epoch 16 num_samples 18400 loss 0.14756987194192\n",
      "Epoch 16 num_samples 18500 loss 0.12378657213555178\n",
      "Epoch 17 num_samples 0 loss 0.19842129175233822\n",
      "Epoch 17 num_samples 100 loss 0.20773201414874598\n",
      "Epoch 17 num_samples 200 loss 0.1540824854785156\n",
      "Epoch 17 num_samples 300 loss 0.1258302495519303\n",
      "Epoch 17 num_samples 400 loss 0.11839831288948485\n",
      "Epoch 17 num_samples 500 loss 0.12373858846632288\n",
      "Epoch 17 num_samples 600 loss 0.20486277605342265\n",
      "Epoch 17 num_samples 700 loss 0.14370704700761136\n",
      "Epoch 17 num_samples 800 loss 0.15455903334817592\n",
      "Epoch 17 num_samples 900 loss 0.11521265748133137\n",
      "Epoch 17 num_samples 1000 loss 0.13009338406877913\n",
      "Epoch 17 num_samples 1100 loss 0.11080352181157153\n",
      "Epoch 17 num_samples 1200 loss 0.10887445580472406\n",
      "Epoch 17 num_samples 1300 loss 0.1793948040913118\n",
      "Epoch 17 num_samples 1400 loss 0.1669841897722072\n",
      "Epoch 17 num_samples 1500 loss 0.23316360910648526\n",
      "Epoch 17 num_samples 1600 loss 0.1310029511451133\n",
      "Epoch 17 num_samples 1700 loss 0.1876569051680492\n",
      "Epoch 17 num_samples 1800 loss 0.1575079409964803\n",
      "Epoch 17 num_samples 1900 loss 0.2034345899173178\n",
      "Epoch 17 num_samples 2000 loss 0.11412811869676333\n",
      "Epoch 17 num_samples 2100 loss 0.15720314146074105\n",
      "Epoch 17 num_samples 2200 loss 0.13128227819423696\n",
      "Epoch 17 num_samples 2300 loss 0.08103577409626532\n",
      "Epoch 17 num_samples 2400 loss 0.07866885226062914\n",
      "Epoch 17 num_samples 2500 loss 0.08611521177405906\n",
      "Epoch 17 num_samples 2600 loss 0.14608494447336645\n",
      "Epoch 17 num_samples 2700 loss 0.15003875551277965\n",
      "Epoch 17 num_samples 2800 loss 0.129686556612127\n",
      "Epoch 17 num_samples 2900 loss 0.0997913436123765\n",
      "Epoch 17 num_samples 3000 loss 0.16070769131926144\n",
      "Epoch 17 num_samples 3100 loss 0.11921704393302036\n",
      "Epoch 17 num_samples 3200 loss 0.15775068971943887\n",
      "Epoch 17 num_samples 3300 loss 0.14553563595984786\n",
      "Epoch 17 num_samples 3400 loss 0.11768533208341264\n",
      "Epoch 17 num_samples 3500 loss 0.1178705234768913\n",
      "Epoch 17 num_samples 3600 loss 0.13056451959069704\n",
      "Epoch 17 num_samples 3700 loss 0.08691972827471883\n",
      "Epoch 17 num_samples 3800 loss 0.16515027851182734\n",
      "Epoch 17 num_samples 3900 loss 0.1509807278880978\n",
      "Epoch 17 num_samples 4000 loss 0.11116473190885232\n",
      "Epoch 17 num_samples 4100 loss 0.14744721584402024\n",
      "Epoch 17 num_samples 4200 loss 0.10550824518048545\n",
      "Epoch 17 num_samples 4300 loss 0.13743681118750778\n",
      "Epoch 17 num_samples 4400 loss 0.13306140879809047\n",
      "Epoch 17 num_samples 4500 loss 0.17630145557195326\n",
      "Epoch 17 num_samples 4600 loss 0.09988341853621886\n",
      "Epoch 17 num_samples 4700 loss 0.1770626759235143\n",
      "Epoch 17 num_samples 4800 loss 0.09775478735736928\n",
      "Epoch 17 num_samples 4900 loss 0.15186436796144823\n",
      "Epoch 17 num_samples 5000 loss 0.18046365532688738\n",
      "Epoch 17 num_samples 5100 loss 0.1826770921775157\n",
      "Epoch 17 num_samples 5200 loss 0.1348488615800545\n",
      "Epoch 17 num_samples 5300 loss 0.15924030090444385\n",
      "Epoch 17 num_samples 5400 loss 0.11294234755660142\n",
      "Epoch 17 num_samples 5500 loss 0.17351549109210201\n",
      "Epoch 17 num_samples 5600 loss 0.21001057112381638\n",
      "Epoch 17 num_samples 5700 loss 0.18188443879977584\n",
      "Epoch 17 num_samples 5800 loss 0.14809534322535367\n",
      "Epoch 17 num_samples 5900 loss 0.12582538228737533\n",
      "Epoch 17 num_samples 6000 loss 0.1734872490328337\n",
      "Epoch 17 num_samples 6100 loss 0.1261500776216426\n",
      "Epoch 17 num_samples 6200 loss 0.173225120938452\n",
      "Epoch 17 num_samples 6300 loss 0.19981064034943558\n",
      "Epoch 17 num_samples 6400 loss 0.13561648533044968\n",
      "Epoch 17 num_samples 6500 loss 0.1608002860627926\n",
      "Epoch 17 num_samples 6600 loss 0.0931012515536051\n",
      "Epoch 17 num_samples 6700 loss 0.1872417244849\n",
      "Epoch 17 num_samples 6800 loss 0.14094495498301637\n",
      "Epoch 17 num_samples 6900 loss 0.14460729783174173\n",
      "Epoch 17 num_samples 7000 loss 0.11911097500716906\n",
      "Epoch 17 num_samples 7100 loss 0.15053877981874628\n",
      "Epoch 17 num_samples 7200 loss 0.0798648749248722\n",
      "Epoch 17 num_samples 7300 loss 0.20926233648960385\n",
      "Epoch 17 num_samples 7400 loss 0.1880935851483302\n",
      "Epoch 17 num_samples 7500 loss 0.13174086922446018\n",
      "Epoch 17 num_samples 7600 loss 0.11522180641036187\n",
      "Epoch 17 num_samples 7700 loss 0.16664295661790682\n",
      "Epoch 17 num_samples 7800 loss 0.12417825860867521\n",
      "Epoch 17 num_samples 7900 loss 0.08905868643263343\n",
      "Epoch 17 num_samples 8000 loss 0.18109675623580354\n",
      "Epoch 17 num_samples 8100 loss 0.15364939375199854\n",
      "Epoch 17 num_samples 8200 loss 0.09624894161373318\n",
      "Epoch 17 num_samples 8300 loss 0.10872206410540775\n",
      "Epoch 17 num_samples 8400 loss 0.08199416906541955\n",
      "Epoch 17 num_samples 8500 loss 0.14443311471906983\n",
      "Epoch 17 num_samples 8600 loss 0.15648868896769969\n",
      "Epoch 17 num_samples 8700 loss 0.22883322412932217\n",
      "Epoch 17 num_samples 8800 loss 0.07885237491197362\n",
      "Epoch 17 num_samples 8900 loss 0.19958271710209993\n",
      "Epoch 17 num_samples 9000 loss 0.10662565188682231\n",
      "Epoch 17 num_samples 9100 loss 0.17405594986648956\n",
      "Epoch 17 num_samples 9200 loss 0.1273928494796551\n",
      "Epoch 17 num_samples 9300 loss 0.12766048560808227\n",
      "Epoch 17 num_samples 9400 loss 0.13011262113923122\n",
      "Epoch 17 num_samples 9500 loss 0.12639798580448658\n",
      "Epoch 17 num_samples 9600 loss 0.10810812985567939\n",
      "Epoch 17 num_samples 9700 loss 0.20705307280461047\n",
      "Epoch 17 num_samples 9800 loss 0.07229432883405323\n",
      "Epoch 17 num_samples 9900 loss 0.205819156826282\n",
      "Epoch 17 num_samples 10000 loss 0.0965427767747056\n",
      "Epoch 17 num_samples 10100 loss 0.1608446787756622\n",
      "Epoch 17 num_samples 10200 loss 0.13292846297921965\n",
      "Epoch 17 num_samples 10300 loss 0.09158824383991948\n",
      "Epoch 17 num_samples 10400 loss 0.17414964109846343\n",
      "Epoch 17 num_samples 10500 loss 0.16990834080699724\n",
      "Epoch 17 num_samples 10600 loss 0.15297435290919437\n",
      "Epoch 17 num_samples 10700 loss 0.1681781378239566\n",
      "Epoch 17 num_samples 10800 loss 0.10545786646808468\n",
      "Epoch 17 num_samples 10900 loss 0.24052667605380237\n",
      "Epoch 17 num_samples 11000 loss 0.17924297137023917\n",
      "Epoch 17 num_samples 11100 loss 0.10866285781078089\n",
      "Epoch 17 num_samples 11200 loss 0.24246261309577308\n",
      "Epoch 17 num_samples 11300 loss 0.11353192189059885\n",
      "Epoch 17 num_samples 11400 loss 0.1341677751117697\n",
      "Epoch 17 num_samples 11500 loss 0.16434690131889845\n",
      "Epoch 17 num_samples 11600 loss 0.14566947083846332\n",
      "Epoch 17 num_samples 11700 loss 0.09642926004920885\n",
      "Epoch 17 num_samples 11800 loss 0.10744293151030773\n",
      "Epoch 17 num_samples 11900 loss 0.11942393195382676\n",
      "Epoch 17 num_samples 12000 loss 0.145883996933701\n",
      "Epoch 17 num_samples 12100 loss 0.11557572425959808\n",
      "Epoch 17 num_samples 12200 loss 0.10623145095622898\n",
      "Epoch 17 num_samples 12300 loss 0.1771907794705317\n",
      "Epoch 17 num_samples 12400 loss 0.1582856285023543\n",
      "Epoch 17 num_samples 12500 loss 0.10907955875506176\n",
      "Epoch 17 num_samples 12600 loss 0.16352796541138248\n",
      "Epoch 17 num_samples 12700 loss 0.1739572389000596\n",
      "Epoch 17 num_samples 12800 loss 0.11629754282896414\n",
      "Epoch 17 num_samples 12900 loss 0.24304086506489636\n",
      "Epoch 17 num_samples 13000 loss 0.10865729454077633\n",
      "Epoch 17 num_samples 13100 loss 0.08758061323763357\n",
      "Epoch 17 num_samples 13200 loss 0.20117615493068045\n",
      "Epoch 17 num_samples 13300 loss 0.11264912296261859\n",
      "Epoch 17 num_samples 13400 loss 0.1667727689146588\n",
      "Epoch 17 num_samples 13500 loss 0.07441865526698754\n",
      "Epoch 17 num_samples 13600 loss 0.13420561727669586\n",
      "Epoch 17 num_samples 13700 loss 0.14411418344351776\n",
      "Epoch 17 num_samples 13800 loss 0.18431690354821686\n",
      "Epoch 17 num_samples 13900 loss 0.14974956716373725\n",
      "Epoch 17 num_samples 14000 loss 0.15251013159194862\n",
      "Epoch 17 num_samples 14100 loss 0.14481331188814775\n",
      "Epoch 17 num_samples 14200 loss 0.1586993883119891\n",
      "Epoch 17 num_samples 14300 loss 0.14408702069026347\n",
      "Epoch 17 num_samples 14400 loss 0.15157425848262437\n",
      "Epoch 17 num_samples 14500 loss 0.10214762896220521\n",
      "Epoch 17 num_samples 14600 loss 0.21043434271545383\n",
      "Epoch 17 num_samples 14700 loss 0.1606521159889069\n",
      "Epoch 17 num_samples 14800 loss 0.16516827398662529\n",
      "Epoch 17 num_samples 14900 loss 0.07721911327052473\n",
      "Epoch 17 num_samples 15000 loss 0.13602087429040155\n",
      "Epoch 17 num_samples 15100 loss 0.12080721304895485\n",
      "Epoch 17 num_samples 15200 loss 0.23072223240000675\n",
      "Epoch 17 num_samples 15300 loss 0.1211264798025512\n",
      "Epoch 17 num_samples 15400 loss 0.09212284418484745\n",
      "Epoch 17 num_samples 15500 loss 0.09158865074868945\n",
      "Epoch 17 num_samples 15600 loss 0.10037195218449455\n",
      "Epoch 17 num_samples 15700 loss 0.12730606510730985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 num_samples 15800 loss 0.13476897748499567\n",
      "Epoch 17 num_samples 15900 loss 0.2094163467151931\n",
      "Epoch 17 num_samples 16000 loss 0.17824199318201173\n",
      "Epoch 17 num_samples 16100 loss 0.12402461958626067\n",
      "Epoch 17 num_samples 16200 loss 0.1470949275655349\n",
      "Epoch 17 num_samples 16300 loss 0.20579907688224458\n",
      "Epoch 17 num_samples 16400 loss 0.20592894647274126\n",
      "Epoch 17 num_samples 16500 loss 0.1844381113599361\n",
      "Epoch 17 num_samples 16600 loss 0.17295767971768966\n",
      "Epoch 17 num_samples 16700 loss 0.12538485763918483\n",
      "Epoch 17 num_samples 16800 loss 0.15493831805584116\n",
      "Epoch 17 num_samples 16900 loss 0.15154432912852828\n",
      "Epoch 17 num_samples 17000 loss 0.16628349591482597\n",
      "Epoch 17 num_samples 17100 loss 0.10592227152276192\n",
      "Epoch 17 num_samples 17200 loss 0.11878699407687865\n",
      "Epoch 17 num_samples 17300 loss 0.12045356140160117\n",
      "Epoch 17 num_samples 17400 loss 0.13102232606184633\n",
      "Epoch 17 num_samples 17500 loss 0.11860473787613288\n",
      "Epoch 17 num_samples 17600 loss 0.1384149029103418\n",
      "Epoch 17 num_samples 17700 loss 0.14958577139675286\n",
      "Epoch 17 num_samples 17800 loss 0.12075854616505342\n",
      "Epoch 17 num_samples 17900 loss 0.13414672054413823\n",
      "Epoch 17 num_samples 18000 loss 0.1007728509518148\n",
      "Epoch 17 num_samples 18100 loss 0.14842662355681063\n",
      "Epoch 17 num_samples 18200 loss 0.10373374740835581\n",
      "Epoch 17 num_samples 18300 loss 0.15581154161806346\n",
      "Epoch 17 num_samples 18400 loss 0.13135392068356372\n",
      "Epoch 17 num_samples 18500 loss 0.1131053738082286\n",
      "Epoch 18 num_samples 0 loss 0.1858221463207954\n",
      "Epoch 18 num_samples 100 loss 0.19274571877174876\n",
      "Epoch 18 num_samples 200 loss 0.14214935447079363\n",
      "Epoch 18 num_samples 300 loss 0.11685491905862946\n",
      "Epoch 18 num_samples 400 loss 0.11022088024915973\n",
      "Epoch 18 num_samples 500 loss 0.11514213690080341\n",
      "Epoch 18 num_samples 600 loss 0.18705771696731874\n",
      "Epoch 18 num_samples 700 loss 0.133080946460137\n",
      "Epoch 18 num_samples 800 loss 0.14347986629988987\n",
      "Epoch 18 num_samples 900 loss 0.10515209095967526\n",
      "Epoch 18 num_samples 1000 loss 0.12155885812392332\n",
      "Epoch 18 num_samples 1100 loss 0.10088380119170484\n",
      "Epoch 18 num_samples 1200 loss 0.10203335164729647\n",
      "Epoch 18 num_samples 1300 loss 0.16446440086341776\n",
      "Epoch 18 num_samples 1400 loss 0.1568003026586427\n",
      "Epoch 18 num_samples 1500 loss 0.21189088967439595\n",
      "Epoch 18 num_samples 1600 loss 0.12092566711586197\n",
      "Epoch 18 num_samples 1700 loss 0.17407886280910098\n",
      "Epoch 18 num_samples 1800 loss 0.14713914490954283\n",
      "Epoch 18 num_samples 1900 loss 0.18544780248485002\n",
      "Epoch 18 num_samples 2000 loss 0.10345723265332021\n",
      "Epoch 18 num_samples 2100 loss 0.14224192479771886\n",
      "Epoch 18 num_samples 2200 loss 0.12337629125723208\n",
      "Epoch 18 num_samples 2300 loss 0.07258430120715405\n",
      "Epoch 18 num_samples 2400 loss 0.07094235938522336\n",
      "Epoch 18 num_samples 2500 loss 0.0798278538770428\n",
      "Epoch 18 num_samples 2600 loss 0.13487473852685153\n",
      "Epoch 18 num_samples 2700 loss 0.13584361370241124\n",
      "Epoch 18 num_samples 2800 loss 0.12236826858079367\n",
      "Epoch 18 num_samples 2900 loss 0.0917380626455364\n",
      "Epoch 18 num_samples 3000 loss 0.14836132912342137\n",
      "Epoch 18 num_samples 3100 loss 0.10720909904868119\n",
      "Epoch 18 num_samples 3200 loss 0.14889175541676578\n",
      "Epoch 18 num_samples 3300 loss 0.13403815937824112\n",
      "Epoch 18 num_samples 3400 loss 0.10641315603317729\n",
      "Epoch 18 num_samples 3500 loss 0.10782405303808063\n",
      "Epoch 18 num_samples 3600 loss 0.11998965285551998\n",
      "Epoch 18 num_samples 3700 loss 0.08062364379644847\n",
      "Epoch 18 num_samples 3800 loss 0.1497482101577047\n",
      "Epoch 18 num_samples 3900 loss 0.141078596000481\n",
      "Epoch 18 num_samples 4000 loss 0.10456096927865367\n",
      "Epoch 18 num_samples 4100 loss 0.13839836752430779\n",
      "Epoch 18 num_samples 4200 loss 0.09648730362147116\n",
      "Epoch 18 num_samples 4300 loss 0.12680147736654207\n",
      "Epoch 18 num_samples 4400 loss 0.12536436569427992\n",
      "Epoch 18 num_samples 4500 loss 0.1610429805639723\n",
      "Epoch 18 num_samples 4600 loss 0.09025267957305193\n",
      "Epoch 18 num_samples 4700 loss 0.1631703224042025\n",
      "Epoch 18 num_samples 4800 loss 0.08780139988331119\n",
      "Epoch 18 num_samples 4900 loss 0.1368354608887409\n",
      "Epoch 18 num_samples 5000 loss 0.1668852225005578\n",
      "Epoch 18 num_samples 5100 loss 0.1694028908615073\n",
      "Epoch 18 num_samples 5200 loss 0.12242867265118633\n",
      "Epoch 18 num_samples 5300 loss 0.14604947728973733\n",
      "Epoch 18 num_samples 5400 loss 0.10010289661092336\n",
      "Epoch 18 num_samples 5500 loss 0.16251760128010218\n",
      "Epoch 18 num_samples 5600 loss 0.19644102388903872\n",
      "Epoch 18 num_samples 5700 loss 0.16758198523688406\n",
      "Epoch 18 num_samples 5800 loss 0.13853386268417572\n",
      "Epoch 18 num_samples 5900 loss 0.117660245143074\n",
      "Epoch 18 num_samples 6000 loss 0.16531334236703188\n",
      "Epoch 18 num_samples 6100 loss 0.1142411324702308\n",
      "Epoch 18 num_samples 6200 loss 0.1569931784074389\n",
      "Epoch 18 num_samples 6300 loss 0.18750530133900398\n",
      "Epoch 18 num_samples 6400 loss 0.12525961223716192\n",
      "Epoch 18 num_samples 6500 loss 0.1506453012544064\n",
      "Epoch 18 num_samples 6600 loss 0.08330341524669997\n",
      "Epoch 18 num_samples 6700 loss 0.1719439153778036\n",
      "Epoch 18 num_samples 6800 loss 0.1293326730530272\n",
      "Epoch 18 num_samples 6900 loss 0.1354219472372284\n",
      "Epoch 18 num_samples 7000 loss 0.1091543774252446\n",
      "Epoch 18 num_samples 7100 loss 0.13950023241863707\n",
      "Epoch 18 num_samples 7200 loss 0.07279045963098817\n",
      "Epoch 18 num_samples 7300 loss 0.19115052693579218\n",
      "Epoch 18 num_samples 7400 loss 0.17971135148418912\n",
      "Epoch 18 num_samples 7500 loss 0.12223738517342231\n",
      "Epoch 18 num_samples 7600 loss 0.1073340447774877\n",
      "Epoch 18 num_samples 7700 loss 0.15419746658526617\n",
      "Epoch 18 num_samples 7800 loss 0.1124962460644554\n",
      "Epoch 18 num_samples 7900 loss 0.08287698271596594\n",
      "Epoch 18 num_samples 8000 loss 0.16405620100666332\n",
      "Epoch 18 num_samples 8100 loss 0.1416184068118834\n",
      "Epoch 18 num_samples 8200 loss 0.08840069565494954\n",
      "Epoch 18 num_samples 8300 loss 0.10109596700218651\n",
      "Epoch 18 num_samples 8400 loss 0.07326199842939941\n",
      "Epoch 18 num_samples 8500 loss 0.1351508159043466\n",
      "Epoch 18 num_samples 8600 loss 0.14346442649097266\n",
      "Epoch 18 num_samples 8700 loss 0.21059044908441613\n",
      "Epoch 18 num_samples 8800 loss 0.07147098354903793\n",
      "Epoch 18 num_samples 8900 loss 0.18829588300379857\n",
      "Epoch 18 num_samples 9000 loss 0.09798059807232065\n",
      "Epoch 18 num_samples 9100 loss 0.15976158555128225\n",
      "Epoch 18 num_samples 9200 loss 0.11757878942181557\n",
      "Epoch 18 num_samples 9300 loss 0.11669340328477304\n",
      "Epoch 18 num_samples 9400 loss 0.11979993313781297\n",
      "Epoch 18 num_samples 9500 loss 0.11314984739819584\n",
      "Epoch 18 num_samples 9600 loss 0.09740747916999044\n",
      "Epoch 18 num_samples 9700 loss 0.19124880786285928\n",
      "Epoch 18 num_samples 9800 loss 0.06770441583894951\n",
      "Epoch 18 num_samples 9900 loss 0.1889402341070185\n",
      "Epoch 18 num_samples 10000 loss 0.0896621906830531\n",
      "Epoch 18 num_samples 10100 loss 0.14530293438130454\n",
      "Epoch 18 num_samples 10200 loss 0.11935869677102857\n",
      "Epoch 18 num_samples 10300 loss 0.08400965031724127\n",
      "Epoch 18 num_samples 10400 loss 0.15753705204720692\n",
      "Epoch 18 num_samples 10500 loss 0.1592234894537198\n",
      "Epoch 18 num_samples 10600 loss 0.13833558916007116\n",
      "Epoch 18 num_samples 10700 loss 0.15316220778037345\n",
      "Epoch 18 num_samples 10800 loss 0.09890766611499148\n",
      "Epoch 18 num_samples 10900 loss 0.22450633677593815\n",
      "Epoch 18 num_samples 11000 loss 0.16761337232892046\n",
      "Epoch 18 num_samples 11100 loss 0.1007865362641508\n",
      "Epoch 18 num_samples 11200 loss 0.21994469844643402\n",
      "Epoch 18 num_samples 11300 loss 0.10358110966845924\n",
      "Epoch 18 num_samples 11400 loss 0.12317198696704548\n",
      "Epoch 18 num_samples 11500 loss 0.15160963649072562\n",
      "Epoch 18 num_samples 11600 loss 0.1323179491530723\n",
      "Epoch 18 num_samples 11700 loss 0.08813940208893416\n",
      "Epoch 18 num_samples 11800 loss 0.09785844066041643\n",
      "Epoch 18 num_samples 11900 loss 0.11053689685346053\n",
      "Epoch 18 num_samples 12000 loss 0.13223457003993033\n",
      "Epoch 18 num_samples 12100 loss 0.10539616754146727\n",
      "Epoch 18 num_samples 12200 loss 0.09718194390714195\n",
      "Epoch 18 num_samples 12300 loss 0.16322991342008975\n",
      "Epoch 18 num_samples 12400 loss 0.1434646577858364\n",
      "Epoch 18 num_samples 12500 loss 0.10261050346615548\n",
      "Epoch 18 num_samples 12600 loss 0.15398604523779522\n",
      "Epoch 18 num_samples 12700 loss 0.1632586729595401\n",
      "Epoch 18 num_samples 12800 loss 0.10761808201579537\n",
      "Epoch 18 num_samples 12900 loss 0.2335050375225464\n",
      "Epoch 18 num_samples 13000 loss 0.09764287187816585\n",
      "Epoch 18 num_samples 13100 loss 0.07977135527204476\n",
      "Epoch 18 num_samples 13200 loss 0.18587259946687215\n",
      "Epoch 18 num_samples 13300 loss 0.10228556111583754\n",
      "Epoch 18 num_samples 13400 loss 0.15729393513978468\n",
      "Epoch 18 num_samples 13500 loss 0.06725046292150041\n",
      "Epoch 18 num_samples 13600 loss 0.12145942611131683\n",
      "Epoch 18 num_samples 13700 loss 0.13251190753741918\n",
      "Epoch 18 num_samples 13800 loss 0.17006756772064502\n",
      "Epoch 18 num_samples 13900 loss 0.13714409070864012\n",
      "Epoch 18 num_samples 14000 loss 0.13998452406894032\n",
      "Epoch 18 num_samples 14100 loss 0.1354718432883675\n",
      "Epoch 18 num_samples 14200 loss 0.1435012702722035\n",
      "Epoch 18 num_samples 14300 loss 0.1299683216991648\n",
      "Epoch 18 num_samples 14400 loss 0.14571099033173218\n",
      "Epoch 18 num_samples 14500 loss 0.09393290154049923\n",
      "Epoch 18 num_samples 14600 loss 0.19718408785931246\n",
      "Epoch 18 num_samples 14700 loss 0.14930945581587143\n",
      "Epoch 18 num_samples 14800 loss 0.15235474061146045\n",
      "Epoch 18 num_samples 14900 loss 0.06889043134227009\n",
      "Epoch 18 num_samples 15000 loss 0.1253082014232006\n",
      "Epoch 18 num_samples 15100 loss 0.1147211461707429\n",
      "Epoch 18 num_samples 15200 loss 0.216893535842286\n",
      "Epoch 18 num_samples 15300 loss 0.11162319253439185\n",
      "Epoch 18 num_samples 15400 loss 0.08508176705459436\n",
      "Epoch 18 num_samples 15500 loss 0.08209805487175989\n",
      "Epoch 18 num_samples 15600 loss 0.09111936301158838\n",
      "Epoch 18 num_samples 15700 loss 0.11704109107618202\n",
      "Epoch 18 num_samples 15800 loss 0.12155300113531466\n",
      "Epoch 18 num_samples 15900 loss 0.19068150108416865\n",
      "Epoch 18 num_samples 16000 loss 0.16276231353444168\n",
      "Epoch 18 num_samples 16100 loss 0.113465266145292\n",
      "Epoch 18 num_samples 16200 loss 0.13581368598768934\n",
      "Epoch 18 num_samples 16300 loss 0.1871511864637781\n",
      "Epoch 18 num_samples 16400 loss 0.18543722111643113\n",
      "Epoch 18 num_samples 16500 loss 0.17199312773661493\n",
      "Epoch 18 num_samples 16600 loss 0.15810788243911197\n",
      "Epoch 18 num_samples 16700 loss 0.11695938507064434\n",
      "Epoch 18 num_samples 16800 loss 0.1424135734812827\n",
      "Epoch 18 num_samples 16900 loss 0.13847098433881344\n",
      "Epoch 18 num_samples 17000 loss 0.15429112306685103\n",
      "Epoch 18 num_samples 17100 loss 0.09569935116023491\n",
      "Epoch 18 num_samples 17200 loss 0.10735227352919091\n",
      "Epoch 18 num_samples 17300 loss 0.11160520128506494\n",
      "Epoch 18 num_samples 17400 loss 0.11854337466195368\n",
      "Epoch 18 num_samples 17500 loss 0.10748587006569746\n",
      "Epoch 18 num_samples 17600 loss 0.1284744568874624\n",
      "Epoch 18 num_samples 17700 loss 0.13951386100661928\n",
      "Epoch 18 num_samples 17800 loss 0.11081431836536527\n",
      "Epoch 18 num_samples 17900 loss 0.12321765943015102\n",
      "Epoch 18 num_samples 18000 loss 0.0928632121000234\n",
      "Epoch 18 num_samples 18100 loss 0.13604385655805987\n",
      "Epoch 18 num_samples 18200 loss 0.09601769730466486\n",
      "Epoch 18 num_samples 18300 loss 0.14134271940186324\n",
      "Epoch 18 num_samples 18400 loss 0.11986983744119975\n",
      "Epoch 18 num_samples 18500 loss 0.10288550680684798\n",
      "Epoch 19 num_samples 0 loss 0.1746220943228763\n",
      "Epoch 19 num_samples 100 loss 0.17761279763281837\n",
      "Epoch 19 num_samples 200 loss 0.13007808224125658\n",
      "Epoch 19 num_samples 300 loss 0.10830312913456781\n",
      "Epoch 19 num_samples 400 loss 0.10203463430191077\n",
      "Epoch 19 num_samples 500 loss 0.10672629564890078\n",
      "Epoch 19 num_samples 600 loss 0.17086711103688207\n",
      "Epoch 19 num_samples 700 loss 0.12294150321671658\n",
      "Epoch 19 num_samples 800 loss 0.13292690948538818\n",
      "Epoch 19 num_samples 900 loss 0.09607755687752069\n",
      "Epoch 19 num_samples 1000 loss 0.11358964032251583\n",
      "Epoch 19 num_samples 1100 loss 0.09087717563094784\n",
      "Epoch 19 num_samples 1200 loss 0.09509599335843057\n",
      "Epoch 19 num_samples 1300 loss 0.1513648952704055\n",
      "Epoch 19 num_samples 1400 loss 0.1480847936439961\n",
      "Epoch 19 num_samples 1500 loss 0.1928068635230728\n",
      "Epoch 19 num_samples 1600 loss 0.11034478561248241\n",
      "Epoch 19 num_samples 1700 loss 0.15911896431019001\n",
      "Epoch 19 num_samples 1800 loss 0.1363868067221703\n",
      "Epoch 19 num_samples 1900 loss 0.1702870383556091\n",
      "Epoch 19 num_samples 2000 loss 0.09482289197746917\n",
      "Epoch 19 num_samples 2100 loss 0.12851459794859219\n",
      "Epoch 19 num_samples 2200 loss 0.11489689521658429\n",
      "Epoch 19 num_samples 2300 loss 0.06670777761760713\n",
      "Epoch 19 num_samples 2400 loss 0.06413218001854912\n",
      "Epoch 19 num_samples 2500 loss 0.0739998332999169\n",
      "Epoch 19 num_samples 2600 loss 0.12391706819371934\n",
      "Epoch 19 num_samples 2700 loss 0.12375516704302761\n",
      "Epoch 19 num_samples 2800 loss 0.11455100276165425\n",
      "Epoch 19 num_samples 2900 loss 0.084658541934037\n",
      "Epoch 19 num_samples 3000 loss 0.1394107901925282\n",
      "Epoch 19 num_samples 3100 loss 0.09720674816313803\n",
      "Epoch 19 num_samples 3200 loss 0.14168336260305664\n",
      "Epoch 19 num_samples 3300 loss 0.12169306887911574\n",
      "Epoch 19 num_samples 3400 loss 0.09708593701356923\n",
      "Epoch 19 num_samples 3500 loss 0.09895412945420581\n",
      "Epoch 19 num_samples 3600 loss 0.11149719524728813\n",
      "Epoch 19 num_samples 3700 loss 0.07414137637614279\n",
      "Epoch 19 num_samples 3800 loss 0.13658942964189916\n",
      "Epoch 19 num_samples 3900 loss 0.1313160603825308\n",
      "Epoch 19 num_samples 4000 loss 0.09844092058288412\n",
      "Epoch 19 num_samples 4100 loss 0.13109095139576124\n",
      "Epoch 19 num_samples 4200 loss 0.08947851435276177\n",
      "Epoch 19 num_samples 4300 loss 0.11620472743577161\n",
      "Epoch 19 num_samples 4400 loss 0.11855472267772721\n",
      "Epoch 19 num_samples 4500 loss 0.14752179318115885\n",
      "Epoch 19 num_samples 4600 loss 0.0819955395979785\n",
      "Epoch 19 num_samples 4700 loss 0.1479848473228007\n",
      "Epoch 19 num_samples 4800 loss 0.08091436442118571\n",
      "Epoch 19 num_samples 4900 loss 0.12431490110338796\n",
      "Epoch 19 num_samples 5000 loss 0.15563405767770683\n",
      "Epoch 19 num_samples 5100 loss 0.15687598461836175\n",
      "Epoch 19 num_samples 5200 loss 0.11170292793657717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 num_samples 5300 loss 0.1344427592795515\n",
      "Epoch 19 num_samples 5400 loss 0.08817941800923135\n",
      "Epoch 19 num_samples 5500 loss 0.15037210457961325\n",
      "Epoch 19 num_samples 5600 loss 0.1845154833419077\n",
      "Epoch 19 num_samples 5700 loss 0.15172535685425145\n",
      "Epoch 19 num_samples 5800 loss 0.1296512222370589\n",
      "Epoch 19 num_samples 5900 loss 0.10740823208635718\n",
      "Epoch 19 num_samples 6000 loss 0.15830180308098554\n",
      "Epoch 19 num_samples 6100 loss 0.10365529989799077\n",
      "Epoch 19 num_samples 6200 loss 0.14091093861033566\n",
      "Epoch 19 num_samples 6300 loss 0.17249593390264428\n",
      "Epoch 19 num_samples 6400 loss 0.11557668841781003\n",
      "Epoch 19 num_samples 6500 loss 0.14216804894387036\n",
      "Epoch 19 num_samples 6600 loss 0.07588824767734086\n",
      "Epoch 19 num_samples 6700 loss 0.15656266037782607\n",
      "Epoch 19 num_samples 6800 loss 0.11775525208821419\n",
      "Epoch 19 num_samples 6900 loss 0.12663493868113881\n",
      "Epoch 19 num_samples 7000 loss 0.10046255062350258\n",
      "Epoch 19 num_samples 7100 loss 0.1301828111701022\n",
      "Epoch 19 num_samples 7200 loss 0.06695835303548094\n",
      "Epoch 19 num_samples 7300 loss 0.177014011857996\n",
      "Epoch 19 num_samples 7400 loss 0.1694009767903089\n",
      "Epoch 19 num_samples 7500 loss 0.11382096164695142\n",
      "Epoch 19 num_samples 7600 loss 0.0998835583434474\n",
      "Epoch 19 num_samples 7700 loss 0.1422090738533827\n",
      "Epoch 19 num_samples 7800 loss 0.10226568036224423\n",
      "Epoch 19 num_samples 7900 loss 0.07728823161581501\n",
      "Epoch 19 num_samples 8000 loss 0.14813548826114614\n",
      "Epoch 19 num_samples 8100 loss 0.12965080328402284\n",
      "Epoch 19 num_samples 8200 loss 0.07979074229204848\n",
      "Epoch 19 num_samples 8300 loss 0.09410179387587993\n",
      "Epoch 19 num_samples 8400 loss 0.0656710016257073\n",
      "Epoch 19 num_samples 8500 loss 0.12545012540616407\n",
      "Epoch 19 num_samples 8600 loss 0.13156769253072853\n",
      "Epoch 19 num_samples 8700 loss 0.1937369844036757\n",
      "Epoch 19 num_samples 8800 loss 0.0637532282816068\n",
      "Epoch 19 num_samples 8900 loss 0.17595352249236285\n",
      "Epoch 19 num_samples 9000 loss 0.08900176550943754\n",
      "Epoch 19 num_samples 9100 loss 0.14617812333419514\n",
      "Epoch 19 num_samples 9200 loss 0.10925151319273495\n",
      "Epoch 19 num_samples 9300 loss 0.10692145816092317\n",
      "Epoch 19 num_samples 9400 loss 0.11063937080923641\n",
      "Epoch 19 num_samples 9500 loss 0.10084410812525704\n",
      "Epoch 19 num_samples 9600 loss 0.08914300410303921\n",
      "Epoch 19 num_samples 9700 loss 0.17727593673045877\n",
      "Epoch 19 num_samples 9800 loss 0.0638382712109645\n",
      "Epoch 19 num_samples 9900 loss 0.17416030100295088\n",
      "Epoch 19 num_samples 10000 loss 0.08354206954663894\n",
      "Epoch 19 num_samples 10100 loss 0.13127979978439208\n",
      "Epoch 19 num_samples 10200 loss 0.10523123265071899\n",
      "Epoch 19 num_samples 10300 loss 0.07783521669210348\n",
      "Epoch 19 num_samples 10400 loss 0.14178694509482026\n",
      "Epoch 19 num_samples 10500 loss 0.14870523988387963\n",
      "Epoch 19 num_samples 10600 loss 0.1256649357540327\n",
      "Epoch 19 num_samples 10700 loss 0.1393891883472704\n",
      "Epoch 19 num_samples 10800 loss 0.09347873742583052\n",
      "Epoch 19 num_samples 10900 loss 0.20812688113657618\n",
      "Epoch 19 num_samples 11000 loss 0.15734963126961587\n",
      "Epoch 19 num_samples 11100 loss 0.0939846690663067\n",
      "Epoch 19 num_samples 11200 loss 0.1971890177245391\n",
      "Epoch 19 num_samples 11300 loss 0.09689781321867125\n",
      "Epoch 19 num_samples 11400 loss 0.11282922137195428\n",
      "Epoch 19 num_samples 11500 loss 0.14074883760351922\n",
      "Epoch 19 num_samples 11600 loss 0.12154420932965579\n",
      "Epoch 19 num_samples 11700 loss 0.0806167186989298\n",
      "Epoch 19 num_samples 11800 loss 0.08874792710942188\n",
      "Epoch 19 num_samples 11900 loss 0.10437550520128154\n",
      "Epoch 19 num_samples 12000 loss 0.12032046602954943\n",
      "Epoch 19 num_samples 12100 loss 0.09432922146197541\n",
      "Epoch 19 num_samples 12200 loss 0.08841120607310658\n",
      "Epoch 19 num_samples 12300 loss 0.1506844850712627\n",
      "Epoch 19 num_samples 12400 loss 0.13160966898779417\n",
      "Epoch 19 num_samples 12500 loss 0.09642470877236033\n",
      "Epoch 19 num_samples 12600 loss 0.14533630904471095\n",
      "Epoch 19 num_samples 12700 loss 0.15324032045809027\n",
      "Epoch 19 num_samples 12800 loss 0.09783107892485753\n",
      "Epoch 19 num_samples 12900 loss 0.22718304501236264\n",
      "Epoch 19 num_samples 13000 loss 0.08783737909086825\n",
      "Epoch 19 num_samples 13100 loss 0.07396573859457825\n",
      "Epoch 19 num_samples 13200 loss 0.17082851304583285\n",
      "Epoch 19 num_samples 13300 loss 0.09441153681692427\n",
      "Epoch 19 num_samples 13400 loss 0.14824544612789553\n",
      "Epoch 19 num_samples 13500 loss 0.06001041457675424\n",
      "Epoch 19 num_samples 13600 loss 0.11095682043137783\n",
      "Epoch 19 num_samples 13700 loss 0.12296726546069227\n",
      "Epoch 19 num_samples 13800 loss 0.15800630022803996\n",
      "Epoch 19 num_samples 13900 loss 0.12641385180806042\n",
      "Epoch 19 num_samples 14000 loss 0.1301760577342883\n",
      "Epoch 19 num_samples 14100 loss 0.12490293578145627\n",
      "Epoch 19 num_samples 14200 loss 0.12787461290243665\n",
      "Epoch 19 num_samples 14300 loss 0.11930534976828192\n",
      "Epoch 19 num_samples 14400 loss 0.13771651391581585\n",
      "Epoch 19 num_samples 14500 loss 0.08831230967581295\n",
      "Epoch 19 num_samples 14600 loss 0.18187485045902776\n",
      "Epoch 19 num_samples 14700 loss 0.13935046410253515\n",
      "Epoch 19 num_samples 14800 loss 0.14116807576659965\n",
      "Epoch 19 num_samples 14900 loss 0.06311864819890571\n",
      "Epoch 19 num_samples 15000 loss 0.11514653891684898\n",
      "Epoch 19 num_samples 15100 loss 0.10850004350619238\n",
      "Epoch 19 num_samples 15200 loss 0.20308886450469374\n",
      "Epoch 19 num_samples 15300 loss 0.10299734230564801\n",
      "Epoch 19 num_samples 15400 loss 0.07810617183866164\n",
      "Epoch 19 num_samples 15500 loss 0.07383304171404093\n",
      "Epoch 19 num_samples 15600 loss 0.08425789656577665\n",
      "Epoch 19 num_samples 15700 loss 0.1074996634288501\n",
      "Epoch 19 num_samples 15800 loss 0.11062245664491568\n",
      "Epoch 19 num_samples 15900 loss 0.1735644782554121\n",
      "Epoch 19 num_samples 16000 loss 0.14900741618488142\n",
      "Epoch 19 num_samples 16100 loss 0.10403510553036986\n",
      "Epoch 19 num_samples 16200 loss 0.12475922587570847\n",
      "Epoch 19 num_samples 16300 loss 0.16961258667423693\n",
      "Epoch 19 num_samples 16400 loss 0.16601683642252646\n",
      "Epoch 19 num_samples 16500 loss 0.1595783576658822\n",
      "Epoch 19 num_samples 16600 loss 0.14343429035577182\n",
      "Epoch 19 num_samples 16700 loss 0.10597198558270424\n",
      "Epoch 19 num_samples 16800 loss 0.13123382392081026\n",
      "Epoch 19 num_samples 16900 loss 0.12671599748295187\n",
      "Epoch 19 num_samples 17000 loss 0.14289225831052296\n",
      "Epoch 19 num_samples 17100 loss 0.08793133822202492\n",
      "Epoch 19 num_samples 17200 loss 0.09892827781057274\n",
      "Epoch 19 num_samples 17300 loss 0.10529074699283546\n",
      "Epoch 19 num_samples 17400 loss 0.10819017586624353\n",
      "Epoch 19 num_samples 17500 loss 0.09599810605029024\n",
      "Epoch 19 num_samples 17600 loss 0.11835650937328016\n",
      "Epoch 19 num_samples 17700 loss 0.13061288635199791\n",
      "Epoch 19 num_samples 17800 loss 0.10095382346068338\n",
      "Epoch 19 num_samples 17900 loss 0.11360384308543484\n",
      "Epoch 19 num_samples 18000 loss 0.08648640593080309\n",
      "Epoch 19 num_samples 18100 loss 0.12447958740578108\n",
      "Epoch 19 num_samples 18200 loss 0.08871880417366519\n",
      "Epoch 19 num_samples 18300 loss 0.12794562032064669\n",
      "Epoch 19 num_samples 18400 loss 0.10971892402417142\n",
      "Epoch 19 num_samples 18500 loss 0.09470369023872516\n",
      "label: 3 predict: 3\n",
      "label: 1 predict: 1\n",
      "label: 3 predict: 3\n",
      "label: 4 predict: 4\n",
      "label: 9 predict: 9\n",
      "label: 1 predict: 1\n",
      "label: 4 predict: 4\n",
      "label: 7 predict: 7\n",
      "label: 8 predict: 8\n",
      "label: 4 predict: 4\n",
      "label: O predict: O\n",
      "label: 6 predict: 6\n",
      "label: 4 predict: 4\n",
      "label: O predict: O\n",
      "label: 9 predict: 9\n",
      "label: 4 predict: 4\n",
      "label: 9 predict: 9\n",
      "label: 4 predict: 4\n",
      "label: 7 predict: 7\n",
      "label: 5 predict: 5\n",
      "label: O predict: O\n",
      "label: Z predict: Z\n",
      "label: 2 predict: 2\n",
      "label: 5 predict: 5\n",
      "label: Z predict: 2\n",
      "label: 2 predict: 2\n",
      "label: O predict: 4\n",
      "label: 3 predict: 3\n",
      "label: Z predict: Z\n",
      "label: 4 predict: 4\n",
      "label: 3 predict: 3\n",
      "label: 6 predict: 6\n",
      "label: 5 predict: 6\n",
      "label: O predict: O\n",
      "label: 6 predict: 6\n",
      "label: 1 predict: 1\n",
      "label: 5 predict: 5\n",
      "label: Z predict: Z\n",
      "label: 2 predict: 2\n",
      "label: 7 predict: 7\n",
      "label: 8 predict: 8\n",
      "label: 4 predict: 4\n",
      "label: 7 predict: 7\n",
      "label: 9 predict: 9\n",
      "label: 7 predict: 7\n",
      "label: 5 predict: 5\n",
      "label: 8 predict: 8\n",
      "label: Z predict: Z\n",
      "label: 6 predict: 6\n",
      "label: 9 predict: 9\n",
      "label: Z predict: Z\n",
      "label: 6 predict: 6\n",
      "label: 2 predict: O\n",
      "label: 2 predict: 2\n",
      "label: 5 predict: 5\n",
      "label: 6 predict: 6\n",
      "label: 8 predict: 8\n",
      "label: 9 predict: 9\n",
      "label: Z predict: Z\n",
      "label: 2 predict: 2\n",
      "label: 6 predict: 6\n",
      "label: 9 predict: 9\n",
      "label: O predict: O\n",
      "label: 9 predict: 9\n",
      "label: 1 predict: 1\n",
      "label: 7 predict: 7\n",
      "label: 8 predict: 8\n",
      "label: 3 predict: 3\n",
      "label: 3 predict: 3\n",
      "label: 2 predict: 2\n",
      "label: 5 predict: 5\n",
      "label: O predict: O\n",
      "label: 7 predict: 7\n",
      "label: 6 predict: 6\n",
      "label: 2 predict: 2\n",
      "label: 5 predict: 5\n",
      "label: 4 predict: 4\n",
      "label: 9 predict: 9\n",
      "label: 3 predict: 3\n",
      "label: 5 predict: 5\n",
      "label: 7 predict: 7\n",
      "label: 2 predict: 2\n",
      "label: 7 predict: 7\n",
      "label: 1 predict: 1\n",
      "label: 8 predict: 8\n",
      "label: Z predict: Z\n",
      "label: 6 predict: 6\n",
      "label: Z predict: Z\n",
      "label: 8 predict: 8\n",
      "label: O predict: O\n",
      "label: 1 predict: 1\n",
      "label: 8 predict: 8\n",
      "label: 3 predict: 3\n",
      "label: 4 predict: 4\n",
      "label: 1 predict: 1\n",
      "label: 7 predict: 7\n",
      "label: 8 predict: 8\n",
      "label: 6 predict: 6\n",
      "label: 1 predict: 1\n",
      "label: 8 predict: 8\n",
      "label: 3 predict: 3\n",
      "label: 9 predict: 9\n",
      "label: 2 predict: 2\n",
      "label: O predict: O\n",
      "label: Z predict: Z\n",
      "label: 1 predict: 1\n",
      "label: 3 predict: 3\n",
      "label: O predict: O\n",
      "label: 1 predict: 1\n",
      "label: 5 predict: 5\n",
      "Acc: 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    np.random.seed(777)\n",
    "    # We splice the raw feat with left 5 frames and right 5 frames\n",
    "    # So the input here is 39 * (5 + 1 + 5) = 429\n",
    "    dnn = DNN(429, 11, 128, 1)\n",
    "    dnn.set_learning_rate(5e-3)\n",
    "    train(dnn)\n",
    "    test(dnn)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceba91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
